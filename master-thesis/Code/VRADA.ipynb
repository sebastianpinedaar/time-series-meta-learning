{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base on code from https://github.com/floft/codats/blob/294ccca16e545f5e51ca9d3cb971fcab1625b5a3/models.py#L1086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlow 2.0 implementation of VRNN\n",
    "Based on my 1.x implementation:\n",
    "https://github.com/floft/deep-activity-learning/blob/tf_1.x/vrnn.py\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class VRNN(tf.keras.layers.Layer):\n",
    "    \"\"\" Wrap VRNNCell into a RNN \"\"\"\n",
    "    def __init__(self, h_dim, z_dim, return_z=True, return_sequences=False,\n",
    "            go_backwards=False, stateful=False, unroll=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.return_z = return_z\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.return_sequences = return_sequences\n",
    "        self.go_backwards = go_backwards\n",
    "        self.stateful = stateful\n",
    "        self.unroll = unroll\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_features = input_shape[-1]\n",
    "        cell = VRNNCell(num_features, self.h_dim, self.z_dim)\n",
    "        # We return sequences here so we can compute VRNN reconstruction loss\n",
    "        self.rnn = tf.keras.layers.RNN(cell,\n",
    "            return_sequences=True,\n",
    "            return_state=False, go_backwards=self.go_backwards,\n",
    "            stateful=self.stateful, unroll=self.unroll)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.rnn(inputs, **kwargs)\n",
    "\n",
    "        h, c, \\\n",
    "            encoder_mu, encoder_sigma, \\\n",
    "            decoder_mu, decoder_sigma, \\\n",
    "            prior_mu, prior_sigma, \\\n",
    "            x_1, z_1 = outputs\n",
    "\n",
    "        # VRADA uses z not h\n",
    "        if self.return_z:\n",
    "            rnn_output = z_1\n",
    "        else:\n",
    "            rnn_output = h\n",
    "\n",
    "        # Get the output at the end of the sequence\n",
    "        if not self.return_sequences:\n",
    "            rnn_output = rnn_output[:, -1]\n",
    "\n",
    "        # For use in loss, note these are return_sequences=True\n",
    "        other_outputs = [encoder_mu, encoder_sigma, decoder_mu, decoder_sigma,\n",
    "            prior_mu, prior_sigma]\n",
    "\n",
    "        return rnn_output, other_outputs\n",
    "\n",
    "\n",
    "class VRNNCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    VRNN cell implementation for use in VRADA\n",
    "    Based on:\n",
    "    - https://github.com/phreeza/tensorflow-vrnn/blob/master/model_vrnn.py\n",
    "    - https://github.com/kimkilho/tensorflow-vrnn/blob/master/cell.py\n",
    "    - https://github.com/kimkilho/tensorflow-vrnn/blob/master/main.py\n",
    "    - https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/rnn_cell_impl.py\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, h_dim, z_dim, **kwargs):\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # Dimensions of x input, hidden layers, latent variable (z)\n",
    "        self.n_x = self.x_dim\n",
    "        self.n_h = self.h_dim\n",
    "        self.n_z = self.z_dim\n",
    "\n",
    "        # Dimensions of phi(z)\n",
    "        self.n_x_1 = self.x_dim\n",
    "        self.n_z_1 = self.z_dim\n",
    "\n",
    "        # Dimensions of encoder, decoder, and prior\n",
    "        self.n_enc_hidden = self.z_dim\n",
    "        self.n_dec_hidden = self.x_dim\n",
    "        self.n_prior_hidden = self.z_dim\n",
    "\n",
    "        # Note: first two are the state of the LSTM\n",
    "        self.state_size = (\n",
    "            self.n_h, self.n_h,\n",
    "            self.n_z, self.n_z,\n",
    "            self.n_x, self.n_x,\n",
    "            self.n_z, self.n_z,\n",
    "            self.n_x_1, self.n_z_1)\n",
    "\n",
    "        # What cell we're going to use internally for the RNN\n",
    "        self.cell = tf.keras.layers.LSTMCell(h_dim)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Input: previous hidden state\n",
    "        self.prior_h = self.add_weight(\"prior/hidden/weights\",\n",
    "            shape=(self.n_h, self.n_prior_hidden), initializer=\"glorot_uniform\")\n",
    "        self.prior_mu = self.add_weight(\"prior/mu/weights\",\n",
    "            shape=(self.n_prior_hidden, self.n_z), initializer=\"glorot_uniform\")\n",
    "        self.prior_sigma = self.add_weight(\"prior/sigma/weights\",\n",
    "            shape=(self.n_prior_hidden, self.n_z), initializer=\"glorot_uniform\")\n",
    "\n",
    "        self.prior_h_b = self.add_weight(\"prior/hidden/bias\",\n",
    "            shape=(self.n_prior_hidden,), initializer=tf.constant_initializer())\n",
    "        self.prior_sigma_b = self.add_weight(\"prior/sigma/bias\",\n",
    "            shape=(self.n_z,), initializer=tf.constant_initializer())\n",
    "        self.prior_mu_b = self.add_weight(\"prior/mu/bias\",\n",
    "            shape=(self.n_z,), initializer=tf.constant_initializer())\n",
    "\n",
    "        # Input: x\n",
    "        self.x_1 = self.add_weight(\"phi_x/weights\",\n",
    "            shape=(self.n_x, self.n_x_1), initializer=\"glorot_uniform\")\n",
    "        self.x_1_b = self.add_weight(\"phi_x/bias\",\n",
    "            shape=(self.n_x_1,), initializer=tf.constant_initializer())\n",
    "\n",
    "        # Input: x and previous hidden state\n",
    "        self.encoder_h = self.add_weight(\"encoder/hidden/weights\",\n",
    "            shape=(self.n_x_1+self.n_h, self.n_enc_hidden), initializer=\"glorot_uniform\")\n",
    "        self.encoder_mu = self.add_weight(\"encoder/mu/weights\",\n",
    "            shape=(self.n_enc_hidden, self.n_z), initializer=\"glorot_uniform\")\n",
    "        self.encoder_sigma = self.add_weight(\"encoder/sigma/weights\",\n",
    "            shape=(self.n_enc_hidden, self.n_z), initializer=\"glorot_uniform\")\n",
    "\n",
    "        self.encoder_h_b = self.add_weight(\"encoder/hidden/bias\",\n",
    "            shape=(self.n_enc_hidden,), initializer=tf.constant_initializer())\n",
    "        self.encoder_sigma_b = self.add_weight(\"encoder/sigma/bias\",\n",
    "            shape=(self.n_z,), initializer=tf.constant_initializer())\n",
    "        self.encoder_mu_b = self.add_weight(\"encoder/mu/bias\",\n",
    "            shape=(self.n_z,), initializer=tf.constant_initializer())\n",
    "\n",
    "        # Input: z = enc_sigma*eps + enc_mu -- i.e. reparameterization trick\n",
    "        self.z_1 = self.add_weight(\"phi_z/weights\",\n",
    "            shape=(self.n_z, self.n_z_1), initializer=\"glorot_uniform\")\n",
    "        self.z_1_b = self.add_weight(\"phi_z/bias\",\n",
    "            shape=(self.n_z_1,), initializer=tf.constant_initializer())\n",
    "\n",
    "        # Input: latent variable (z) and previous hidden state\n",
    "        self.decoder_h = self.add_weight(\"decoder/hidden/weights\",\n",
    "            shape=(self.n_z+self.n_h, self.n_dec_hidden), initializer=\"glorot_uniform\")\n",
    "        self.decoder_mu = self.add_weight(\"decoder/mu/weights\",\n",
    "            shape=(self.n_dec_hidden, self.n_x), initializer=\"glorot_uniform\")\n",
    "        self.decoder_sigma = self.add_weight(\"decoder/sigma/weights\",\n",
    "            shape=(self.n_dec_hidden, self.n_x), initializer=\"glorot_uniform\")\n",
    "\n",
    "        self.decoder_h_b = self.add_weight(\"decoder/hidden/bias\",\n",
    "            shape=(self.n_dec_hidden,), initializer=tf.constant_initializer())\n",
    "        self.decoder_sigma_b = self.add_weight(\"decoder/sigma/bias\",\n",
    "            shape=(self.n_x,), initializer=tf.constant_initializer())\n",
    "        self.decoder_mu_b = self.add_weight(\"decoder/mu/bias\",\n",
    "            shape=(self.n_x,), initializer=tf.constant_initializer())\n",
    "\n",
    "    def call(self, inputs, states, **kwargs):\n",
    "        # Get relevant states\n",
    "        h = states[0]\n",
    "        c = states[1]  # only passed to the LSTM\n",
    "\n",
    "        # Input: previous hidden state (h)\n",
    "        prior_h = tf.nn.relu(tf.matmul(h, self.prior_h) + self.prior_h_b)\n",
    "        prior_sigma = tf.nn.softplus(tf.matmul(prior_h, self.prior_sigma) + self.prior_sigma_b)  # >= 0\n",
    "        prior_mu = tf.matmul(prior_h, self.prior_mu) + self.prior_mu_b\n",
    "\n",
    "        # Input: x\n",
    "        # TODO removed ReLU since in the dataset not all x values are positive\n",
    "        x_1 = tf.matmul(inputs, self.x_1) + self.x_1_b\n",
    "\n",
    "        # Input: x and previous hidden state\n",
    "        encoder_input = tf.concat((x_1, h), 1)\n",
    "        encoder_h = tf.nn.relu(tf.matmul(encoder_input, self.encoder_h) + self.encoder_h_b)\n",
    "        encoder_sigma = tf.nn.softplus(tf.matmul(encoder_h, self.encoder_sigma) + self.encoder_sigma_b)\n",
    "        encoder_mu = tf.matmul(encoder_h, self.encoder_mu) + self.encoder_mu_b\n",
    "\n",
    "        # Input: z = enc_sigma*eps + enc_mu -- i.e. reparameterization trick\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        eps = tf.keras.backend.random_normal((batch_size, self.n_z), dtype=tf.float32)\n",
    "        z = encoder_sigma*eps + encoder_mu\n",
    "        z_1 = tf.nn.relu(tf.matmul(z, self.z_1) + self.z_1_b)\n",
    "\n",
    "        # Input: latent variable (z) and previous hidden state\n",
    "        decoder_input = tf.concat((z_1, h), 1)\n",
    "        decoder_h = tf.nn.relu(tf.matmul(decoder_input, self.decoder_h) + self.decoder_h_b)\n",
    "        decoder_sigma = tf.nn.softplus(tf.matmul(decoder_h, self.decoder_sigma) + self.decoder_sigma_b)\n",
    "        decoder_mu = tf.matmul(decoder_h, self.decoder_mu) + self.decoder_mu_b\n",
    "\n",
    "        # Pass to cell (e.g. LSTM). Note that the LSTM has both \"h\" and \"c\" that are combined\n",
    "        # into the same next state vector. We'll combine them together to pass in and split them\n",
    "        # back out after the LSTM returns the next state.\n",
    "        rnn_cell_input = tf.concat((x_1, z_1), 1)\n",
    "        _, (h_next, c_next) = self.cell(rnn_cell_input, [h, c])  # Note: (h,c) in Keras (c,h) in tf contrib\n",
    "\n",
    "        # VRNN state\n",
    "        next_state = (\n",
    "            h_next,\n",
    "            c_next,\n",
    "            encoder_mu,\n",
    "            encoder_sigma,\n",
    "            decoder_mu,\n",
    "            decoder_sigma,\n",
    "            prior_mu,\n",
    "            prior_sigma,\n",
    "            x_1,\n",
    "            z_1,\n",
    "        )\n",
    "\n",
    "        #return output, next_state\n",
    "        return next_state, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ModelBase(tf.keras.Model):\n",
    "    \"\"\" Base model class (inheriting from Keras' Model class) \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _get_trainable_variables_list(self, model_list):\n",
    "        \"\"\" Get all trainable variables if model is a list \"\"\"\n",
    "        model_vars = []\n",
    "\n",
    "        for m in model_list:\n",
    "            model_vars += m.trainable_variables\n",
    "\n",
    "        return model_vars\n",
    "\n",
    "    def _get_trainable_variables(self, model):\n",
    "        \"\"\" Get trainable variables if model is a list or not \"\"\"\n",
    "        if isinstance(model, list):\n",
    "            return self._get_trainable_variables_list(model)\n",
    "\n",
    "        return model.trainable_variables\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_fe(self):\n",
    "        return self._get_trainable_variables(self.feature_extractor)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_task(self):\n",
    "        return self._get_trainable_variables(self.task_classifier)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_domain(self):\n",
    "        return self._get_trainable_variables(self.domain_classifier)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_task_fe(self):\n",
    "        return self.trainable_variables_fe \\\n",
    "            + self.trainable_variables_task\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_task_fe_domain(self):\n",
    "        return self.trainable_variables_fe \\\n",
    "            + self.trainable_variables_task \\\n",
    "            + self.trainable_variables_domain\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        \"\"\" Returns all trainable variables in the model \"\"\"\n",
    "        return self.trainable_variables_task_fe_domain\n",
    "\n",
    "    def set_learning_phase(self, training):\n",
    "        # Manually set the learning phase since we probably aren't using .fit()\n",
    "        # but layers like batch norm and dropout still need to know if\n",
    "        # training/testing\n",
    "        if training is True:\n",
    "            tf.keras.backend.set_learning_phase(1)\n",
    "        elif training is False:\n",
    "            tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "    # Allow easily overriding each part of the call() function, without having\n",
    "    # to override call() in its entirety\n",
    "    def call_feature_extractor(self, inputs, which_fe=None, which_tc=None,\n",
    "            which_dc=None, **kwargs):\n",
    "        if which_fe is not None:\n",
    "            assert isinstance(self.feature_extractor, list)\n",
    "            return self.feature_extractor[which_fe](inputs, **kwargs)\n",
    "\n",
    "        return self.feature_extractor(inputs, **kwargs)\n",
    "\n",
    "    def call_task_classifier(self, fe, which_fe=None, which_tc=None,\n",
    "            which_dc=None, **kwargs):\n",
    "        if which_tc is not None:\n",
    "            assert isinstance(self.task_classifier, list)\n",
    "            return self.task_classifier[which_tc](fe, **kwargs)\n",
    "\n",
    "        return self.task_classifier(fe, **kwargs)\n",
    "\n",
    "    def call_domain_classifier(self, fe, task, which_fe=None, which_tc=None,\n",
    "            which_dc=None, **kwargs):\n",
    "        if which_dc is not None:\n",
    "            assert isinstance(self.domain_classifier, list)\n",
    "            return self.domain_classifier[which_dc](fe, **kwargs)\n",
    "\n",
    "        return self.domain_classifier(fe, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        self.set_learning_phase(training)\n",
    "        fe = self.call_feature_extractor(inputs, **kwargs)\n",
    "        task = self.call_task_classifier(fe, **kwargs)\n",
    "        domain = self.call_domain_classifier(fe, task, **kwargs)\n",
    "        return task, domain, fe\n",
    "\n",
    "\n",
    "class DannModelBase:\n",
    "    \"\"\" DANN adds a gradient reversal layer before the domain classifier\n",
    "    Note: we don't inherit from CnnModelBase or any other specific model because\n",
    "    we want to support either CnnModelBase, RnnModelBase, etc. with multiple\n",
    "    inheritance.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_domains, global_step,\n",
    "            total_steps, **kwargs):\n",
    "        super().__init__(num_classes, num_domains, **kwargs)\n",
    "        grl_schedule = DannGrlSchedule(total_steps)\n",
    "        self.flip_gradient = FlipGradient(global_step, grl_schedule)\n",
    "\n",
    "    def call_domain_classifier(self, fe, task, **kwargs):\n",
    "        # Pass FE output through GRL then to DC\n",
    "        grl_output = self.flip_gradient(fe, **kwargs)\n",
    "        return super().call_domain_classifier(grl_output, task, **kwargs)\n",
    "    \n",
    "\n",
    "\n",
    "class RnnModelBase(ModelBase):\n",
    "    \"\"\" RNN-based model - for R-DANN and VRADA \"\"\"\n",
    "    def __init__(self, num_classes, num_domains, model_name, vrada, **kwargs):\n",
    "        # Note: we ignore model_name here and only define one RNN-based model\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_domains = num_domains\n",
    "        self.feature_extractor = VradaFeatureExtractor(vrada)\n",
    "        self.task_classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(num_classes),\n",
    "        ])\n",
    "        self.domain_classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(num_domains),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        \"\"\" Since our RNN feature extractor returns two values (output and\n",
    "        RNN state, which we need for the loss) we need to only pass the output\n",
    "        to the classifiers, i.e. fe[0] rather than fe \"\"\"\n",
    "        self.set_learning_phase(training)\n",
    "        fe = self.call_feature_extractor(inputs, **kwargs)\n",
    "        task = self.call_task_classifier(fe[0], **kwargs)\n",
    "        domain = self.call_domain_classifier(fe[0], task, **kwargs)\n",
    "        return task, domain, fe\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class VradaFeatureExtractor(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Need to get VRNN state, so we can't directly use Sequential since it can't\n",
    "    return intermediate layer's extra outputs. And, can't use the functional\n",
    "    API directly since we don't now the input shape.\n",
    "    Note: only returns state if vrada=True\n",
    "    \"\"\"\n",
    "    def __init__(self, vrada=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert vrada is True or vrada is False\n",
    "        self.vrada = vrada\n",
    "\n",
    "        if self.vrada:\n",
    "            # Use z for predictions in VRADA like in original paper\n",
    "            self.rnn = VRNN(100, 100, return_z=True, return_sequences=False)\n",
    "        else:\n",
    "            self.rnn = tf.keras.layers.LSTM(100, return_sequences=False)\n",
    "\n",
    "        self.fe = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.Dense(100),\n",
    "            tf.keras.layers.Dense(100),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.vrada:\n",
    "            rnn_output, rnn_state = self.rnn(inputs, **kwargs)\n",
    "        else:\n",
    "            rnn_output = self.rnn(inputs, **kwargs)\n",
    "            rnn_state = None\n",
    "\n",
    "        fe_output = self.fe(rnn_output, **kwargs)\n",
    "\n",
    "        return fe_output, rnn_state\n",
    "\n",
    "\n",
    "class RnnModelBase(ModelBase):\n",
    "    \"\"\" RNN-based model - for R-DANN and VRADA \"\"\"\n",
    "    def __init__(self, num_classes, num_domains, model_name, vrada, **kwargs):\n",
    "        # Note: we ignore model_name here and only define one RNN-based model\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_domains = num_domains\n",
    "        self.feature_extractor = VradaFeatureExtractor(vrada)\n",
    "        self.task_classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(num_classes),\n",
    "        ])\n",
    "        self.domain_classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(50),\n",
    "            tf.keras.layers.Dense(num_domains),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        \"\"\" Since our RNN feature extractor returns two values (output and\n",
    "        RNN state, which we need for the loss) we need to only pass the output\n",
    "        to the classifiers, i.e. fe[0] rather than fe \"\"\"\n",
    "        self.set_learning_phase(training)\n",
    "        fe = self.call_feature_extractor(inputs, **kwargs)\n",
    "        task = self.call_task_classifier(fe[0], **kwargs)\n",
    "        domain = self.call_domain_classifier(fe[0], task, **kwargs)\n",
    "        return task, domain, fe\n",
    "\n",
    "    \n",
    "class VradaModel(DannModelBase, RnnModelBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vrada=True, **kwargs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodDann(MethodBase):\n",
    "    def __init__(self, source_datasets, target_dataset,\n",
    "            global_step, total_steps, *args, **kwargs):\n",
    "        self.global_step = global_step  # should be TF variable\n",
    "        self.total_steps = total_steps\n",
    "        super().__init__(source_datasets, target_dataset, *args, **kwargs)\n",
    "        self.loss_names += [\"task\", \"domain\"]\n",
    "\n",
    "    def create_model(self, model_name):\n",
    "        return models.DannModel(self.num_classes, self.domain_outputs,\n",
    "            self.global_step, self.total_steps, model_name=model_name)\n",
    "\n",
    "    def create_optimizers(self):\n",
    "        opt = super().create_optimizers()\n",
    "        # We need an additional optimizer for DANN\n",
    "        opt[\"d_opt\"] = self.create_optimizer(\n",
    "            learning_rate=FLAGS.lr*FLAGS.lr_domain_mult)\n",
    "        return opt\n",
    "\n",
    "    def create_losses(self):\n",
    "        # Note: at the moment these are the same, but if we go back to\n",
    "        # single-source, then the domain classifier may be sigmoid not softmax\n",
    "        super().create_losses()\n",
    "        self.domain_loss = make_loss()\n",
    "\n",
    "    def prepare_data(self, data_sources, data_target):\n",
    "        assert data_target is not None, \"cannot run DANN without target\"\n",
    "        x_a, y_a, domain_a = data_sources\n",
    "        x_b, y_b, domain_b = data_target\n",
    "\n",
    "        # Concatenate all source domains' data\n",
    "        x_a = tf.concat(x_a, axis=0)\n",
    "        y_a = tf.concat(y_a, axis=0)\n",
    "        domain_a = tf.concat(domain_a, axis=0)\n",
    "\n",
    "        # Concatenate for adaptation - concatenate source labels with all-zero\n",
    "        # labels for target since we can't use the target labels during\n",
    "        # unsupervised domain adaptation\n",
    "        x = tf.concat((x_a, x_b), axis=0)\n",
    "        task_y_true = tf.concat((y_a, tf.zeros_like(y_b)), axis=0)\n",
    "        domain_y_true = tf.concat((domain_a, domain_b), axis=0)\n",
    "\n",
    "        return x, task_y_true, domain_y_true\n",
    "\n",
    "    def compute_losses(self, x, task_y_true, domain_y_true, task_y_pred,\n",
    "            domain_y_pred, fe_output, which_model, training):\n",
    "        nontarget = tf.where(tf.not_equal(domain_y_true, 0))\n",
    "        task_y_true = tf.gather(task_y_true, nontarget)\n",
    "        task_y_pred = tf.gather(task_y_pred, nontarget)\n",
    "\n",
    "        task_loss = self.task_loss(task_y_true, task_y_pred)\n",
    "        d_loss = self.domain_loss(domain_y_true, domain_y_pred)\n",
    "        total_loss = task_loss + d_loss\n",
    "        return [total_loss, task_loss, d_loss]\n",
    "\n",
    "    def compute_gradients(self, tape, losses, which_model):\n",
    "        total_loss, task_loss, d_loss = losses\n",
    "        grad = tape.gradient(total_loss,\n",
    "            self.model[which_model].trainable_variables_task_fe_domain)\n",
    "        d_grad = tape.gradient(d_loss,\n",
    "            self.model[which_model].trainable_variables_domain)\n",
    "        return [grad, d_grad]\n",
    "\n",
    "    def apply_gradients(self, gradients, which_model):\n",
    "        grad, d_grad = gradients\n",
    "        self.opt[which_model][\"opt\"].apply_gradients(zip(grad,\n",
    "            self.model[which_model].trainable_variables_task_fe_domain))\n",
    "        # Update discriminator again\n",
    "        self.opt[which_model][\"d_opt\"].apply_gradients(zip(d_grad,\n",
    "            self.model[which_model].trainable_variables_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MethodBase:\n",
    "    def __init__(self, source_datasets, target_dataset, model_name,\n",
    "            *args, ensemble_size=1, trainable=True, moving_average=False,\n",
    "            share_most_weights=False, **kwargs):\n",
    "        self.source_datasets = source_datasets\n",
    "        self.target_dataset = target_dataset\n",
    "        self.moving_average = moving_average\n",
    "        self.ensemble_size = ensemble_size\n",
    "        assert ensemble_size > 0, \"ensemble_size should be >= 1\"\n",
    "        self.share_most_weights = share_most_weights  # for HeterogeneousBase\n",
    "\n",
    "        # Support multiple targets when we add that functionality\n",
    "        self.num_source_domains = len(source_datasets)\n",
    "        self.num_domains = len(source_datasets)\n",
    "\n",
    "        if target_dataset is not None:\n",
    "            if isinstance(target_dataset, list):\n",
    "                self.num_domains += len(target_dataset)\n",
    "            elif isinstance(target_dataset, load_datasets.Dataset):\n",
    "                self.num_domains += 1\n",
    "            else:\n",
    "                raise NotImplementedError(\"target_dataset should be either one \"\n",
    "                    \"load_datasets.Dataset() or a list of them, \"\n",
    "                    \"but is \"+str(target_dataset))\n",
    "\n",
    "        # How to calculate the number of domain outputs\n",
    "        self.domain_outputs = self.calculate_domain_outputs()\n",
    "\n",
    "        # We need to know the num_classes for creating the model\n",
    "        # We'll just pick the first source since we have to have at least one\n",
    "        # source and we've already verified they're all the same in load_da()\n",
    "        self.num_classes = source_datasets[0].num_classes\n",
    "\n",
    "        # What we want in the checkpoint\n",
    "        self.checkpoint_variables = {}\n",
    "\n",
    "        # Initialize components -- support ensemble, training all simultaneously\n",
    "        # I think will be faster / more efficient overall time-wise\n",
    "        self.create_iterators()\n",
    "        self.opt = [self.create_optimizers() for _ in range(ensemble_size)]\n",
    "        self.model = [self.create_model(model_name) for _ in range(ensemble_size)]\n",
    "        self.create_losses()\n",
    "\n",
    "        # Checkpoint/save the model and optimizers\n",
    "        for i, model in enumerate(self.model):\n",
    "            self.checkpoint_variables[\"model_\" + str(i)] = model\n",
    "\n",
    "        for i, opt_dict in enumerate(self.opt):\n",
    "            for name, opt in opt_dict.items():\n",
    "                self.checkpoint_variables[\"opt_\" + name + \"_\" + str(i)] = opt\n",
    "\n",
    "        # Names of the losses returned in compute_losses\n",
    "        self.loss_names = [\"total\"]\n",
    "\n",
    "        # Should this method be trained (if not, then in main.py the config\n",
    "        # is written and then it exits)\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def calculate_domain_outputs(self):\n",
    "        \"\"\" Calculate the number of outputs for the domain classifier. By\n",
    "        default it's the number of domains. However, for example, in domain\n",
    "        generalization we ignore the target, so it'll actually be the number of\n",
    "        source domains only, in which case override this function. \"\"\"\n",
    "        return self.num_domains\n",
    "\n",
    "    def create_iterators(self):\n",
    "        \"\"\" Get the source/target train/eval datasets \"\"\"\n",
    "        self.source_train_iterators = [iter(x.train) for x in self.source_datasets]\n",
    "        self.source_train_eval_datasets = [x.train_evaluation for x in self.source_datasets]\n",
    "        self.source_test_eval_datasets = [x.test_evaluation for x in self.source_datasets]\n",
    "\n",
    "        if self.target_dataset is not None:\n",
    "            self.target_train_iterator = iter(self.target_dataset.train)\n",
    "            self.target_train_eval_dataset = self.target_dataset.train_evaluation\n",
    "            self.target_test_eval_dataset = self.target_dataset.test_evaluation\n",
    "        else:\n",
    "            self.target_train_iterator = None\n",
    "            self.target_train_eval_dataset = None\n",
    "            self.target_test_eval_dataset = None\n",
    "\n",
    "    def create_optimizer(self, *args, **kwargs):\n",
    "        \"\"\" Create a single optimizer \"\"\"\n",
    "        opt = tf.keras.optimizers.Adam(*args, **kwargs)\n",
    "\n",
    "        if self.moving_average:\n",
    "            opt = tfa.optimizers.MovingAverage(opt)\n",
    "\n",
    "        return opt\n",
    "\n",
    "    def create_optimizers(self):\n",
    "        return {\"opt\": self.create_optimizer(learning_rate=FLAGS.lr)}\n",
    "\n",
    "    def create_model(self, model_name):\n",
    "        return models.BasicModel(self.num_classes, self.domain_outputs,\n",
    "            model_name=model_name)\n",
    "\n",
    "    def create_losses(self):\n",
    "        self.task_loss = make_loss()\n",
    "\n",
    "    def get_next_train_data(self):\n",
    "        \"\"\" Get next batch of training data \"\"\"\n",
    "        # Note we will use this same exact data in Metrics() as we use in\n",
    "        # train_step()\n",
    "        data_sources = [next(x) for x in self.source_train_iterators]\n",
    "        data_target = next(self.target_train_iterator) \\\n",
    "            if self.target_train_iterator is not None else None\n",
    "        return self.get_next_batch_both(data_sources, data_target)\n",
    "\n",
    "    def domain_label(self, index, is_target):\n",
    "        \"\"\" Default domain labeling. Indexes should be in [0,+inf) and integers.\n",
    "        0 = target\n",
    "        1 = source #0\n",
    "        2 = source #1\n",
    "        3 = source #2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        if is_target:\n",
    "            return 0\n",
    "        else:\n",
    "            return index+1\n",
    "\n",
    "    @tf.function\n",
    "    def get_next_batch_both(self, data_sources, data_target):\n",
    "        \"\"\" Compile for training. Don't for evaluation (called directly,\n",
    "        not this _both function). \"\"\"\n",
    "        data_sources = self.get_next_batch_multiple(data_sources, is_target=False)\n",
    "        data_target = self.get_next_batch_single(data_target, is_target=True)\n",
    "        return data_sources, data_target\n",
    "\n",
    "    def get_next_batch_multiple(self, data, is_target):\n",
    "        \"\"\"\n",
    "        Get next set of training data. data should be a list of data (probably\n",
    "        something like [next(x) for x in iterators]).\n",
    "        Returns: (\n",
    "            [x_a1, x_a2, x_a3, ...],\n",
    "            [y_a1, y_a2, y_a3, ...],\n",
    "            [domain_a1, domain_a2, domain_a3, ...]\n",
    "        )\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        assert not is_target or len(data) == 1, \\\n",
    "            \"only support one target at present\"\n",
    "\n",
    "        xs = []\n",
    "        ys = []\n",
    "        ds = []\n",
    "\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "            ds.append(tf.ones_like(y)*self.domain_label(index=i,\n",
    "                is_target=is_target))\n",
    "\n",
    "        return (xs, ys, ds)\n",
    "\n",
    "    def get_next_batch_single(self, data, is_target, index=0):\n",
    "        \"\"\"\n",
    "        Get next set of training data. data should be a single batch (probably\n",
    "        something like next(iterator)). When processing target data, index\n",
    "        must be 0 since we only support one target at the moment. However,\n",
    "        during evaluation we evaluate each source's data individually so if\n",
    "        is_target is False, then index can be whichever source domain was\n",
    "        passed.\n",
    "        Returns: (x, y, domain)\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        assert not is_target or index == 0, \\\n",
    "            \"only support one target at present\"\n",
    "\n",
    "        x, y = data\n",
    "        d = tf.ones_like(y)*self.domain_label(index=index, is_target=is_target)\n",
    "        data_target = (x, y, d)\n",
    "\n",
    "        return data_target\n",
    "\n",
    "    # Allow easily overriding each part of the train_step() function, without\n",
    "    # having to override train_step() in its entirety\n",
    "    def prepare_data(self, data_sources, data_target):\n",
    "        \"\"\" Prepare the data for the model, e.g. by concatenating all sources\n",
    "        together. Note: do not put code in here that changes the domain labels\n",
    "        since you presumably want that during evaluation too. Put that in\n",
    "        domain_label() \"\"\"\n",
    "        # By default (e.g. for no adaptation or domain generalization), ignore\n",
    "        # the target data\n",
    "        x_a, y_a, domain_a = data_sources\n",
    "        x = tf.concat(x_a, axis=0)\n",
    "        task_y_true = tf.concat(y_a, axis=0)\n",
    "        domain_y_true = tf.concat(domain_a, axis=0)\n",
    "        return x, task_y_true, domain_y_true\n",
    "\n",
    "    def prepare_data_eval(self, data, is_target):\n",
    "        \"\"\" Prepare the data for the model, e.g. by concatenating all sources\n",
    "        together. This is like prepare_data() but use during evaluation. \"\"\"\n",
    "        x, y, domain = data\n",
    "\n",
    "        assert isinstance(x, list), \\\n",
    "            \"Must pass x=[...] even if only one domain for tf.function consistency\"\n",
    "        assert isinstance(y, list), \\\n",
    "            \"Must pass y=[...] even if only one domain for tf.function consistency\"\n",
    "        assert isinstance(domain, list), \\\n",
    "            \"Must pass domain=[...] even if only one domain for tf.function consistency\"\n",
    "\n",
    "        # Concatenate all the data (e.g. if multiple source domains)\n",
    "        x = tf.concat(x, axis=0)\n",
    "        y = tf.concat(y, axis=0)\n",
    "        domain = tf.concat(domain, axis=0)\n",
    "\n",
    "        return x, y, domain\n",
    "\n",
    "    def post_data_eval(self, task_y_true, task_y_pred, domain_y_true,\n",
    "            domain_y_pred):\n",
    "        \"\"\" Optionally do something with the data after feeding through the\n",
    "        model. Since the model outputs logits, here we actually take the softmax\n",
    "        so that during evaluation we have probability distributions. \"\"\"\n",
    "        task_y_pred = tf.nn.softmax(task_y_pred)\n",
    "        domain_y_pred = tf.nn.softmax(domain_y_pred)\n",
    "        return task_y_true, task_y_pred, domain_y_true, domain_y_pred\n",
    "\n",
    "    def call_model(self, x, which_model, is_target=None, **kwargs):\n",
    "        return self.model[which_model](x, **kwargs)\n",
    "\n",
    "    def compute_losses(self, x, task_y_true, domain_y_true, task_y_pred,\n",
    "            domain_y_pred, fe_output, which_model, training):\n",
    "        # Maybe: regularization = sum(model.losses) and add to loss\n",
    "        return self.task_loss(task_y_true, task_y_pred)\n",
    "\n",
    "    def compute_gradients(self, tape, loss, which_model):\n",
    "        return tape.gradient(loss,\n",
    "            self.model[which_model].trainable_variables_task_fe)\n",
    "\n",
    "    def apply_gradients(self, grad, which_model):\n",
    "        self.opt[which_model][\"opt\"].apply_gradients(zip(grad,\n",
    "            self.model[which_model].trainable_variables_task_fe))\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Get batch of data, prepare data, run through model, compute losses,\n",
    "        apply the gradients\n",
    "        Override the individual parts with prepare_data(), call_model(),\n",
    "        compute_losses(), compute_gradients(), and apply_gradients()\n",
    "        We return the batch of data so we can use the exact same training batch\n",
    "        for the \"train\" evaluation metrics.\n",
    "        \"\"\"\n",
    "        # TensorFlow errors constructing the graph (with tf.function, which\n",
    "        # makes training faster) if we don't know the data size. Thus, first\n",
    "        # load batches, then pass to compiled train step.\n",
    "        all_data_sources = []\n",
    "        all_data_target = []\n",
    "\n",
    "        for i in range(self.ensemble_size):\n",
    "            data_sources, data_target = self.get_next_train_data()\n",
    "            all_data_sources.append(data_sources)\n",
    "            all_data_target.append(data_target)\n",
    "\n",
    "            # If desired, use the same batch for each of the models.\n",
    "            if FLAGS.ensemble_same_data:\n",
    "                break\n",
    "\n",
    "        self._train_step(all_data_sources, all_data_target)\n",
    "\n",
    "        # We return the first one since we don't really care about the \"train\"\n",
    "        # evaluation metrics that much.\n",
    "        return all_data_sources[0], all_data_target[0]\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, all_data_sources, all_data_target):\n",
    "        \"\"\" The compiled part of train_step. We can't compile everything since\n",
    "        some parts of the model need to know the shape of the data apparently.\n",
    "        The first batch is passed in because to compile this, TF needs to know\n",
    "        the shape. Doesn't look pretty... but it runs...\n",
    "        \"\"\"\n",
    "        for i in range(self.ensemble_size):\n",
    "            # Get random batch for this model in the ensemble (either same for\n",
    "            # all or different for each)\n",
    "            if FLAGS.ensemble_same_data:\n",
    "                data_sources = all_data_sources[0]\n",
    "                data_target = all_data_target[0]\n",
    "            else:\n",
    "                data_sources = all_data_sources[i]\n",
    "                data_target = all_data_target[i]\n",
    "\n",
    "            # Prepare\n",
    "            x, task_y_true, domain_y_true = self.prepare_data(data_sources,\n",
    "                data_target)\n",
    "\n",
    "            # Run batch through the model and compute loss\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                task_y_pred, domain_y_pred, fe_output = self.call_model(\n",
    "                    x, which_model=i, training=True)\n",
    "                losses = self.compute_losses(x, task_y_true, domain_y_true,\n",
    "                    task_y_pred, domain_y_pred, fe_output, which_model=i,\n",
    "                    training=True)\n",
    "\n",
    "            # Update model\n",
    "            gradients = self.compute_gradients(tape, losses, which_model=i)\n",
    "            del tape\n",
    "            self.apply_gradients(gradients, which_model=i)\n",
    "\n",
    "    def eval_step(self, data, is_target):\n",
    "        \"\"\" Evaluate a batch of source or target data, called in metrics.py.\n",
    "        This preprocesses the data to have x, y, domain always be lists so\n",
    "        we can use the same compiled tf.function code in eval_step_list() for\n",
    "        both sources and target domains. \"\"\"\n",
    "        x, y, domain = data\n",
    "\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        if not isinstance(y, list):\n",
    "            y = [y]\n",
    "        if not isinstance(domain, list):\n",
    "            domain = [domain]\n",
    "\n",
    "        return self.eval_step_list((x, y, domain), is_target)\n",
    "\n",
    "    def add_multiple_losses(self, losses, average=False):\n",
    "        \"\"\"\n",
    "        losses = [\n",
    "            [total_loss1, task_loss1, ...],\n",
    "            [total_loss2, task_loss2, ...],\n",
    "            ...\n",
    "        ]\n",
    "        returns [total_loss, task_loss, ...] either the sum or average\n",
    "        \"\"\"\n",
    "        losses_added = None\n",
    "\n",
    "        for loss_list in losses:\n",
    "            # If no losses yet, then just set to this\n",
    "            if losses_added is None:\n",
    "                losses_added = loss_list\n",
    "            # Otherwise, add to the previous loss values\n",
    "            else:\n",
    "                assert len(losses_added) == len(loss_list), \\\n",
    "                    \"subsequent losses have different length than the first\"\n",
    "\n",
    "                for i, loss in enumerate(loss_list):\n",
    "                    losses_added[i] += loss\n",
    "\n",
    "        assert losses_added is not None, \\\n",
    "            \"must return losses from at least one domain\"\n",
    "\n",
    "        if average:\n",
    "            averaged_losses = []\n",
    "\n",
    "            for loss in losses_added:\n",
    "                averaged_losses.append(loss / len(losses))\n",
    "\n",
    "            return averaged_losses\n",
    "        else:\n",
    "            return losses_added\n",
    "\n",
    "    #@tf.function  # faster not to compile\n",
    "    def eval_step_list(self, data, is_target):\n",
    "        \"\"\" Override preparation in prepare_data_eval() \"\"\"\n",
    "        x, orig_task_y_true, orig_domain_y_true = self.prepare_data_eval(data,\n",
    "            is_target)\n",
    "\n",
    "        task_y_true_list = []\n",
    "        task_y_pred_list = []\n",
    "        domain_y_true_list = []\n",
    "        domain_y_pred_list = []\n",
    "        losses_list = []\n",
    "\n",
    "        for i in range(self.ensemble_size):\n",
    "            # Run through model\n",
    "            task_y_pred, domain_y_pred, fe_output = self.call_model(x,\n",
    "                which_model=i, is_target=is_target, training=False)\n",
    "\n",
    "            # Calculate losses\n",
    "            losses = self.compute_losses(x, orig_task_y_true,\n",
    "                orig_domain_y_true, task_y_pred, domain_y_pred, fe_output,\n",
    "                which_model=i, training=False)\n",
    "\n",
    "            if not isinstance(losses, list):\n",
    "                losses = [losses]\n",
    "\n",
    "            losses_list.append(losses)\n",
    "\n",
    "            # Post-process data (e.g. compute softmax from logits)\n",
    "            task_y_true, task_y_pred, domain_y_true, domain_y_pred = \\\n",
    "                self.post_data_eval(orig_task_y_true, task_y_pred,\n",
    "                    orig_domain_y_true, domain_y_pred)\n",
    "\n",
    "            task_y_true_list.append(task_y_true)\n",
    "            task_y_pred_list.append(task_y_pred)\n",
    "            domain_y_true_list.append(domain_y_true)\n",
    "            domain_y_pred_list.append(domain_y_pred)\n",
    "\n",
    "        # Combine information from each model in the ensemble -- averaging.\n",
    "        #\n",
    "        # Note: this is how the ensemble predictions are made with InceptionTime\n",
    "        # having an ensemble of 5 models -- they average the softmax outputs\n",
    "        # over the ensemble (and we now have softmax after the post_data_eval()\n",
    "        # call). See their code:\n",
    "        # https://github.com/hfawaz/InceptionTime/blob/master/classifiers/nne.py\n",
    "        task_y_true_avg = tf.math.reduce_mean(task_y_true_list, axis=0)\n",
    "        task_y_pred_avg = tf.math.reduce_mean(task_y_pred_list, axis=0)\n",
    "        domain_y_true_avg = tf.math.reduce_mean(domain_y_true_list, axis=0)\n",
    "        domain_y_pred_avg = tf.math.reduce_mean(domain_y_pred_list, axis=0)\n",
    "        losses_avg = self.add_multiple_losses(losses_list, average=True)\n",
    "\n",
    "        return task_y_true_avg, task_y_pred_avg, domain_y_true_avg, \\\n",
    "            domain_y_pred_avg, losses_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
