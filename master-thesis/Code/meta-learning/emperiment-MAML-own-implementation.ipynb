{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "from ts_dataset import TSDataset\n",
    "from base_models import LSTMModel\n",
    "from metrics import torch_mae as mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-947c8d44a1de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'maml'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from maml.models.model import Model\n",
    "\n",
    "\n",
    "def weight_init(module):\n",
    "    if (isinstance(module, torch.nn.Linear)\n",
    "        or isinstance(module, torch.nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ConvModel(Model):\n",
    "    \"\"\"\n",
    "    NOTE: difference to tf implementation: batch norm scaling is enabled here\n",
    "    TODO: enable 'non-transductive' setting as per\n",
    "          https://arxiv.org/abs/1803.02999\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_size, num_channels=64,\n",
    "                 kernel_size=3, padding=1, nonlinearity=F.relu,\n",
    "                 use_max_pool=False, img_side_len=28, verbose=False):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self._input_channels = input_channels\n",
    "        self._output_size = output_size\n",
    "        self._num_channels = num_channels\n",
    "        self._kernel_size = kernel_size\n",
    "        self._nonlinearity = nonlinearity\n",
    "        self._use_max_pool = use_max_pool\n",
    "        self._padding = padding\n",
    "        self._bn_affine = False\n",
    "        self._reuse = False\n",
    "        self._verbose = verbose\n",
    "\n",
    "        if self._use_max_pool:\n",
    "            self._conv_stride = 1\n",
    "            self._features_size = 1\n",
    "            self.features = torch.nn.Sequential(OrderedDict([\n",
    "                ('layer1_conv', torch.nn.Conv2d(self._input_channels,\n",
    "                                                self._num_channels,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer1_bn', torch.nn.BatchNorm2d(self._num_channels,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer1_max_pool', torch.nn.MaxPool2d(kernel_size=2,\n",
    "                                                       stride=2)),\n",
    "                ('layer1_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer2_conv', torch.nn.Conv2d(self._num_channels,\n",
    "                                                self._num_channels*2,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer2_bn', torch.nn.BatchNorm2d(self._num_channels*2,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer2_max_pool', torch.nn.MaxPool2d(kernel_size=2,\n",
    "                                                       stride=2)),\n",
    "                ('layer2_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer3_conv', torch.nn.Conv2d(self._num_channels*2,\n",
    "                                                self._num_channels*4,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer3_bn', torch.nn.BatchNorm2d(self._num_channels*4,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer3_max_pool', torch.nn.MaxPool2d(kernel_size=2,\n",
    "                                                       stride=2)),\n",
    "                ('layer3_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer4_conv', torch.nn.Conv2d(self._num_channels*4,\n",
    "                                                self._num_channels*8,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer4_bn', torch.nn.BatchNorm2d(self._num_channels*8,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer4_max_pool', torch.nn.MaxPool2d(kernel_size=2,\n",
    "                                                       stride=2)),\n",
    "                ('layer4_relu', torch.nn.ReLU(inplace=True)),\n",
    "            ]))\n",
    "        else:\n",
    "            self._conv_stride = 2\n",
    "            self._features_size = (img_side_len // 14)**2\n",
    "            self.features = torch.nn.Sequential(OrderedDict([\n",
    "                ('layer1_conv', torch.nn.Conv2d(self._input_channels,\n",
    "                                                self._num_channels,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer1_bn', torch.nn.BatchNorm2d(self._num_channels,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer1_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer2_conv', torch.nn.Conv2d(self._num_channels,\n",
    "                                                self._num_channels*2,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer2_bn', torch.nn.BatchNorm2d(self._num_channels*2,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer2_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer3_conv', torch.nn.Conv2d(self._num_channels*2,\n",
    "                                                self._num_channels*4,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer3_bn', torch.nn.BatchNorm2d(self._num_channels*4,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer3_relu', torch.nn.ReLU(inplace=True)),\n",
    "                ('layer4_conv', torch.nn.Conv2d(self._num_channels*4,\n",
    "                                                self._num_channels*8,\n",
    "                                                self._kernel_size,\n",
    "                                                stride=self._conv_stride,\n",
    "                                                padding=self._padding)),\n",
    "                ('layer4_bn', torch.nn.BatchNorm2d(self._num_channels*8,\n",
    "                                                   affine=self._bn_affine,\n",
    "                                                   momentum=0.001)),\n",
    "                ('layer4_relu', torch.nn.ReLU(inplace=True)),\n",
    "            ]))\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(OrderedDict([\n",
    "            ('fully_connected', torch.nn.Linear(self._num_channels*8,\n",
    "                                                self._output_size))\n",
    "        ]))\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, task, params=None, embeddings=None):\n",
    "        if not self._reuse and self._verbose: print('='*10 + ' Model ' + '='*10)\n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "\n",
    "        x = task.x\n",
    "        if not self._reuse and self._verbose: print('input size: {}'.format(x.size()))\n",
    "        for layer_name, layer in self.features.named_children():\n",
    "            weight = params.get('features.' + layer_name + '.weight', None)\n",
    "            bias = params.get('features.' + layer_name + '.bias', None)\n",
    "            if 'conv' in layer_name:\n",
    "                x = F.conv2d(x, weight=weight, bias=bias,\n",
    "                             stride=self._conv_stride, padding=self._padding)\n",
    "            elif 'bn' in layer_name:\n",
    "                x = F.batch_norm(x, weight=weight, bias=bias,\n",
    "                                 running_mean=layer.running_mean,\n",
    "                                 running_var=layer.running_var,\n",
    "                                 training=True)\n",
    "            elif 'max_pool' in layer_name:\n",
    "                x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "            elif 'relu' in layer_name:\n",
    "                x = F.relu(x)\n",
    "            elif 'fully_connected' in layer_name:\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError('Unrecognized layer {}'.format(layer_name))\n",
    "            if not self._reuse and self._verbose: print('{}: {}'.format(layer_name, x.size()))\n",
    "\n",
    "        # in maml network the conv maps are average pooled\n",
    "        x = x.view(x.size(0), self._num_channels*8, self._features_size)\n",
    "        if not self._reuse and self._verbose: print('reshape to: {}'.format(x.size()))\n",
    "        x = torch.mean(x, dim=2)\n",
    "        if not self._reuse and self._verbose: print('reduce mean: {}'.format(x.size()))\n",
    "        logits = F.linear(\n",
    "            x, weight=params['classifier.fully_connected.weight'],\n",
    "            bias=params['classifier.fully_connected.bias'])\n",
    "        if not self._reuse and self._verbose: print('logits size: {}'.format(logits.size()))\n",
    "        if not self._reuse and self._verbose: print('='*27)\n",
    "        self._reuse = True\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNNBase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2cc1f69e125d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     r\"\"\"Applies a multi-layer long short-term memory (LSTM) RNN to an input\n\u001b[0;32m      3\u001b[0m     \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RNNBase' is not defined"
     ]
    }
   ],
   "source": [
    "class LSTM(RNNBase):\n",
    "    r\"\"\"Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
    "    sequence.\n",
    "\n",
    "\n",
    "    For each element in the input sequence, each layer computes the following\n",
    "    function:\n",
    "\n",
    "    .. math::\n",
    "        \\begin{array}{ll} \\\\\n",
    "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "            c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "            h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}\n",
    "\n",
    "    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
    "    state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`\n",
    "    is the hidden state of the layer at time `t-1` or the initial hidden\n",
    "    state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n",
    "    :math:`o_t` are the input, forget, cell, and output gates, respectively.\n",
    "    :math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n",
    "\n",
    "    In a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n",
    "    (:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\n",
    "    dropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\n",
    "    variable which is :math:`0` with probability :attr:`dropout`.\n",
    "\n",
    "    Args:\n",
    "        input_size: The number of expected features in the input `x`\n",
    "        hidden_size: The number of features in the hidden state `h`\n",
    "        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "            would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "            with the second LSTM taking in outputs of the first LSTM and\n",
    "            computing the final results. Default: 1\n",
    "        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
    "            Default: ``True``\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``\n",
    "        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
    "            LSTM layer except the last layer, with dropout probability equal to\n",
    "            :attr:`dropout`. Default: 0\n",
    "        bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
    "\n",
    "    Inputs: input, (h_0, c_0)\n",
    "        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
    "          of the input sequence.\n",
    "          The input can also be a packed variable length sequence.\n",
    "          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
    "          :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
    "        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "          containing the initial hidden state for each element in the batch.\n",
    "          If the LSTM is bidirectional, num_directions should be 2, else it should be 1.\n",
    "        - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "          containing the initial cell state for each element in the batch.\n",
    "\n",
    "          If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.\n",
    "\n",
    "\n",
    "    Outputs: output, (h_n, c_n)\n",
    "        - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\n",
    "          containing the output features `(h_t)` from the last layer of the LSTM,\n",
    "          for each `t`. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
    "          given as the input, the output will also be a packed sequence.\n",
    "\n",
    "          For the unpacked case, the directions can be separated\n",
    "          using ``output.view(seq_len, batch, num_directions, hidden_size)``,\n",
    "          with forward and backward being direction `0` and `1` respectively.\n",
    "          Similarly, the directions can be separated in the packed case.\n",
    "        - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "          containing the hidden state for `t = seq_len`.\n",
    "\n",
    "          Like *output*, the layers can be separated using\n",
    "          ``h_n.view(num_layers, num_directions, batch, hidden_size)`` and similarly for *c_n*.\n",
    "        - **c_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "          containing the cell state for `t = seq_len`.\n",
    "\n",
    "    Attributes:\n",
    "        weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
    "            `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.\n",
    "            Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`\n",
    "        weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
    "            `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`\n",
    "        bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
    "            `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
    "        bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
    "            `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
    "\n",
    "    .. note::\n",
    "        All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
    "        where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
    "\n",
    "    .. include:: ../cudnn_persistent_rnn.rst\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> rnn = nn.LSTM(10, 20, 2)\n",
    "        >>> input = torch.randn(5, 3, 10)\n",
    "        >>> h0 = torch.randn(2, 3, 20)\n",
    "        >>> c0 = torch.randn(2, 3, 20)\n",
    "        >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LSTM, self).__init__('LSTM', *args, **kwargs)\n",
    "\n",
    "    def check_forward_args(self, input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]):\n",
    "        self.check_input(input, batch_sizes)\n",
    "        expected_hidden_size = self.get_expected_hidden_size(input, batch_sizes)\n",
    "\n",
    "        self.check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                               'Expected hidden[0] size {}, got {}')\n",
    "        self.check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                               'Expected hidden[1] size {}, got {}')\n",
    "\n",
    "    def permute_hidden(self, hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        if permutation is None:\n",
    "            return hx\n",
    "        return apply_permutation(hx[0], permutation), apply_permutation(hx[1], permutation)\n",
    "\n",
    "    @overload\n",
    "    @torch._jit_internal._overload_method  # noqa: F811\n",
    "    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None\n",
    "                ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:  # noqa: F811\n",
    "        pass\n",
    "\n",
    "    @overload\n",
    "    @torch._jit_internal._overload_method  # noqa: F811\n",
    "    def forward(self, input: PackedSequence, hx: Optional[Tuple[Tensor, Tensor]] = None\n",
    "                ) -> Tuple[PackedSequence, Tuple[Tensor, Tensor]]:  # noqa: F811\n",
    "        pass\n",
    "\n",
    "    def forward(self, input, hx=None):  # noqa: F811\n",
    "        orig_input = input\n",
    "        # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "        if isinstance(orig_input, PackedSequence):\n",
    "            input, batch_sizes, sorted_indices, unsorted_indices = input\n",
    "            max_batch_size = batch_sizes[0]\n",
    "            max_batch_size = int(max_batch_size)\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "            sorted_indices = None\n",
    "            unsorted_indices = None\n",
    "\n",
    "        if hx is None:\n",
    "            num_directions = 2 if self.bidirectional else 1\n",
    "            zeros = torch.zeros(self.num_layers * num_directions,\n",
    "                                max_batch_size, self.hidden_size,\n",
    "                                dtype=input.dtype, device=input.device)\n",
    "            hx = (zeros, zeros)\n",
    "        else:\n",
    "            # Each batch of the hidden state should match the input sequence that\n",
    "            # the user believes he/she is passing in.\n",
    "            hx = self.permute_hidden(hx, sorted_indices)\n",
    "\n",
    "        self.check_forward_args(input, hx, batch_sizes)\n",
    "        if batch_sizes is None:\n",
    "            result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
    "                              self.dropout, self.training, self.bidirectional, self.batch_first)\n",
    "        else:\n",
    "            result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
    "                              self.num_layers, self.dropout, self.training, self.bidirectional)\n",
    "        output = result[0]\n",
    "        hidden = result[1:]\n",
    "        # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "        if isinstance(orig_input, PackedSequence):\n",
    "            output_packed = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n",
    "            return output_packed, self.permute_hidden(hidden, unsorted_indices)\n",
    "        else:\n",
    "            return output, self.permute_hidden(hidden, unsorted_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import _VF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"HR\"\n",
    "dataset_name = \"POLLUTION\"\n",
    "model_name = \"LSTM\"\n",
    "\n",
    "task_size = 50\n",
    "batch_size = 64\n",
    "output_dim = 1\n",
    "\n",
    "batch_size = 20\n",
    "horizon = 10\n",
    "meta_learning_rate = 10e-6\n",
    "learning_rate = 10e-5\n",
    "n_inner_iter = 1\n",
    "##test\n",
    "\n",
    "if dataset_name == \"HR\":\n",
    "    window_size = 32\n",
    "    input_dim = 13\n",
    "elif dataset_name == \"POLLUTION\":\n",
    "    window_size = 5\n",
    "    input_dim = 14\n",
    "\n",
    "model = LSTMModel( batch_size=batch_size, seq_len = window_size, input_dim = input_dim, n_layers = 2, hidden_dim = 120, output_dim =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedLSTMModel(LSTMModel):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ExtendedLSTMModel, self).__init__( *args, **kwargs)  \n",
    "    \n",
    "    def get_flat_weights(self):\n",
    "        \n",
    "        self._flat_weights = [(lambda wn: getattr(self, wn) if hasattr(self, wn) else None)(wn)] \n",
    "        return self._flat_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtendedLSTMModel( batch_size=batch_size, seq_len = window_size, input_dim = input_dim, n_layers = 2, hidden_dim = 120, output_dim =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-205ea8887ff8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-92f6229b6dab>\u001b[0m in \u001b[0;36mget_flat_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_flat_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wn' is not defined"
     ]
    }
   ],
   "source": [
    "model.get_flat_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.LSTM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LSTMModel, self).__init__( *args, **kwargs)  \n",
    "        \n",
    "    def forward(self, input, params = None, hx=None, embeddings = None):  # noqa: F811\n",
    "        \n",
    "            if params is None:\n",
    "                params = [(lambda wn: getattr(self, wn) if hasattr(self, wn) else None)(wn)] \n",
    "                \n",
    "            \n",
    "            orig_input = input\n",
    "            # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "            if isinstance(orig_input, PackedSequence):\n",
    "                input, batch_sizes, sorted_indices, unsorted_indices = input\n",
    "                max_batch_size = batch_sizes[0]\n",
    "                max_batch_size = int(max_batch_size)\n",
    "            else:\n",
    "                batch_sizes = None\n",
    "                max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "                sorted_indices = None\n",
    "                unsorted_indices = None\n",
    "\n",
    "            if hx is None:\n",
    "                num_directions = 2 if self.bidirectional else 1\n",
    "                zeros = torch.zeros(self.num_layers * num_directions,\n",
    "                                    max_batch_size, self.hidden_size,\n",
    "                                    dtype=input.dtype, device=input.device)\n",
    "                hx = (zeros, zeros)\n",
    "            else:\n",
    "                # Each batch of the hidden state should match the input sequence that\n",
    "                # the user believes he/she is passing in.\n",
    "                hx = self.permute_hidden(hx, sorted_indices)\n",
    "\n",
    "            self.check_forward_args(input, hx, batch_sizes)\n",
    "            if batch_sizes is None:\n",
    "                result = _VF.lstm(input, hx, params, bias, self.num_layers,\n",
    "                                  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
    "            else:\n",
    "                result = _VF.lstm(input, batch_sizes, hx, params, bias,\n",
    "                                  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
    "            output = result[0]\n",
    "            hidden = result[1:]\n",
    "            # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "            if isinstance(orig_input, PackedSequence):\n",
    "                output_packed = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n",
    "                return output_packed, self.permute_hidden(hidden, unsorted_indices)\n",
    "            else:\n",
    "                return output, self.permute_hidden(hidden, unsorted_indices)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        \n",
    "        self._flat_weights = [(lambda wn: getattr(self, wn) if hasattr(self, wn) else None)(wn) for wn in self._flat_weights_names]\n",
    "        return self._flat_weights\n",
    "    \n",
    "    def set_weights_names(self, names):\n",
    "        self._flat_weights_names = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1864, -0.1020, -0.0542, -0.2225, -0.1711, -0.2549,  0.2816, -0.0213,\n",
       "           0.1339, -0.2140],\n",
       "         [-0.0253, -0.0252, -0.2634, -0.2152, -0.2168, -0.1847,  0.2027, -0.1384,\n",
       "           0.0336, -0.1987],\n",
       "         [-0.2594,  0.0354,  0.2068, -0.0355, -0.0759,  0.1903,  0.2200, -0.0020,\n",
       "          -0.2667, -0.1872],\n",
       "         [ 0.2406,  0.1528,  0.0144,  0.2642, -0.0766, -0.2302,  0.1422,  0.1423,\n",
       "           0.1520,  0.1857],\n",
       "         [-0.2977, -0.2405, -0.1424, -0.2855,  0.3041, -0.0335,  0.2057, -0.1081,\n",
       "           0.0950, -0.1421],\n",
       "         [ 0.2610,  0.1224, -0.1570, -0.1070, -0.1861, -0.2650,  0.2597, -0.1921,\n",
       "           0.0733, -0.0582],\n",
       "         [ 0.0858,  0.2482,  0.3040,  0.1830,  0.0109, -0.0207, -0.0294, -0.0480,\n",
       "           0.2048, -0.0468],\n",
       "         [-0.2926, -0.3000, -0.1401, -0.0004, -0.0733, -0.1035,  0.1918,  0.1821,\n",
       "           0.0571,  0.2439],\n",
       "         [-0.1654, -0.2939, -0.0889, -0.0926, -0.0903,  0.0923, -0.0872,  0.0194,\n",
       "          -0.1670,  0.1744],\n",
       "         [-0.0084, -0.2083, -0.1262,  0.0921, -0.0190,  0.1156, -0.1515,  0.1938,\n",
       "          -0.0141,  0.1782],\n",
       "         [ 0.0177, -0.2367,  0.0817,  0.1148, -0.1651,  0.0121, -0.0133, -0.1612,\n",
       "          -0.0865,  0.1693],\n",
       "         [-0.0886,  0.0535,  0.0138, -0.2453,  0.2439, -0.2790,  0.2651,  0.1228,\n",
       "           0.2933,  0.0525],\n",
       "         [ 0.1115, -0.2294, -0.0272,  0.1370,  0.2306,  0.0266,  0.2022, -0.3063,\n",
       "           0.2558,  0.2904],\n",
       "         [ 0.1113,  0.0393, -0.3061, -0.0167,  0.2970, -0.1180,  0.1479,  0.0975,\n",
       "          -0.0527, -0.0178],\n",
       "         [ 0.1136,  0.2254,  0.2131, -0.0635, -0.2209,  0.0140, -0.2545,  0.0214,\n",
       "          -0.2985, -0.1590],\n",
       "         [ 0.2758, -0.0018,  0.0516,  0.0937,  0.1688,  0.1166,  0.1846,  0.0521,\n",
       "          -0.2089, -0.1738],\n",
       "         [-0.1738, -0.0602,  0.1088, -0.2066, -0.2023, -0.0083, -0.1362, -0.0295,\n",
       "          -0.2931,  0.1465],\n",
       "         [-0.0553, -0.1812,  0.1803, -0.2881,  0.1323,  0.2528, -0.1075, -0.0230,\n",
       "           0.1471,  0.0217],\n",
       "         [-0.0529,  0.1549,  0.2738,  0.0160,  0.0685, -0.1486, -0.0908, -0.1565,\n",
       "          -0.2161, -0.1379],\n",
       "         [ 0.1798, -0.1878, -0.2588, -0.2733,  0.2064, -0.0058,  0.1146,  0.2644,\n",
       "           0.1412, -0.1360],\n",
       "         [-0.1115, -0.2698, -0.1617,  0.3102, -0.2932, -0.1193, -0.0096, -0.2869,\n",
       "          -0.0132,  0.1325],\n",
       "         [-0.2317,  0.2409, -0.2077,  0.0426,  0.2614, -0.2923, -0.1443,  0.2790,\n",
       "           0.0129, -0.2129],\n",
       "         [-0.1903, -0.2646, -0.2590, -0.1868,  0.0196, -0.0368, -0.0611,  0.1969,\n",
       "          -0.3161,  0.3008],\n",
       "         [ 0.1678,  0.2270, -0.0413, -0.0751, -0.1875,  0.0542,  0.2750,  0.0878,\n",
       "           0.1976,  0.2462],\n",
       "         [-0.1075, -0.1913, -0.1848, -0.1088, -0.3107,  0.1971,  0.2703,  0.0425,\n",
       "          -0.0107,  0.2618],\n",
       "         [-0.2163, -0.1627,  0.2173, -0.2216, -0.0760, -0.3090,  0.2999, -0.1762,\n",
       "           0.2543,  0.0727],\n",
       "         [ 0.2493, -0.2349, -0.0928,  0.2807, -0.1184,  0.1500,  0.2560,  0.1175,\n",
       "           0.0831,  0.2017],\n",
       "         [ 0.2478, -0.1231, -0.1484,  0.0316, -0.3098, -0.1232, -0.2269,  0.2566,\n",
       "          -0.0863,  0.0074],\n",
       "         [ 0.1248,  0.1751,  0.2524, -0.2711,  0.2339,  0.0839, -0.0707, -0.0516,\n",
       "           0.1411, -0.3040],\n",
       "         [-0.0534, -0.1212,  0.0048,  0.1231, -0.0495,  0.0845, -0.2902,  0.0874,\n",
       "          -0.2919,  0.2860],\n",
       "         [ 0.2674, -0.0157, -0.3120, -0.0517,  0.0997, -0.1072,  0.3160, -0.0505,\n",
       "           0.2314, -0.2919],\n",
       "         [ 0.2962, -0.0213,  0.1779, -0.0019, -0.0548,  0.2146,  0.2511,  0.0379,\n",
       "           0.0610, -0.1010],\n",
       "         [-0.1053, -0.1111,  0.0706, -0.2319,  0.1768,  0.2057, -0.0312,  0.0244,\n",
       "           0.1629,  0.1633],\n",
       "         [ 0.1653, -0.3090,  0.2213, -0.0962, -0.0160, -0.2483,  0.2141,  0.1885,\n",
       "          -0.1497,  0.0639],\n",
       "         [-0.3152, -0.0510,  0.3098,  0.2689,  0.2440, -0.2703, -0.0948,  0.2984,\n",
       "           0.1907,  0.2368],\n",
       "         [-0.2489,  0.0770,  0.0759,  0.1001, -0.2518, -0.1438,  0.0496,  0.1480,\n",
       "           0.1048,  0.2422],\n",
       "         [ 0.3136, -0.2403, -0.1395, -0.2212, -0.1079,  0.1489, -0.1254,  0.1269,\n",
       "          -0.1319,  0.0104],\n",
       "         [ 0.0680,  0.0573, -0.0330,  0.1564, -0.0792,  0.0161,  0.2180,  0.2876,\n",
       "          -0.0542, -0.1633],\n",
       "         [-0.1256, -0.0238, -0.3081, -0.2575, -0.0884, -0.2917, -0.0214,  0.2269,\n",
       "          -0.3091,  0.2721],\n",
       "         [-0.0513,  0.1324, -0.1288, -0.2274,  0.0792, -0.2486,  0.0068, -0.0992,\n",
       "           0.2377, -0.0831]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2040,  0.1121, -0.1740, -0.0424,  0.0015,  0.2608, -0.0932, -0.0874,\n",
       "          -0.2591, -0.2918],\n",
       "         [-0.1769,  0.3115, -0.1625,  0.0756, -0.0751,  0.1160,  0.1478,  0.0258,\n",
       "           0.2911, -0.2443],\n",
       "         [ 0.0355, -0.1913, -0.1690,  0.0081,  0.0333,  0.2275, -0.2358,  0.1671,\n",
       "           0.2971, -0.2033],\n",
       "         [ 0.1334,  0.1964,  0.0122, -0.2065,  0.1181, -0.1678, -0.1905,  0.1475,\n",
       "           0.3155, -0.0130],\n",
       "         [-0.0464,  0.0527,  0.0570, -0.2814, -0.2317,  0.3114,  0.1129, -0.1924,\n",
       "           0.2283, -0.2812],\n",
       "         [-0.1443, -0.1038,  0.3124,  0.2418, -0.1566, -0.0152, -0.1898,  0.0371,\n",
       "          -0.1267,  0.1294],\n",
       "         [ 0.1147,  0.1785,  0.1296, -0.1229, -0.0906,  0.0832, -0.2014,  0.1105,\n",
       "          -0.3057,  0.2997],\n",
       "         [ 0.2894, -0.2022, -0.0003, -0.1388,  0.0049, -0.1132,  0.2409,  0.2546,\n",
       "          -0.1466,  0.1415],\n",
       "         [ 0.1886, -0.0244, -0.0594,  0.3076, -0.2853, -0.2609, -0.2314, -0.2494,\n",
       "          -0.0466,  0.1627],\n",
       "         [ 0.0477, -0.1234,  0.0882,  0.1488, -0.2432,  0.1579,  0.2347, -0.0125,\n",
       "           0.0418,  0.0010],\n",
       "         [ 0.1501, -0.0722,  0.1588,  0.0346, -0.3141,  0.2586,  0.2874,  0.1045,\n",
       "          -0.0272,  0.1156],\n",
       "         [ 0.2688, -0.0566, -0.1498, -0.0275,  0.0253,  0.1868,  0.2502, -0.2543,\n",
       "           0.2223, -0.2930],\n",
       "         [-0.2544,  0.0739, -0.1156,  0.0174,  0.0124, -0.2317,  0.2966,  0.3073,\n",
       "           0.1375, -0.1404],\n",
       "         [ 0.0817,  0.1133, -0.1309,  0.0791, -0.0264,  0.0754,  0.2694,  0.2175,\n",
       "          -0.2853, -0.0394],\n",
       "         [-0.0515,  0.0687, -0.0664,  0.1156, -0.2399, -0.2161, -0.2429,  0.2570,\n",
       "           0.0902,  0.2171],\n",
       "         [ 0.2111, -0.0089,  0.0036, -0.2189, -0.2483,  0.1022,  0.3077, -0.2996,\n",
       "           0.2040,  0.0707],\n",
       "         [ 0.2739,  0.0010, -0.1217, -0.0280, -0.0107,  0.3096, -0.1398,  0.1582,\n",
       "           0.0547, -0.0287],\n",
       "         [-0.0768,  0.1786,  0.1332,  0.1015,  0.2741,  0.0106, -0.1209, -0.3130,\n",
       "           0.2632,  0.0893],\n",
       "         [ 0.0533,  0.1775, -0.1601,  0.0259,  0.2482,  0.1267, -0.2025,  0.1414,\n",
       "           0.2785,  0.0274],\n",
       "         [ 0.2164,  0.0638,  0.2795,  0.1684,  0.3161, -0.0965,  0.1662, -0.0687,\n",
       "           0.3018, -0.1367],\n",
       "         [ 0.2771,  0.2407,  0.2662,  0.2859, -0.1215, -0.1939,  0.0333,  0.2570,\n",
       "           0.1269,  0.1151],\n",
       "         [-0.0811, -0.1307, -0.0738,  0.2729,  0.0487, -0.0168,  0.0726,  0.1350,\n",
       "          -0.0751, -0.2508],\n",
       "         [ 0.3055,  0.1488, -0.2395, -0.1697,  0.0237, -0.2411, -0.2456, -0.3046,\n",
       "          -0.1053, -0.0599],\n",
       "         [-0.0748, -0.0635, -0.0082, -0.2811, -0.1715,  0.0594,  0.2364,  0.1019,\n",
       "           0.1427, -0.0959],\n",
       "         [-0.1425,  0.2771, -0.0308,  0.0686,  0.3028,  0.2578,  0.0449,  0.0831,\n",
       "           0.3077,  0.0023],\n",
       "         [ 0.1934, -0.2823,  0.2065,  0.1349,  0.1673, -0.2490,  0.0098, -0.0987,\n",
       "           0.2552, -0.0610],\n",
       "         [-0.2970,  0.3152, -0.2923, -0.2376, -0.0024, -0.1822,  0.1211, -0.0996,\n",
       "          -0.1258,  0.2454],\n",
       "         [ 0.1980,  0.2980,  0.0169, -0.0374, -0.2860,  0.3152, -0.1805, -0.2762,\n",
       "           0.0640, -0.0005],\n",
       "         [ 0.1948, -0.1618, -0.0026,  0.0119,  0.2044, -0.3126,  0.0667,  0.1509,\n",
       "          -0.0121, -0.0039],\n",
       "         [ 0.2402,  0.2810, -0.1670,  0.3017,  0.0900,  0.0327, -0.2776,  0.2811,\n",
       "           0.2071, -0.2832],\n",
       "         [-0.3137, -0.0166,  0.1396, -0.2962,  0.1302, -0.2037,  0.0769, -0.0387,\n",
       "          -0.2957,  0.0825],\n",
       "         [-0.2072,  0.2324,  0.1523, -0.1075,  0.2435,  0.1069,  0.0152, -0.0646,\n",
       "           0.1204,  0.1146],\n",
       "         [-0.2625,  0.2074,  0.1527,  0.0787,  0.2332,  0.1715,  0.0497, -0.1804,\n",
       "           0.0437,  0.1782],\n",
       "         [ 0.2630,  0.1397,  0.1939, -0.0090, -0.0794,  0.0927, -0.1177, -0.3079,\n",
       "          -0.2447,  0.2112],\n",
       "         [ 0.0519, -0.1488, -0.1795, -0.0069,  0.0525,  0.0308, -0.3130,  0.1550,\n",
       "           0.0838, -0.0598],\n",
       "         [ 0.2056,  0.0898,  0.1892, -0.1476,  0.0881,  0.0535,  0.2651, -0.1605,\n",
       "          -0.0689,  0.1116],\n",
       "         [-0.0870, -0.1020, -0.0662,  0.0409, -0.3047,  0.0504, -0.3015,  0.3072,\n",
       "          -0.3148,  0.2448],\n",
       "         [ 0.2640,  0.2769, -0.1313, -0.0834,  0.0466,  0.0589, -0.3108,  0.3016,\n",
       "          -0.1766,  0.2585],\n",
       "         [-0.0209,  0.2874, -0.0630,  0.3142,  0.2356,  0.1354,  0.1755, -0.1045,\n",
       "           0.1488, -0.0694],\n",
       "         [ 0.1379,  0.1471, -0.2628, -0.1104, -0.2050, -0.2692, -0.0615, -0.1318,\n",
       "           0.1117, -0.0515]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2318, -0.2484, -0.2058,  0.2744,  0.2842, -0.2594,  0.2883,  0.0589,\n",
       "         -0.1881,  0.2426,  0.1486, -0.0230,  0.2615,  0.2205,  0.2329, -0.3119,\n",
       "          0.0051, -0.2638,  0.3125, -0.1219,  0.1960, -0.1402, -0.0584, -0.2668,\n",
       "          0.2994,  0.2444,  0.3096, -0.2577,  0.2149, -0.1327,  0.0628,  0.0514,\n",
       "         -0.2846, -0.0200, -0.0572, -0.1460, -0.0670, -0.1471, -0.0258, -0.1983],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1457, -0.1875,  0.1078, -0.1024, -0.2095, -0.1560,  0.2384,  0.0731,\n",
       "          0.0767,  0.1283, -0.2357,  0.1600,  0.2393, -0.0288,  0.1533,  0.1521,\n",
       "          0.0931, -0.1501, -0.0014,  0.3156, -0.1991, -0.2113,  0.2478, -0.1612,\n",
       "         -0.1359, -0.0073, -0.2009, -0.0518, -0.1583,  0.3143,  0.2235,  0.3103,\n",
       "         -0.1853,  0.0580,  0.0241, -0.0352, -0.2973,  0.2805,  0.1683,  0.2422],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = OrderedDict(model.named_parameters())\n",
    "\n",
    "parameters_names = parameters.keys()\n",
    "model.set_weights_names(parameters_names)\n",
    "model.get_flat_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0775, -0.3154,  0.1546, -0.2460,  0.1831,  0.2236,  0.0089,  0.3077,\n",
       "          0.1711, -0.0082],\n",
       "        [ 0.2781, -0.1481,  0.1846,  0.2998, -0.2993,  0.2662, -0.2189, -0.0665,\n",
       "         -0.0350,  0.0188],\n",
       "        [-0.1296, -0.0244,  0.2397, -0.2587, -0.1435,  0.0125, -0.0644, -0.1076,\n",
       "         -0.2335, -0.2059],\n",
       "        [-0.2578,  0.1926, -0.0452,  0.2372,  0.0795,  0.1121, -0.1481,  0.0334,\n",
       "         -0.1250, -0.1756],\n",
       "        [ 0.1847, -0.1864, -0.1630,  0.1873,  0.2444,  0.0997, -0.2940,  0.1329,\n",
       "         -0.0631, -0.2121],\n",
       "        [ 0.1892, -0.0198,  0.0732,  0.0626,  0.1806, -0.2081, -0.0498,  0.1543,\n",
       "         -0.1753, -0.1434],\n",
       "        [ 0.0576,  0.0841,  0.2888,  0.0020,  0.0088,  0.3000, -0.3147, -0.0568,\n",
       "          0.1999, -0.2270],\n",
       "        [-0.0452,  0.2069,  0.1122, -0.2621,  0.1303, -0.0151,  0.2186,  0.1120,\n",
       "         -0.0502, -0.2213],\n",
       "        [ 0.0010, -0.3004,  0.2452, -0.1986, -0.2756,  0.2356, -0.1022, -0.2562,\n",
       "         -0.1978,  0.1479],\n",
       "        [ 0.1527,  0.3012, -0.2690, -0.1596, -0.0634, -0.2958,  0.1210,  0.1272,\n",
       "         -0.2366, -0.2024],\n",
       "        [ 0.1874, -0.2755, -0.2789, -0.2975,  0.0866,  0.0303, -0.0282, -0.2708,\n",
       "          0.0357,  0.1702],\n",
       "        [-0.1712,  0.2854,  0.1517,  0.0072,  0.0697, -0.1524,  0.0939,  0.1516,\n",
       "         -0.1608, -0.2803],\n",
       "        [ 0.3154,  0.3148,  0.0306, -0.0713,  0.0254,  0.1423, -0.2462,  0.0561,\n",
       "          0.0083, -0.1837],\n",
       "        [ 0.1003, -0.1424, -0.2687,  0.3140,  0.2430, -0.0544,  0.1011,  0.1123,\n",
       "          0.2374,  0.3092],\n",
       "        [ 0.0519,  0.1649, -0.1324, -0.2858, -0.2396, -0.2149,  0.0751,  0.3138,\n",
       "          0.1539,  0.0192],\n",
       "        [-0.0733,  0.2184, -0.0606, -0.1977, -0.1502,  0.1090,  0.0551,  0.0787,\n",
       "         -0.3128, -0.0775],\n",
       "        [ 0.0564,  0.2080,  0.0582, -0.2491, -0.2332,  0.1712,  0.2130,  0.2023,\n",
       "          0.1160, -0.1237],\n",
       "        [-0.0758,  0.0584,  0.0983,  0.1060,  0.2670, -0.1245,  0.1988, -0.0329,\n",
       "         -0.1700,  0.2686],\n",
       "        [ 0.2507, -0.1037, -0.2297, -0.0760, -0.3111, -0.2512,  0.0348,  0.2724,\n",
       "         -0.2398,  0.0693],\n",
       "        [-0.2274, -0.2842,  0.1781,  0.2588, -0.1788, -0.2470,  0.1549, -0.1780,\n",
       "         -0.0524, -0.0245],\n",
       "        [ 0.2775, -0.0599, -0.0430,  0.2660, -0.1546, -0.1010, -0.2462, -0.2068,\n",
       "          0.3077,  0.2587],\n",
       "        [-0.0272, -0.1409, -0.0372, -0.1478, -0.0140,  0.2011,  0.0797,  0.3037,\n",
       "          0.0145, -0.0334],\n",
       "        [ 0.1087,  0.1161,  0.2640, -0.0735, -0.3029, -0.2228, -0.0967, -0.3046,\n",
       "         -0.2140,  0.1168],\n",
       "        [ 0.3136, -0.0388, -0.0333,  0.1859, -0.2443, -0.0065, -0.0863, -0.1038,\n",
       "          0.2378, -0.0622],\n",
       "        [-0.1849,  0.1349,  0.0492, -0.2812,  0.2947, -0.0918,  0.0887,  0.2026,\n",
       "          0.1243,  0.0422],\n",
       "        [-0.2129, -0.1997, -0.2115,  0.2246,  0.0128,  0.0961, -0.1342, -0.1393,\n",
       "         -0.1005, -0.2332],\n",
       "        [-0.2523,  0.1967, -0.1542,  0.2243,  0.2376, -0.1955, -0.0139,  0.1850,\n",
       "          0.1671,  0.2195],\n",
       "        [-0.0175, -0.1120, -0.2383, -0.1121,  0.0140, -0.2561, -0.2111,  0.0065,\n",
       "         -0.0025, -0.0215],\n",
       "        [-0.2084, -0.2483,  0.2169,  0.0368,  0.1571, -0.1428,  0.1690, -0.0719,\n",
       "         -0.2769, -0.2464],\n",
       "        [ 0.3026, -0.2693, -0.2961,  0.0392,  0.1854, -0.1325,  0.1922, -0.1286,\n",
       "          0.0311, -0.0228],\n",
       "        [ 0.1016, -0.2086,  0.2018, -0.0568, -0.1480, -0.3025,  0.2980, -0.2849,\n",
       "          0.0933,  0.1647],\n",
       "        [ 0.1032,  0.1345,  0.1900, -0.1533,  0.2404,  0.0474, -0.0757,  0.3141,\n",
       "          0.2700,  0.1993],\n",
       "        [-0.2285,  0.1309,  0.0957, -0.3034, -0.1849, -0.1790,  0.0856,  0.0676,\n",
       "         -0.1653,  0.3073],\n",
       "        [-0.2394,  0.2075, -0.0335, -0.2413,  0.0286, -0.2185, -0.2269, -0.2986,\n",
       "         -0.1335, -0.0090],\n",
       "        [-0.0887, -0.2255, -0.0663, -0.1346,  0.1036,  0.1681, -0.1689,  0.0642,\n",
       "          0.0662,  0.0151],\n",
       "        [ 0.3088, -0.2460,  0.0789, -0.1048,  0.2079,  0.0466,  0.3126,  0.1517,\n",
       "          0.0340, -0.3091],\n",
       "        [-0.1399,  0.0836, -0.2638,  0.0831,  0.2202,  0.2343, -0.1235,  0.2078,\n",
       "          0.2375,  0.2109],\n",
       "        [-0.1438, -0.2521,  0.2577,  0.2989,  0.0841,  0.0177, -0.2462, -0.0269,\n",
       "         -0.1939, -0.1355],\n",
       "        [ 0.2563, -0.0445,  0.2605, -0.2086,  0.1591,  0.0007,  0.2607, -0.0324,\n",
       "         -0.1339,  0.1119],\n",
       "        [-0.3092, -0.1570, -0.0014,  0.0788,  0.0301,  0.2435, -0.1629, -0.0476,\n",
       "         -0.0854,  0.2638]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.get_flat_weights()\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = OrderedDict(model.named_parameters())\n",
    "type(a.get(\"weight_ih_l0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-694c4530bb4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..utils.rnn import PackedSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(self, loss, params):\n",
    "    \"\"\"Apply one step of gradient descent on the loss function `loss`,\n",
    "    with step-size `self._fast_lr`, and returns the updated parameters.\n",
    "    \"\"\"\n",
    "    create_graph = not self._first_order\n",
    "    grads = torch.autograd.grad(loss, params.values(),\n",
    "                                create_graph=create_graph, allow_unused=True)\n",
    "    for (name, param), grad in zip(params.items(), grads):\n",
    "        if self._inner_loop_grad_clip > 0 and grad is not None:\n",
    "            grad = grad.clamp(min=-self._inner_loop_grad_clip,\n",
    "                              max=self._inner_loop_grad_clip)\n",
    "        if grad is not None:\n",
    "            params[name] = param - self._fast_lr * grad\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step(self, adapted_params_list, embeddings_list, val_tasks,\n",
    "         is_training):\n",
    "    for optimizer in self._optimizers:\n",
    "        optimizer.zero_grad()\n",
    "    post_update_losses = []\n",
    "\n",
    "    for adapted_params, embeddings, task in zip(\n",
    "            adapted_params_list, embeddings_list, val_tasks):\n",
    "        preds = self._model(task, params=adapted_params,\n",
    "                            embeddings=embeddings)\n",
    "        loss = self._loss_func(preds, task.y)\n",
    "        post_update_losses.append(loss)\n",
    "        self._update_measurements(task, loss, preds)\n",
    "\n",
    "    mean_loss = torch.mean(torch.stack(post_update_losses))\n",
    "    if is_training:\n",
    "        mean_loss.backward()\n",
    "        if self._alternating:\n",
    "            self._optimizers[self._alternating_index].step()\n",
    "            self._alternating_count += 1\n",
    "            if self._alternating_count % self._alternating_schedules[self._alternating_index] == 0:\n",
    "                self._alternating_index = (1 - self._alternating_index)\n",
    "                self._alternating_count = 0\n",
    "        else:\n",
    "            self._optimizers[0].step()\n",
    "            if len(self._optimizers) > 1:\n",
    "                if self._embedding_grad_clip > 0:\n",
    "                    _grad_norm = clip_grad_norm_(self._embedding_model.parameters(), self._embedding_grad_clip)\n",
    "                else:\n",
    "                    _grad_norm = get_grad_norm(self._embedding_model.parameters())\n",
    "                    # grad_norm\n",
    "                    self._grads_mean.append(_grad_norm)\n",
    "                    self._optimizers[1].step()\n",
    "\n",
    "    measurements = self._pop_measurements()\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(self, train_tasks):\n",
    "    adapted_params = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    for task in train_tasks:\n",
    "        params = self._model.param_dict\n",
    "        embeddings = None\n",
    "        if self._embedding_model:\n",
    "            embeddings = self._embedding_model(task)\n",
    "        for i in range(self._num_updates):\n",
    "            preds = self._model(task, params=params, embeddings=embeddings)\n",
    "            loss = self._loss_func(preds, task.y)\n",
    "            params = self.update_params(loss, params=params)\n",
    "            if i == 0:\n",
    "                self._update_measurements(task, loss, preds)\n",
    "        adapted_params.append(params)\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    measurements = self._pop_measurements()\n",
    "    return measurements, adapted_params, embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad_norm(parameters, norm_type=2):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "    total_norm = total_norm ** (1. / norm_type)\n",
    "\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomLSTM(nn.LSTM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomLSTM, self).__init__( *args, **kwargs)  \n",
    "        \n",
    "    def forward(self, input, params = None, hx=None, embeddings = None):  # noqa: F811\n",
    "        \n",
    "            if params is None:\n",
    "                params = [(lambda wn: getattr(self, wn) if hasattr(self, wn) else None)(wn)] \n",
    "                \n",
    "            \n",
    "            orig_input = input\n",
    "            # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "            if isinstance(orig_input, PackedSequence):\n",
    "                input, batch_sizes, sorted_indices, unsorted_indices = input\n",
    "                max_batch_size = batch_sizes[0]\n",
    "                max_batch_size = int(max_batch_size)\n",
    "            else:\n",
    "                batch_sizes = None\n",
    "                max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "                sorted_indices = None\n",
    "                unsorted_indices = None\n",
    "\n",
    "            if hx is None:\n",
    "                num_directions = 2 if self.bidirectional else 1\n",
    "                zeros = torch.zeros(self.num_layers * num_directions,\n",
    "                                    max_batch_size, self.hidden_size,\n",
    "                                    dtype=input.dtype, device=input.device)\n",
    "                hx = (zeros, zeros)\n",
    "            else:\n",
    "                # Each batch of the hidden state should match the input sequence that\n",
    "                # the user believes he/she is passing in.\n",
    "                hx = self.permute_hidden(hx, sorted_indices)\n",
    "\n",
    "            self.check_forward_args(input, hx, batch_sizes)\n",
    "            if batch_sizes is None:\n",
    "                result = _VF.lstm(input, hx, params, bias, self.num_layers,\n",
    "                                  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
    "            else:\n",
    "                result = _VF.lstm(input, batch_sizes, hx, params, bias,\n",
    "                                  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
    "            output = result[0]\n",
    "            hidden = result[1:]\n",
    "            # xxx: isinstance check needs to be in conditional for TorchScript to compile\n",
    "            if isinstance(orig_input, PackedSequence):\n",
    "                output_packed = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n",
    "                return output_packed, self.permute_hidden(hidden, unsorted_indices)\n",
    "            else:\n",
    "                return output, self.permute_hidden(hidden, unsorted_indices)\n",
    "\n",
    "    def get_flat_weights(self):\n",
    "        \n",
    "        self._flat_weights = [(lambda wn: getattr(self, wn) if hasattr(self, wn) else None)(wn) for wn in self._flat_weights_names]\n",
    "        return self._flat_weights\n",
    "    \n",
    "    def set_weights_names(self, names):\n",
    "        self._flat_weights_names = names\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, seq_len, input_dim, n_layers, hidden_dim, output_dim, lin_hidden_dim = 100):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        #self.lstm = nn.CustomLSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        #self.linear = nn.Linear(hidden_dim, output_dim)#\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.hidden = self.init_hidden()\n",
    "        self.input_dim = input_dim\n",
    "        self.features = torch.nn.Sequential(OrderedDict([\n",
    "            (\"lstm\",  CustomLSTM(input_dim, hidden_dim, n_layers, batch_first=True)),\n",
    "            (\"linear\", nn.Linear(hidden_dim, output_dim))]))\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x, params = None):\n",
    "        \n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pickle.load(  open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "train_data_ML = pickle.load( open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "validation_data = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "validation_data_ML = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "test_data_ML = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel( batch_size=batch_size, seq_len = window_size, input_dim = input_dim, n_layers = 2, hidden_dim = 120, output_dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.67037964e+00, -1.68076986e+00, -1.66151987e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.64887883e-01,\n",
       "          8.37460334e-01, -2.67225548e+00,  1.30311394e+00,\n",
       "         -3.46121894e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.51704004e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.01413885e-01,\n",
       "          7.39253183e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.55730232e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.37256021e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.64887883e-01,\n",
       "          6.41046032e-01, -2.67225548e+00,  1.30311394e+00,\n",
       "         -2.04036598e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.22808038e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -1.74465889e-01,\n",
       "          6.41046032e-01, -2.95883332e+00,  1.30311394e+00,\n",
       "         -1.13644937e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.08360054e+00,\n",
       "          1.36758687e+00, -4.92141225e-01, -2.37939887e-01,\n",
       "          5.42838882e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.32532755e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -1.51704004e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.01413885e-01,\n",
       "          7.39253183e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.55730232e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.37256021e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.64887883e-01,\n",
       "          6.41046032e-01, -2.67225548e+00,  1.30311394e+00,\n",
       "         -2.04036598e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.22808038e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -1.74465889e-01,\n",
       "          6.41046032e-01, -2.95883332e+00,  1.30311394e+00,\n",
       "         -1.13644937e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.08360054e+00,\n",
       "          1.36758687e+00, -4.92141225e-01, -2.37939887e-01,\n",
       "          5.42838882e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.32532755e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -9.39120714e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.01413885e-01,\n",
       "          4.44631731e-01, -2.57672953e+00,  1.30311394e+00,\n",
       "          6.71383859e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -1.37256021e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -3.64887883e-01,\n",
       "          6.41046032e-01, -2.67225548e+00,  1.30311394e+00,\n",
       "         -2.04036598e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.22808038e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -1.74465889e-01,\n",
       "          6.41046032e-01, -2.95883332e+00,  1.30311394e+00,\n",
       "         -1.13644937e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.08360054e+00,\n",
       "          1.36758687e+00, -4.92141225e-01, -2.37939887e-01,\n",
       "          5.42838882e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.32532755e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -9.39120714e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.01413885e-01,\n",
       "          4.44631731e-01, -2.57672953e+00,  1.30311394e+00,\n",
       "          6.71383859e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.94640883e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          1.57530047e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -1.22808038e+00,\n",
       "          1.36758687e+00, -5.08999427e-01, -1.74465889e-01,\n",
       "          6.41046032e-01, -2.95883332e+00,  1.30311394e+00,\n",
       "         -1.13644937e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -1.08360054e+00,\n",
       "          1.36758687e+00, -4.92141225e-01, -2.37939887e-01,\n",
       "          5.42838882e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.32532755e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -9.39120714e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.01413885e-01,\n",
       "          4.44631731e-01, -2.57672953e+00,  1.30311394e+00,\n",
       "          6.71383859e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.94640883e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          1.57530047e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -6.50161052e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.09223681e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -1.08360054e+00,\n",
       "          1.36758687e+00, -4.92141225e-01, -2.37939887e-01,\n",
       "          5.42838882e-01, -2.76778143e+00,  1.30311394e+00,\n",
       "         -2.32532755e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -9.39120714e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.01413885e-01,\n",
       "          4.44631731e-01, -2.57672953e+00,  1.30311394e+00,\n",
       "          6.71383859e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.94640883e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          1.57530047e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -6.50161052e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.09223681e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -5.05681222e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.99615343e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -9.39120714e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.01413885e-01,\n",
       "          4.44631731e-01, -2.57672953e+00,  1.30311394e+00,\n",
       "          6.71383859e-02, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.94640883e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          1.57530047e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -6.50161052e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.09223681e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -5.05681222e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.99615343e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -3.61201391e-01,\n",
       "          1.36758687e+00, -4.92141225e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.38567764e+00,  1.30311394e+00,\n",
       "          3.90007004e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -7.94640883e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          1.57530047e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -6.50161052e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.09223681e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -5.05681222e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.99615343e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -3.61201391e-01,\n",
       "          1.36758687e+00, -4.92141225e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.38567764e+00,  1.30311394e+00,\n",
       "          3.90007004e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -2.16721560e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.29015169e+00,  1.30311394e+00,\n",
       "          5.06101087e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -6.50161052e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.09223681e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -5.05681222e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.99615343e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -3.61201391e-01,\n",
       "          1.36758687e+00, -4.92141225e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.38567764e+00,  1.30311394e+00,\n",
       "          3.90007004e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -2.16721560e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.29015169e+00,  1.30311394e+00,\n",
       "          5.06101087e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.22417292e-02,\n",
       "          1.36758687e+00, -4.58424821e-01, -4.91835879e-01,\n",
       "          4.44631731e-01, -2.09909980e+00,  1.30311394e+00,\n",
       "          5.96492748e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -5.05681222e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -3.49019384e-01,\n",
       "          4.44631731e-01, -2.48120358e+00,  1.30311394e+00,\n",
       "          2.99615343e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -3.61201391e-01,\n",
       "          1.36758687e+00, -4.92141225e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.38567764e+00,  1.30311394e+00,\n",
       "          3.90007004e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -2.16721560e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.29015169e+00,  1.30311394e+00,\n",
       "          5.06101087e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.22417292e-02,\n",
       "          1.36758687e+00, -4.58424821e-01, -4.91835879e-01,\n",
       "          4.44631731e-01, -2.09909980e+00,  1.30311394e+00,\n",
       "          5.96492748e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00,  7.22381016e-02,\n",
       "          1.36758687e+00, -4.75283023e-01, -5.39441378e-01,\n",
       "          2.48217429e-01, -2.09909980e+00,  1.30311394e+00,\n",
       "          6.86884410e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]],\n",
       "\n",
       "       [[-1.67037964e+00, -1.68076986e+00, -3.61201391e-01,\n",
       "          1.36758687e+00, -4.92141225e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.38567764e+00,  1.30311394e+00,\n",
       "          3.90007004e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -2.16721560e-01,\n",
       "          1.36758687e+00, -4.75283023e-01, -4.60098880e-01,\n",
       "          4.44631731e-01, -2.29015169e+00,  1.30311394e+00,\n",
       "          5.06101087e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00, -7.22417292e-02,\n",
       "          1.36758687e+00, -4.58424821e-01, -4.91835879e-01,\n",
       "          4.44631731e-01, -2.09909980e+00,  1.30311394e+00,\n",
       "          5.96492748e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00,  7.22381016e-02,\n",
       "          1.36758687e+00, -4.75283023e-01, -5.39441378e-01,\n",
       "          2.48217429e-01, -2.09909980e+00,  1.30311394e+00,\n",
       "          6.86884410e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01],\n",
       "        [-1.67037964e+00, -1.68076986e+00,  2.16717932e-01,\n",
       "          1.36758687e+00, -4.58424821e-01, -5.71178377e-01,\n",
       "          2.48217429e-01, -1.90804790e+00,  1.30311394e+00,\n",
       "          7.77276071e-01, -2.93730136e-03, -3.10294429e-03,\n",
       "         -6.42364991e-02, -5.37382028e-01]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_data.x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('features.lstm.weight_ih_l0',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.0117,  0.0500,  0.0021,  ...,  0.0700, -0.0419,  0.0026],\n",
       "                      [-0.0002,  0.0516, -0.0010,  ...,  0.0790, -0.0809,  0.0886],\n",
       "                      [ 0.0638, -0.0348,  0.0266,  ...,  0.0812,  0.0534, -0.0601],\n",
       "                      ...,\n",
       "                      [ 0.0348, -0.0727, -0.0067,  ..., -0.0425,  0.0332,  0.0271],\n",
       "                      [-0.0599,  0.0051, -0.0008,  ..., -0.0709,  0.0529, -0.0716],\n",
       "                      [-0.0043, -0.0236, -0.0127,  ..., -0.0078,  0.0680, -0.0131]],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.weight_hh_l0',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.0751, -0.0654, -0.0578,  ...,  0.0625,  0.0340, -0.0246],\n",
       "                      [ 0.0883, -0.0169,  0.0262,  ...,  0.0713,  0.0012,  0.0349],\n",
       "                      [ 0.0651, -0.0593, -0.0605,  ...,  0.0824, -0.0490, -0.0233],\n",
       "                      ...,\n",
       "                      [-0.0622, -0.0861,  0.0513,  ..., -0.0107, -0.0164, -0.0283],\n",
       "                      [ 0.0154, -0.0509, -0.0018,  ...,  0.0489, -0.0033, -0.0008],\n",
       "                      [ 0.0264,  0.0630, -0.0810,  ...,  0.0823,  0.0379,  0.0527]],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.bias_ih_l0',\n",
       "              Parameter containing:\n",
       "              tensor([-0.0091,  0.0152, -0.0793, -0.0320, -0.0873, -0.0776, -0.0726,  0.0271,\n",
       "                       0.0049,  0.0658,  0.0027,  0.0574,  0.0853,  0.0172, -0.0169, -0.0431,\n",
       "                       0.0536, -0.0815,  0.0815, -0.0601, -0.0305,  0.0118,  0.0806,  0.0836,\n",
       "                      -0.0210, -0.0046, -0.0774, -0.0733, -0.0311,  0.0786, -0.0204,  0.0271,\n",
       "                       0.0266,  0.0595, -0.0165, -0.0394, -0.0224,  0.0125, -0.0638, -0.0074,\n",
       "                       0.0643,  0.0157,  0.0426,  0.0080, -0.0478, -0.0768,  0.0777, -0.0358,\n",
       "                       0.0447,  0.0635, -0.0333, -0.0365,  0.0034, -0.0637, -0.0893, -0.0853,\n",
       "                       0.0003,  0.0790,  0.0603,  0.0104,  0.0092,  0.0793,  0.0311, -0.0192,\n",
       "                       0.0596,  0.0709,  0.0235,  0.0434, -0.0528, -0.0220, -0.0794,  0.0568,\n",
       "                      -0.0178, -0.0476, -0.0004, -0.0309,  0.0557,  0.0140, -0.0060,  0.0634,\n",
       "                      -0.0602, -0.0194,  0.0020, -0.0561, -0.0663, -0.0426, -0.0561, -0.0549,\n",
       "                      -0.0508, -0.0130, -0.0275,  0.0757,  0.0724, -0.0353,  0.0050, -0.0302,\n",
       "                       0.0850,  0.0397,  0.0644, -0.0160,  0.0475,  0.0170,  0.0714, -0.0575,\n",
       "                       0.0268,  0.0026, -0.0778, -0.0504, -0.0245, -0.0576,  0.0274,  0.0025,\n",
       "                       0.0723,  0.0508, -0.0122, -0.0905, -0.0227,  0.0487, -0.0282,  0.0212,\n",
       "                       0.0583, -0.0589, -0.0468,  0.0722,  0.0797,  0.0114, -0.0543, -0.0008,\n",
       "                       0.0083, -0.0382, -0.0913,  0.0375,  0.0626,  0.0432,  0.0323, -0.0601,\n",
       "                      -0.0417, -0.0486,  0.0688, -0.0789,  0.0529,  0.0643, -0.0640, -0.0655,\n",
       "                       0.0465,  0.0229,  0.0745, -0.0678,  0.0292,  0.0262, -0.0756,  0.0748,\n",
       "                      -0.0129, -0.0378, -0.0200, -0.0348,  0.0698, -0.0236, -0.0377,  0.0745,\n",
       "                      -0.0469,  0.0563, -0.0508, -0.0830,  0.0132, -0.0077, -0.0539,  0.0845,\n",
       "                      -0.0614, -0.0287, -0.0095, -0.0602, -0.0355, -0.0526,  0.0282, -0.0491,\n",
       "                      -0.0346, -0.0199,  0.0368,  0.0259,  0.0652, -0.0812,  0.0863, -0.0708,\n",
       "                      -0.0787,  0.0460,  0.0016, -0.0388,  0.0021, -0.0733,  0.0652,  0.0606,\n",
       "                       0.0106,  0.0102, -0.0796, -0.0600,  0.0564,  0.0278, -0.0191, -0.0078,\n",
       "                      -0.0268, -0.0901, -0.0776, -0.0150, -0.0441, -0.0617,  0.0519, -0.0117,\n",
       "                      -0.0527, -0.0340, -0.0553,  0.0022, -0.0655, -0.0771, -0.0451, -0.0744,\n",
       "                      -0.0380,  0.0467,  0.0439,  0.0222,  0.0891,  0.0103, -0.0507, -0.0535,\n",
       "                      -0.0828,  0.0312,  0.0230,  0.0859, -0.0142, -0.0167, -0.0700,  0.0084,\n",
       "                       0.0843,  0.0831,  0.0070, -0.0652, -0.0719, -0.0584, -0.0486, -0.0213,\n",
       "                      -0.0612, -0.0027,  0.0325,  0.0042, -0.0316, -0.0370,  0.0705,  0.0379,\n",
       "                       0.0787, -0.0403,  0.0581, -0.0703,  0.0626, -0.0222,  0.0861, -0.0262,\n",
       "                       0.0582,  0.0056, -0.0133,  0.0048,  0.0867, -0.0380, -0.0352,  0.0126,\n",
       "                      -0.0366, -0.0132,  0.0807, -0.0501,  0.0519, -0.0114, -0.0088,  0.0162,\n",
       "                      -0.0887,  0.0522, -0.0493, -0.0294, -0.0450,  0.0655,  0.0491,  0.0065,\n",
       "                      -0.0596,  0.0850, -0.0701,  0.0224, -0.0796, -0.0519, -0.0642,  0.0191,\n",
       "                       0.0810,  0.0168,  0.0644,  0.0326,  0.0005, -0.0703,  0.0606,  0.0495,\n",
       "                      -0.0507, -0.0858,  0.0301, -0.0662, -0.0847,  0.0491, -0.0226, -0.0399,\n",
       "                       0.0216, -0.0111,  0.0031, -0.0826,  0.0311, -0.0566,  0.0505,  0.0117,\n",
       "                      -0.0661, -0.0206, -0.0284, -0.0066,  0.0283,  0.0396,  0.0192,  0.0799,\n",
       "                       0.0044, -0.0644, -0.0483, -0.0318, -0.0860, -0.0581, -0.0308,  0.0609,\n",
       "                       0.0581,  0.0456, -0.0421,  0.0065, -0.0071,  0.0057, -0.0344, -0.0827,\n",
       "                       0.0711, -0.0597,  0.0639, -0.0529, -0.0552, -0.0237,  0.0623,  0.0462,\n",
       "                      -0.0829,  0.0159, -0.0082, -0.0290, -0.0351,  0.0036,  0.0566, -0.0634,\n",
       "                      -0.0738, -0.0672, -0.0363, -0.0839,  0.0701,  0.0417, -0.0436,  0.0480,\n",
       "                      -0.0892,  0.0269, -0.0500, -0.0172,  0.0455, -0.0867, -0.0675, -0.0318,\n",
       "                      -0.0259, -0.0050, -0.0594,  0.0631,  0.0592,  0.0777,  0.0768, -0.0872,\n",
       "                       0.0201, -0.0222, -0.0686, -0.0753, -0.0384, -0.0871, -0.0732, -0.0359,\n",
       "                       0.0813, -0.0897,  0.0496,  0.0145,  0.0450,  0.0876, -0.0012,  0.0410,\n",
       "                       0.0201,  0.0234, -0.0413, -0.0848,  0.0843,  0.0127, -0.0270, -0.0144,\n",
       "                       0.0202,  0.0159,  0.0666, -0.0752, -0.0772, -0.0122, -0.0863, -0.0840,\n",
       "                       0.0810,  0.0089, -0.0571,  0.0368,  0.0875, -0.0612, -0.0540, -0.0457,\n",
       "                       0.0424, -0.0291,  0.0026, -0.0770, -0.0683,  0.0542, -0.0559,  0.0277,\n",
       "                      -0.0766,  0.0720, -0.0440,  0.0624,  0.0199, -0.0453,  0.0612,  0.0167,\n",
       "                      -0.0169,  0.0475,  0.0831,  0.0420, -0.0375, -0.0432,  0.0305,  0.0131,\n",
       "                      -0.0844,  0.0906, -0.0886, -0.0541, -0.0883, -0.0382,  0.0541,  0.0346,\n",
       "                      -0.0891, -0.0352, -0.0537, -0.0515,  0.0409, -0.0547,  0.0317,  0.0133,\n",
       "                      -0.0188,  0.0383, -0.0618,  0.0821, -0.0735,  0.0613,  0.0324,  0.0532,\n",
       "                      -0.0040,  0.0445, -0.0013,  0.0250, -0.0227, -0.0289,  0.0705,  0.0782,\n",
       "                      -0.0854,  0.0327,  0.0251,  0.0055, -0.0609, -0.0564, -0.0428,  0.0477],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.bias_hh_l0',\n",
       "              Parameter containing:\n",
       "              tensor([-0.0869, -0.0202, -0.0686, -0.0267, -0.0661, -0.0911,  0.0189,  0.0279,\n",
       "                       0.0004, -0.0604, -0.0670, -0.0590, -0.0117,  0.0229,  0.0721, -0.0069,\n",
       "                       0.0216, -0.0376,  0.0705,  0.0795, -0.0646, -0.0832, -0.0010, -0.0125,\n",
       "                       0.0700, -0.0693,  0.0513, -0.0114, -0.0816, -0.0190,  0.0129,  0.0056,\n",
       "                       0.0253, -0.0100,  0.0192,  0.0522,  0.0241, -0.0087,  0.0604, -0.0318,\n",
       "                       0.0808, -0.0150,  0.0479, -0.0501, -0.0190, -0.0688,  0.0515, -0.0842,\n",
       "                       0.0055, -0.0372, -0.0262, -0.0800, -0.0592, -0.0031, -0.0456, -0.0185,\n",
       "                      -0.0351, -0.0334, -0.0539,  0.0793,  0.0251, -0.0092,  0.0541,  0.0023,\n",
       "                      -0.0241,  0.0774, -0.0006,  0.0177,  0.0649, -0.0829, -0.0681,  0.0405,\n",
       "                      -0.0495, -0.0743,  0.0323, -0.0497,  0.0462, -0.0613,  0.0367, -0.0900,\n",
       "                       0.0668,  0.0141, -0.0083,  0.0503, -0.0749, -0.0724, -0.0819, -0.0555,\n",
       "                      -0.0540,  0.0328,  0.0205, -0.0773, -0.0623, -0.0566, -0.0378,  0.0048,\n",
       "                       0.0434, -0.0867, -0.0651,  0.0378, -0.0162, -0.0350, -0.0790, -0.0145,\n",
       "                      -0.0888, -0.0684,  0.0178,  0.0060, -0.0712, -0.0619,  0.0288,  0.0520,\n",
       "                      -0.0443, -0.0020, -0.0843,  0.0806, -0.0777, -0.0651,  0.0217,  0.0405,\n",
       "                      -0.0213,  0.0507,  0.0156, -0.0678,  0.0627, -0.0415, -0.0752,  0.0355,\n",
       "                      -0.0239,  0.0476, -0.0366,  0.0846, -0.0227, -0.0231,  0.0219,  0.0866,\n",
       "                      -0.0758, -0.0180,  0.0283,  0.0797, -0.0741, -0.0573,  0.0471, -0.0450,\n",
       "                       0.0553,  0.0043,  0.0008, -0.0451,  0.0124,  0.0191,  0.0549, -0.0594,\n",
       "                       0.0425, -0.0102, -0.0159, -0.0191,  0.0221, -0.0595,  0.0102, -0.0385,\n",
       "                      -0.0076, -0.0882,  0.0812,  0.0205, -0.0831,  0.0481, -0.0019, -0.0598,\n",
       "                       0.0800,  0.0816, -0.0577, -0.0332, -0.0530, -0.0850, -0.0579,  0.0708,\n",
       "                      -0.0553, -0.0576, -0.0660, -0.0782,  0.0381, -0.0019, -0.0633,  0.0166,\n",
       "                      -0.0830,  0.0350, -0.0360,  0.0087, -0.0120,  0.0088, -0.0716, -0.0675,\n",
       "                       0.0117, -0.0346,  0.0469, -0.0031, -0.0241,  0.0142, -0.0438,  0.0232,\n",
       "                       0.0897,  0.0805,  0.0669, -0.0667, -0.0312, -0.0372, -0.0656, -0.0612,\n",
       "                       0.0613, -0.0809,  0.0133,  0.0816, -0.0444, -0.0419, -0.0279,  0.0232,\n",
       "                       0.0838, -0.0360, -0.0021, -0.0327, -0.0245, -0.0669,  0.0549,  0.0160,\n",
       "                       0.0458, -0.0056,  0.0592,  0.0730,  0.0578,  0.0186, -0.0094,  0.0372,\n",
       "                       0.0167,  0.0810, -0.0549, -0.0781, -0.0641,  0.0297,  0.0592, -0.0580,\n",
       "                       0.0090, -0.0438, -0.0189, -0.0852,  0.0631,  0.0448, -0.0569, -0.0131,\n",
       "                       0.0395, -0.0052, -0.0008, -0.0271,  0.0033, -0.0020,  0.0707,  0.0852,\n",
       "                      -0.0509,  0.0007,  0.0828,  0.0612,  0.0553,  0.0759, -0.0042, -0.0435,\n",
       "                      -0.0749, -0.0871, -0.0361,  0.0858, -0.0625,  0.0154, -0.0892, -0.0112,\n",
       "                       0.0042,  0.0224,  0.0279,  0.0743, -0.0506,  0.0494, -0.0910, -0.0823,\n",
       "                       0.0552,  0.0752,  0.0536, -0.0025, -0.0837,  0.0902, -0.0193, -0.0805,\n",
       "                      -0.0900, -0.0382, -0.0475, -0.0470, -0.0066,  0.0408, -0.0651, -0.0433,\n",
       "                       0.0851, -0.0578,  0.0728,  0.0172, -0.0479, -0.0307,  0.0295,  0.0737,\n",
       "                      -0.0737, -0.0533,  0.0838,  0.0813,  0.0125,  0.0453, -0.0369,  0.0656,\n",
       "                       0.0538,  0.0375, -0.0588,  0.0621,  0.0867, -0.0354,  0.0735,  0.0886,\n",
       "                      -0.0604, -0.0870, -0.0047, -0.0681, -0.0481, -0.0681,  0.0476, -0.0105,\n",
       "                      -0.0846, -0.0615,  0.0812,  0.0018,  0.0414,  0.0144,  0.0474, -0.0752,\n",
       "                       0.0175, -0.0433, -0.0524, -0.0647, -0.0655,  0.0408, -0.0792, -0.0499,\n",
       "                       0.0047,  0.0117, -0.0544,  0.0284, -0.0047,  0.0552,  0.0774, -0.0515,\n",
       "                       0.0782, -0.0797,  0.0431, -0.0179, -0.0307, -0.0385, -0.0488, -0.0451,\n",
       "                       0.0624,  0.0571, -0.0769,  0.0614,  0.0872, -0.0390, -0.0532, -0.0603,\n",
       "                      -0.0436,  0.0447,  0.0126,  0.0641, -0.0160, -0.0193,  0.0088, -0.0729,\n",
       "                      -0.0363, -0.0587, -0.0054,  0.0083,  0.0444,  0.0789,  0.0767,  0.0484,\n",
       "                      -0.0846, -0.0151,  0.0111, -0.0490,  0.0803,  0.0141,  0.0689,  0.0907,\n",
       "                       0.0081,  0.0543, -0.0265,  0.0883, -0.0229, -0.0008, -0.0636,  0.0410,\n",
       "                      -0.0853,  0.0309,  0.0425,  0.0644, -0.0278, -0.0095,  0.0525,  0.0730,\n",
       "                      -0.0151, -0.0116,  0.0704, -0.0367,  0.0434,  0.0592, -0.0230,  0.0714,\n",
       "                      -0.0905, -0.0078, -0.0141, -0.0831, -0.0226,  0.0685, -0.0727,  0.0902,\n",
       "                       0.0676,  0.0542, -0.0084, -0.0148,  0.0678, -0.0554, -0.0782, -0.0720,\n",
       "                       0.0404, -0.0303, -0.0133,  0.0174, -0.0257, -0.0782, -0.0082, -0.0823,\n",
       "                      -0.0140, -0.0784,  0.0603, -0.0719,  0.0165, -0.0379, -0.0293, -0.0470,\n",
       "                      -0.0317,  0.0164,  0.0816,  0.0019,  0.0415,  0.0477, -0.0597, -0.0489,\n",
       "                       0.0142, -0.0546,  0.0288, -0.0769, -0.0203,  0.0840, -0.0389,  0.0688,\n",
       "                      -0.0388,  0.0557, -0.0255, -0.0274, -0.0045, -0.0190,  0.0777,  0.0196,\n",
       "                      -0.0721, -0.0701, -0.0350, -0.0534,  0.0776, -0.0222,  0.0190,  0.0741],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.weight_ih_l1',\n",
       "              Parameter containing:\n",
       "              tensor([[-0.0889,  0.0162, -0.0801,  ...,  0.0326,  0.0542,  0.0819],\n",
       "                      [-0.0847, -0.0493, -0.0870,  ..., -0.0802,  0.0646,  0.0356],\n",
       "                      [-0.0146,  0.0001, -0.0290,  ...,  0.0003, -0.0330,  0.0537],\n",
       "                      ...,\n",
       "                      [-0.0550,  0.0015, -0.0648,  ..., -0.0724,  0.0685,  0.0791],\n",
       "                      [-0.0453, -0.0522, -0.0898,  ..., -0.0206, -0.0091, -0.0023],\n",
       "                      [ 0.0181, -0.0869, -0.0178,  ..., -0.0028,  0.0622, -0.0660]],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.weight_hh_l1',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.0762, -0.0338, -0.0126,  ...,  0.0751, -0.0034, -0.0384],\n",
       "                      [-0.0843, -0.0535, -0.0239,  ...,  0.0590, -0.0035, -0.0230],\n",
       "                      [-0.0264, -0.0501, -0.0384,  ..., -0.0509,  0.0621,  0.0818],\n",
       "                      ...,\n",
       "                      [ 0.0076,  0.0494, -0.0899,  ...,  0.0780,  0.0417,  0.0861],\n",
       "                      [ 0.0203, -0.0689,  0.0372,  ...,  0.0344,  0.0280, -0.0699],\n",
       "                      [ 0.0338, -0.0338, -0.0807,  ...,  0.0808,  0.0649, -0.0271]],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.bias_ih_l1',\n",
       "              Parameter containing:\n",
       "              tensor([ 0.0903,  0.0128,  0.0827, -0.0699,  0.0478, -0.0034, -0.0245, -0.0725,\n",
       "                       0.0772, -0.0124, -0.0649,  0.0796, -0.0874, -0.0755, -0.0352, -0.0026,\n",
       "                       0.0831,  0.0426, -0.0461, -0.0412, -0.0887,  0.0384,  0.0444,  0.0143,\n",
       "                       0.0530, -0.0483, -0.0316,  0.0635, -0.0486,  0.0390, -0.0128, -0.0247,\n",
       "                      -0.0275, -0.0752,  0.0775, -0.0213, -0.0296,  0.0154,  0.0499,  0.0556,\n",
       "                       0.0194,  0.0131,  0.0606, -0.0476, -0.0782,  0.0396,  0.0080,  0.0603,\n",
       "                       0.0079, -0.0632,  0.0362, -0.0840, -0.0659,  0.0661, -0.0005,  0.0092,\n",
       "                       0.0028,  0.0274,  0.0126,  0.0493,  0.0729,  0.0340,  0.0329,  0.0659,\n",
       "                      -0.0213,  0.0284, -0.0173, -0.0731,  0.0076,  0.0800, -0.0091, -0.0003,\n",
       "                      -0.0710, -0.0600,  0.0146, -0.0638, -0.0443, -0.0256,  0.0493, -0.0508,\n",
       "                       0.0860, -0.0400,  0.0255,  0.0583, -0.0829,  0.0379,  0.0054, -0.0498,\n",
       "                      -0.0695, -0.0577,  0.0694, -0.0763, -0.0765,  0.0556, -0.0057,  0.0430,\n",
       "                      -0.0162,  0.0699, -0.0212, -0.0844, -0.0711, -0.0451, -0.0212,  0.0535,\n",
       "                       0.0444, -0.0797, -0.0688, -0.0091,  0.0820,  0.0107,  0.0267,  0.0273,\n",
       "                       0.0219,  0.0242, -0.0555,  0.0356, -0.0778, -0.0287,  0.0193,  0.0317,\n",
       "                       0.0518, -0.0310, -0.0005,  0.0636,  0.0859, -0.0473, -0.0142,  0.0752,\n",
       "                       0.0885, -0.0432, -0.0287, -0.0869, -0.0740,  0.0831, -0.0269, -0.0890,\n",
       "                       0.0351,  0.0045, -0.0174, -0.0849,  0.0495,  0.0776, -0.0310,  0.0448,\n",
       "                       0.0813,  0.0351, -0.0115,  0.0362,  0.0016,  0.0784, -0.0016,  0.0831,\n",
       "                      -0.0012, -0.0776,  0.0584, -0.0792,  0.0123, -0.0445,  0.0278,  0.0253,\n",
       "                       0.0147, -0.0125,  0.0370,  0.0401,  0.0321,  0.0088, -0.0766, -0.0313,\n",
       "                      -0.0518, -0.0209, -0.0907,  0.0815,  0.0100, -0.0164,  0.0900, -0.0749,\n",
       "                       0.0337,  0.0504,  0.0878,  0.0207, -0.0100,  0.0155, -0.0674, -0.0111,\n",
       "                       0.0441,  0.0156,  0.0502, -0.0150, -0.0719,  0.0519, -0.0406,  0.0041,\n",
       "                      -0.0082,  0.0702,  0.0553, -0.0641, -0.0828, -0.0089,  0.0007, -0.0499,\n",
       "                       0.0857, -0.0872,  0.0682, -0.0535,  0.0155, -0.0746, -0.0010, -0.0632,\n",
       "                       0.0829,  0.0856,  0.0906,  0.0418,  0.0322,  0.0787,  0.0697,  0.0315,\n",
       "                      -0.0572, -0.0840, -0.0813,  0.0821, -0.0827, -0.0240, -0.0614,  0.0169,\n",
       "                      -0.0640,  0.0166,  0.0036,  0.0100, -0.0793, -0.0289, -0.0415, -0.0502,\n",
       "                      -0.0006,  0.0763,  0.0788, -0.0777, -0.0699, -0.0477, -0.0616, -0.0362,\n",
       "                      -0.0028,  0.0727,  0.0527,  0.0431,  0.0537,  0.0701, -0.0816,  0.0447,\n",
       "                       0.0225,  0.0016, -0.0421,  0.0851, -0.0204, -0.0626, -0.0344, -0.0333,\n",
       "                       0.0756, -0.0692,  0.0680, -0.0111,  0.0798,  0.0736,  0.0016,  0.0468,\n",
       "                      -0.0328,  0.0437,  0.0031,  0.0656,  0.0423, -0.0275,  0.0845, -0.0566,\n",
       "                       0.0110, -0.0354,  0.0639,  0.0533,  0.0528,  0.0503,  0.0382,  0.0081,\n",
       "                      -0.0357,  0.0729, -0.0821, -0.0239,  0.0639, -0.0257, -0.0724,  0.0884,\n",
       "                       0.0267, -0.0670, -0.0583,  0.0051,  0.0552, -0.0529,  0.0612,  0.0781,\n",
       "                      -0.0260, -0.0848, -0.0620, -0.0596, -0.0843, -0.0711, -0.0255,  0.0035,\n",
       "                      -0.0780, -0.0742,  0.0036, -0.0884, -0.0904,  0.0343, -0.0179, -0.0657,\n",
       "                       0.0043, -0.0007,  0.0887,  0.0490,  0.0607, -0.0236,  0.0659, -0.0340,\n",
       "                       0.0108, -0.0844,  0.0099,  0.0090,  0.0672, -0.0492, -0.0755, -0.0610,\n",
       "                      -0.0353, -0.0082,  0.0668,  0.0512,  0.0194,  0.0027,  0.0573,  0.0785,\n",
       "                       0.0261,  0.0380, -0.0023, -0.0566, -0.0860,  0.0066,  0.0857, -0.0351,\n",
       "                       0.0361, -0.0281,  0.0357, -0.0026, -0.0508,  0.0098, -0.0245, -0.0064,\n",
       "                      -0.0595,  0.0330, -0.0550, -0.0405, -0.0423, -0.0080,  0.0129,  0.0826,\n",
       "                      -0.0486,  0.0534,  0.0542, -0.0320,  0.0569, -0.0358,  0.0798,  0.0033,\n",
       "                      -0.0048,  0.0089, -0.0716,  0.0036,  0.0587, -0.0378,  0.0754, -0.0005,\n",
       "                       0.0271,  0.0741,  0.0569, -0.0825, -0.0067,  0.0379, -0.0380, -0.0149,\n",
       "                      -0.0508, -0.0699, -0.0009,  0.0418, -0.0860,  0.0558, -0.0399, -0.0097,\n",
       "                      -0.0164,  0.0094, -0.0025,  0.0623,  0.0051,  0.0483,  0.0373, -0.0662,\n",
       "                       0.0673, -0.0705,  0.0305, -0.0276,  0.0770,  0.0403, -0.0092, -0.0094,\n",
       "                      -0.0833, -0.0112,  0.0214, -0.0503,  0.0831, -0.0769, -0.0493,  0.0227,\n",
       "                      -0.0336, -0.0212,  0.0520,  0.0229, -0.0724,  0.0587, -0.0032, -0.0273,\n",
       "                      -0.0476,  0.0343,  0.0661,  0.0363, -0.0452, -0.0210,  0.0819, -0.0890,\n",
       "                       0.0456, -0.0325,  0.0059, -0.0682,  0.0509, -0.0898, -0.0409,  0.0312,\n",
       "                      -0.0124, -0.0796,  0.0703,  0.0025, -0.0909, -0.0822,  0.0353, -0.0843,\n",
       "                      -0.0428, -0.0700,  0.0211,  0.0876, -0.0807, -0.0591, -0.0578,  0.0738,\n",
       "                      -0.0181,  0.0013,  0.0255, -0.0052, -0.0005,  0.0019, -0.0309, -0.0733,\n",
       "                      -0.0572, -0.0612,  0.0417,  0.0450, -0.0871, -0.0425,  0.0165,  0.0523,\n",
       "                       0.0458,  0.0206, -0.0161,  0.0044, -0.0664,  0.0305,  0.0681,  0.0331],\n",
       "                     requires_grad=True)),\n",
       "             ('features.lstm.bias_hh_l1',\n",
       "              Parameter containing:\n",
       "              tensor([ 2.8972e-02, -7.5059e-02,  3.2915e-02,  5.5273e-02, -6.6384e-02,\n",
       "                      -2.6834e-02, -1.9005e-02,  8.2106e-02,  8.1549e-02,  7.2246e-02,\n",
       "                       8.2610e-02,  3.1338e-02,  6.1669e-02,  3.8612e-02,  2.2845e-02,\n",
       "                      -8.7599e-02,  2.9101e-02, -4.5875e-02, -2.2923e-02, -1.8446e-02,\n",
       "                      -1.4059e-02,  4.6484e-02,  8.2008e-02,  4.4654e-02,  6.9660e-03,\n",
       "                      -5.3693e-02, -1.9947e-02,  4.8476e-02,  6.7558e-02, -8.4655e-02,\n",
       "                      -4.1041e-02, -6.3748e-02, -3.3734e-02,  2.4536e-02,  1.7865e-02,\n",
       "                      -3.4729e-02,  5.6626e-02,  5.0401e-02,  5.1023e-03, -8.8716e-02,\n",
       "                      -3.7024e-02, -6.2906e-02,  5.2382e-02, -8.6940e-03, -8.8052e-02,\n",
       "                       4.2311e-02,  1.1244e-02, -7.9603e-02, -2.9338e-02,  4.3044e-02,\n",
       "                      -5.3806e-03, -4.5104e-02, -8.6288e-02, -2.2496e-02,  6.6508e-02,\n",
       "                       2.9592e-02,  1.0707e-02,  4.1307e-02, -4.8396e-02,  6.9715e-02,\n",
       "                      -1.5729e-02,  3.2719e-02,  1.5566e-02, -2.6542e-02,  4.0787e-02,\n",
       "                       9.0948e-02, -2.9176e-02,  8.4665e-02,  5.7868e-02,  6.3867e-02,\n",
       "                      -6.6773e-03,  7.0459e-02,  3.9208e-02, -7.3758e-02,  8.3040e-03,\n",
       "                       5.8020e-02,  4.6913e-02, -9.2185e-03,  8.8933e-02,  8.2812e-02,\n",
       "                      -2.1310e-02,  4.8081e-02,  8.5332e-02,  2.5712e-02, -4.0294e-02,\n",
       "                      -2.2554e-02,  7.2414e-02, -4.0130e-02, -7.1260e-02, -8.1627e-02,\n",
       "                       6.1198e-03,  8.1302e-02, -8.4134e-02, -1.7101e-02, -6.3306e-02,\n",
       "                      -6.3341e-02, -3.2739e-02,  1.7451e-02, -3.6684e-02, -7.1285e-02,\n",
       "                       8.6440e-02, -2.5999e-02,  2.4396e-03, -3.3847e-02, -1.5017e-02,\n",
       "                       6.6468e-02,  6.7485e-02, -1.7447e-02, -4.3116e-02,  8.2214e-02,\n",
       "                      -5.0340e-02,  6.6853e-03, -3.3652e-02, -1.9608e-02,  1.5793e-02,\n",
       "                       3.2837e-02,  1.5894e-02,  1.4176e-02,  7.9011e-02,  7.0162e-02,\n",
       "                      -5.1396e-02, -8.8651e-02,  7.7310e-02, -7.9494e-02,  6.9446e-02,\n",
       "                      -2.5144e-02,  4.2306e-02, -1.8871e-02, -1.1625e-02,  3.9205e-02,\n",
       "                       3.5090e-02, -2.3925e-02, -5.0302e-02,  5.9756e-02, -2.6737e-02,\n",
       "                       3.2412e-02, -3.8694e-02,  6.1963e-02, -8.2053e-02, -8.6074e-02,\n",
       "                       4.5456e-02, -7.8847e-02, -5.3475e-02,  8.9540e-02,  5.6825e-02,\n",
       "                      -3.1627e-02, -6.9585e-04,  6.4401e-02,  2.5750e-02, -4.2405e-02,\n",
       "                      -2.9642e-02, -2.0936e-02, -8.6058e-02,  7.8527e-02, -6.9250e-02,\n",
       "                       1.2775e-02, -8.0752e-03, -5.2798e-02,  5.7473e-02, -4.1073e-02,\n",
       "                       4.4631e-02, -6.8571e-02,  8.6159e-02, -6.9449e-02,  7.5508e-02,\n",
       "                       7.6651e-02,  2.3288e-03,  4.6848e-03,  2.9066e-02, -6.7371e-02,\n",
       "                       3.8591e-02, -5.3842e-02, -3.9480e-02, -4.9400e-02, -5.4034e-03,\n",
       "                       7.6937e-03,  4.9098e-02, -5.3078e-02,  4.0668e-02, -2.1634e-02,\n",
       "                      -6.1849e-02,  7.4886e-02, -5.6683e-02, -9.8903e-03,  7.1134e-02,\n",
       "                      -3.8306e-02, -4.8895e-02, -1.7476e-02, -1.5149e-02,  1.6319e-02,\n",
       "                       5.5137e-03, -7.9341e-02, -4.4181e-02,  6.1167e-02, -3.2343e-02,\n",
       "                       4.1053e-02, -4.6011e-02, -7.7770e-02, -1.9599e-02,  3.8851e-02,\n",
       "                       5.9980e-02,  2.5685e-03,  6.0261e-02,  6.4558e-02,  2.5681e-02,\n",
       "                       2.9342e-02,  1.3283e-02, -6.5496e-02,  7.1200e-02,  5.6005e-02,\n",
       "                      -2.9484e-02, -6.6508e-02,  6.5577e-02,  7.5778e-02, -5.7944e-02,\n",
       "                       1.8380e-02,  2.8164e-02, -7.0109e-02, -1.1632e-02, -5.8251e-02,\n",
       "                      -7.3060e-02,  8.0840e-02,  4.7346e-02,  6.8781e-02,  1.8502e-02,\n",
       "                       1.7171e-02, -2.9929e-02,  4.8116e-02,  1.2024e-02,  8.2641e-02,\n",
       "                      -2.5112e-02,  2.7272e-03, -2.8400e-02, -6.9078e-02, -5.5414e-02,\n",
       "                      -5.6988e-02,  6.6838e-02, -2.2912e-02,  2.3409e-02,  8.3946e-02,\n",
       "                       3.8303e-02,  5.5025e-02,  2.8971e-02, -4.6636e-02, -5.8425e-02,\n",
       "                      -7.4745e-02, -2.8680e-02, -4.4011e-02, -4.3977e-02, -1.8572e-02,\n",
       "                      -7.4992e-03,  6.6207e-02,  8.5082e-02,  1.5459e-02, -4.4347e-02,\n",
       "                      -9.1353e-03,  5.0042e-02, -1.6224e-02,  2.9858e-02, -6.7372e-02,\n",
       "                      -1.2895e-02,  2.3684e-02,  1.5731e-02, -3.7811e-02,  6.4101e-02,\n",
       "                      -8.5861e-02, -5.3501e-02, -6.1270e-02,  7.5868e-02, -8.5153e-02,\n",
       "                      -8.7771e-03,  2.7667e-02,  7.6658e-02,  6.1545e-02, -5.2441e-02,\n",
       "                      -5.6883e-02, -8.6228e-02,  6.9145e-02, -3.4355e-02,  2.3904e-02,\n",
       "                      -8.5452e-02,  5.7158e-02, -1.9300e-02,  5.7066e-02,  7.3244e-02,\n",
       "                      -1.1477e-02,  3.9541e-02,  2.7321e-02,  8.3679e-03,  5.6454e-02,\n",
       "                       4.4908e-02, -2.6323e-02, -8.3615e-02,  4.9817e-02, -2.4638e-03,\n",
       "                       6.8901e-02, -2.8677e-02,  2.7261e-03, -3.4609e-02,  4.4504e-02,\n",
       "                      -1.4408e-02,  2.4559e-02, -7.7258e-02, -5.3619e-02,  3.0385e-02,\n",
       "                       1.9882e-02,  4.4960e-03,  5.0332e-02,  3.4554e-02,  2.1591e-02,\n",
       "                      -8.4679e-02,  1.5497e-02,  5.5164e-02,  5.8999e-03, -7.5973e-02,\n",
       "                       8.9117e-02, -4.3579e-02,  4.9421e-02,  9.4727e-03, -6.2444e-02,\n",
       "                       2.6421e-02,  8.2296e-02,  4.2471e-02,  8.5213e-02,  3.1660e-02,\n",
       "                      -8.0027e-02, -4.0718e-02, -1.4915e-02, -1.1715e-02,  4.8265e-05,\n",
       "                       1.0979e-02,  1.0884e-02, -4.5726e-02, -5.0802e-02, -7.7880e-02,\n",
       "                      -4.5359e-02,  6.6603e-02,  8.0271e-02, -8.8263e-02, -3.3082e-02,\n",
       "                       4.6203e-02,  7.6626e-02, -8.1790e-02,  5.2374e-02, -6.6845e-02,\n",
       "                      -2.1438e-02, -3.5175e-02,  7.7253e-02, -2.0677e-02, -2.2827e-02,\n",
       "                       8.4969e-02, -3.1297e-02,  8.2118e-02, -6.2940e-02, -7.6489e-02,\n",
       "                      -5.5012e-02, -6.8554e-02, -9.0688e-02,  8.0245e-02,  7.7623e-02,\n",
       "                      -5.9769e-03,  1.9612e-02, -6.1466e-02, -7.6262e-02,  3.1039e-03,\n",
       "                       5.6001e-02, -1.8298e-02, -4.0191e-02,  9.3545e-03, -7.6067e-02,\n",
       "                      -8.6706e-02, -5.7372e-03,  1.8934e-02, -2.9800e-02,  7.7738e-03,\n",
       "                       5.8590e-02, -7.8362e-02, -5.6723e-02, -6.6749e-02, -8.0165e-02,\n",
       "                      -1.6945e-03,  4.6976e-02, -5.8727e-02,  3.7423e-02,  3.4473e-02,\n",
       "                       7.8435e-02, -2.3314e-02,  7.1177e-02,  1.9704e-02, -6.0860e-02,\n",
       "                       1.0729e-02,  7.3016e-02, -4.3674e-02, -4.5458e-03,  7.1777e-02,\n",
       "                      -6.7966e-02, -4.9269e-02, -6.3094e-02,  3.6826e-02,  5.8123e-02,\n",
       "                       7.1424e-03, -7.1543e-02, -3.2221e-02, -9.1025e-02, -8.5365e-02,\n",
       "                      -4.1001e-02,  7.8515e-02, -1.9401e-02, -7.8962e-02, -5.7349e-02,\n",
       "                       5.9700e-02,  8.2975e-02, -3.0289e-02,  6.2804e-02,  8.7166e-02,\n",
       "                       4.4902e-02, -5.7262e-02,  6.5014e-02, -4.1329e-02, -2.4090e-02,\n",
       "                       6.3150e-02, -1.0183e-02,  2.8451e-03, -8.6603e-02, -7.5023e-02,\n",
       "                      -5.7606e-02,  6.9097e-02, -1.9272e-02, -6.3263e-02, -8.7541e-02,\n",
       "                       1.0776e-02,  4.1401e-02, -8.9634e-02, -6.9320e-02, -2.4163e-02,\n",
       "                      -6.0263e-02,  4.3269e-02,  6.9744e-02,  7.8722e-02,  9.1259e-02,\n",
       "                       5.4340e-02, -4.3658e-03,  5.0143e-02,  2.2485e-02, -5.0850e-02,\n",
       "                      -8.3584e-02,  2.4456e-02,  5.9257e-02, -1.6344e-02, -8.7101e-02,\n",
       "                       1.4017e-02, -4.5950e-02,  1.8461e-02,  2.1187e-02,  4.4055e-02,\n",
       "                      -6.6686e-02,  7.2644e-02, -4.2049e-02,  4.9509e-02,  5.0433e-02,\n",
       "                       7.2747e-02, -4.8424e-02, -1.8524e-02, -5.9943e-02, -6.6348e-02,\n",
       "                       3.7628e-02, -8.5219e-02,  6.7578e-02, -5.9692e-02,  1.1833e-02,\n",
       "                       8.2774e-02,  2.7347e-02, -6.4227e-02,  4.3999e-02, -3.5341e-02,\n",
       "                      -4.7912e-02,  2.8160e-02, -4.7271e-03,  4.9825e-02,  6.5165e-02],\n",
       "                     requires_grad=True)),\n",
       "             ('features.linear.weight',\n",
       "              Parameter containing:\n",
       "              tensor([[ 0.0830,  0.0757,  0.0397,  0.0384, -0.0270, -0.0850,  0.0391,  0.0733,\n",
       "                        0.0162, -0.0488, -0.0665, -0.0681,  0.0388, -0.0323,  0.0069,  0.0904,\n",
       "                        0.0351, -0.0796,  0.0169,  0.0016, -0.0479,  0.0703, -0.0395,  0.0703,\n",
       "                       -0.0804,  0.0793,  0.0146,  0.0342,  0.0572,  0.0465, -0.0808,  0.0334,\n",
       "                        0.0387,  0.0657,  0.0699, -0.0184, -0.0160, -0.0668,  0.0256, -0.0709,\n",
       "                        0.0804,  0.0515, -0.0522,  0.0008, -0.0264, -0.0420,  0.0461, -0.0121,\n",
       "                       -0.0609, -0.0360, -0.0838, -0.0435, -0.0402,  0.0877,  0.0572, -0.0655,\n",
       "                       -0.0579,  0.0740, -0.0498,  0.0742,  0.0596, -0.0469, -0.0168,  0.0308,\n",
       "                       -0.0428,  0.0567,  0.0289,  0.0147, -0.0831, -0.0720,  0.0091, -0.0582,\n",
       "                        0.0626,  0.0775, -0.0703, -0.0712,  0.0152,  0.0091, -0.0350, -0.0047,\n",
       "                        0.0526, -0.0694,  0.0199,  0.0176,  0.0076,  0.0069,  0.0189, -0.0660,\n",
       "                        0.0514, -0.0830,  0.0229, -0.0102,  0.0557, -0.0458,  0.0808, -0.0686,\n",
       "                       -0.0475, -0.0130, -0.0025, -0.0596, -0.0548,  0.0505,  0.0219, -0.0420,\n",
       "                        0.0842, -0.0793, -0.0863, -0.0861, -0.0057, -0.0384,  0.0807,  0.0352,\n",
       "                        0.0108,  0.0272, -0.0211, -0.0099, -0.0700,  0.0357, -0.0681, -0.0233]],\n",
       "                     requires_grad=True)),\n",
       "             ('features.linear.bias',\n",
       "              Parameter containing:\n",
       "              tensor([0.0207], requires_grad=True))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrderedDict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lstm', CustomLSTM(14, 120, num_layers=2, batch_first=True)),\n",
       " ('linear', Linear(in_features=120, out_features=1, bias=True))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.features.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.0117,  0.0500,  0.0021,  ...,  0.0700, -0.0419,  0.0026],\n",
      "        [-0.0002,  0.0516, -0.0010,  ...,  0.0790, -0.0809,  0.0886],\n",
      "        [ 0.0638, -0.0348,  0.0266,  ...,  0.0812,  0.0534, -0.0601],\n",
      "        ...,\n",
      "        [ 0.0348, -0.0727, -0.0067,  ..., -0.0425,  0.0332,  0.0271],\n",
      "        [-0.0599,  0.0051, -0.0008,  ..., -0.0709,  0.0529, -0.0716],\n",
      "        [-0.0043, -0.0236, -0.0127,  ..., -0.0078,  0.0680, -0.0131]],\n",
      "       requires_grad=True)), ('weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.0751, -0.0654, -0.0578,  ...,  0.0625,  0.0340, -0.0246],\n",
      "        [ 0.0883, -0.0169,  0.0262,  ...,  0.0713,  0.0012,  0.0349],\n",
      "        [ 0.0651, -0.0593, -0.0605,  ...,  0.0824, -0.0490, -0.0233],\n",
      "        ...,\n",
      "        [-0.0622, -0.0861,  0.0513,  ..., -0.0107, -0.0164, -0.0283],\n",
      "        [ 0.0154, -0.0509, -0.0018,  ...,  0.0489, -0.0033, -0.0008],\n",
      "        [ 0.0264,  0.0630, -0.0810,  ...,  0.0823,  0.0379,  0.0527]],\n",
      "       requires_grad=True)), ('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.0091,  0.0152, -0.0793, -0.0320, -0.0873, -0.0776, -0.0726,  0.0271,\n",
      "         0.0049,  0.0658,  0.0027,  0.0574,  0.0853,  0.0172, -0.0169, -0.0431,\n",
      "         0.0536, -0.0815,  0.0815, -0.0601, -0.0305,  0.0118,  0.0806,  0.0836,\n",
      "        -0.0210, -0.0046, -0.0774, -0.0733, -0.0311,  0.0786, -0.0204,  0.0271,\n",
      "         0.0266,  0.0595, -0.0165, -0.0394, -0.0224,  0.0125, -0.0638, -0.0074,\n",
      "         0.0643,  0.0157,  0.0426,  0.0080, -0.0478, -0.0768,  0.0777, -0.0358,\n",
      "         0.0447,  0.0635, -0.0333, -0.0365,  0.0034, -0.0637, -0.0893, -0.0853,\n",
      "         0.0003,  0.0790,  0.0603,  0.0104,  0.0092,  0.0793,  0.0311, -0.0192,\n",
      "         0.0596,  0.0709,  0.0235,  0.0434, -0.0528, -0.0220, -0.0794,  0.0568,\n",
      "        -0.0178, -0.0476, -0.0004, -0.0309,  0.0557,  0.0140, -0.0060,  0.0634,\n",
      "        -0.0602, -0.0194,  0.0020, -0.0561, -0.0663, -0.0426, -0.0561, -0.0549,\n",
      "        -0.0508, -0.0130, -0.0275,  0.0757,  0.0724, -0.0353,  0.0050, -0.0302,\n",
      "         0.0850,  0.0397,  0.0644, -0.0160,  0.0475,  0.0170,  0.0714, -0.0575,\n",
      "         0.0268,  0.0026, -0.0778, -0.0504, -0.0245, -0.0576,  0.0274,  0.0025,\n",
      "         0.0723,  0.0508, -0.0122, -0.0905, -0.0227,  0.0487, -0.0282,  0.0212,\n",
      "         0.0583, -0.0589, -0.0468,  0.0722,  0.0797,  0.0114, -0.0543, -0.0008,\n",
      "         0.0083, -0.0382, -0.0913,  0.0375,  0.0626,  0.0432,  0.0323, -0.0601,\n",
      "        -0.0417, -0.0486,  0.0688, -0.0789,  0.0529,  0.0643, -0.0640, -0.0655,\n",
      "         0.0465,  0.0229,  0.0745, -0.0678,  0.0292,  0.0262, -0.0756,  0.0748,\n",
      "        -0.0129, -0.0378, -0.0200, -0.0348,  0.0698, -0.0236, -0.0377,  0.0745,\n",
      "        -0.0469,  0.0563, -0.0508, -0.0830,  0.0132, -0.0077, -0.0539,  0.0845,\n",
      "        -0.0614, -0.0287, -0.0095, -0.0602, -0.0355, -0.0526,  0.0282, -0.0491,\n",
      "        -0.0346, -0.0199,  0.0368,  0.0259,  0.0652, -0.0812,  0.0863, -0.0708,\n",
      "        -0.0787,  0.0460,  0.0016, -0.0388,  0.0021, -0.0733,  0.0652,  0.0606,\n",
      "         0.0106,  0.0102, -0.0796, -0.0600,  0.0564,  0.0278, -0.0191, -0.0078,\n",
      "        -0.0268, -0.0901, -0.0776, -0.0150, -0.0441, -0.0617,  0.0519, -0.0117,\n",
      "        -0.0527, -0.0340, -0.0553,  0.0022, -0.0655, -0.0771, -0.0451, -0.0744,\n",
      "        -0.0380,  0.0467,  0.0439,  0.0222,  0.0891,  0.0103, -0.0507, -0.0535,\n",
      "        -0.0828,  0.0312,  0.0230,  0.0859, -0.0142, -0.0167, -0.0700,  0.0084,\n",
      "         0.0843,  0.0831,  0.0070, -0.0652, -0.0719, -0.0584, -0.0486, -0.0213,\n",
      "        -0.0612, -0.0027,  0.0325,  0.0042, -0.0316, -0.0370,  0.0705,  0.0379,\n",
      "         0.0787, -0.0403,  0.0581, -0.0703,  0.0626, -0.0222,  0.0861, -0.0262,\n",
      "         0.0582,  0.0056, -0.0133,  0.0048,  0.0867, -0.0380, -0.0352,  0.0126,\n",
      "        -0.0366, -0.0132,  0.0807, -0.0501,  0.0519, -0.0114, -0.0088,  0.0162,\n",
      "        -0.0887,  0.0522, -0.0493, -0.0294, -0.0450,  0.0655,  0.0491,  0.0065,\n",
      "        -0.0596,  0.0850, -0.0701,  0.0224, -0.0796, -0.0519, -0.0642,  0.0191,\n",
      "         0.0810,  0.0168,  0.0644,  0.0326,  0.0005, -0.0703,  0.0606,  0.0495,\n",
      "        -0.0507, -0.0858,  0.0301, -0.0662, -0.0847,  0.0491, -0.0226, -0.0399,\n",
      "         0.0216, -0.0111,  0.0031, -0.0826,  0.0311, -0.0566,  0.0505,  0.0117,\n",
      "        -0.0661, -0.0206, -0.0284, -0.0066,  0.0283,  0.0396,  0.0192,  0.0799,\n",
      "         0.0044, -0.0644, -0.0483, -0.0318, -0.0860, -0.0581, -0.0308,  0.0609,\n",
      "         0.0581,  0.0456, -0.0421,  0.0065, -0.0071,  0.0057, -0.0344, -0.0827,\n",
      "         0.0711, -0.0597,  0.0639, -0.0529, -0.0552, -0.0237,  0.0623,  0.0462,\n",
      "        -0.0829,  0.0159, -0.0082, -0.0290, -0.0351,  0.0036,  0.0566, -0.0634,\n",
      "        -0.0738, -0.0672, -0.0363, -0.0839,  0.0701,  0.0417, -0.0436,  0.0480,\n",
      "        -0.0892,  0.0269, -0.0500, -0.0172,  0.0455, -0.0867, -0.0675, -0.0318,\n",
      "        -0.0259, -0.0050, -0.0594,  0.0631,  0.0592,  0.0777,  0.0768, -0.0872,\n",
      "         0.0201, -0.0222, -0.0686, -0.0753, -0.0384, -0.0871, -0.0732, -0.0359,\n",
      "         0.0813, -0.0897,  0.0496,  0.0145,  0.0450,  0.0876, -0.0012,  0.0410,\n",
      "         0.0201,  0.0234, -0.0413, -0.0848,  0.0843,  0.0127, -0.0270, -0.0144,\n",
      "         0.0202,  0.0159,  0.0666, -0.0752, -0.0772, -0.0122, -0.0863, -0.0840,\n",
      "         0.0810,  0.0089, -0.0571,  0.0368,  0.0875, -0.0612, -0.0540, -0.0457,\n",
      "         0.0424, -0.0291,  0.0026, -0.0770, -0.0683,  0.0542, -0.0559,  0.0277,\n",
      "        -0.0766,  0.0720, -0.0440,  0.0624,  0.0199, -0.0453,  0.0612,  0.0167,\n",
      "        -0.0169,  0.0475,  0.0831,  0.0420, -0.0375, -0.0432,  0.0305,  0.0131,\n",
      "        -0.0844,  0.0906, -0.0886, -0.0541, -0.0883, -0.0382,  0.0541,  0.0346,\n",
      "        -0.0891, -0.0352, -0.0537, -0.0515,  0.0409, -0.0547,  0.0317,  0.0133,\n",
      "        -0.0188,  0.0383, -0.0618,  0.0821, -0.0735,  0.0613,  0.0324,  0.0532,\n",
      "        -0.0040,  0.0445, -0.0013,  0.0250, -0.0227, -0.0289,  0.0705,  0.0782,\n",
      "        -0.0854,  0.0327,  0.0251,  0.0055, -0.0609, -0.0564, -0.0428,  0.0477],\n",
      "       requires_grad=True)), ('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.0869, -0.0202, -0.0686, -0.0267, -0.0661, -0.0911,  0.0189,  0.0279,\n",
      "         0.0004, -0.0604, -0.0670, -0.0590, -0.0117,  0.0229,  0.0721, -0.0069,\n",
      "         0.0216, -0.0376,  0.0705,  0.0795, -0.0646, -0.0832, -0.0010, -0.0125,\n",
      "         0.0700, -0.0693,  0.0513, -0.0114, -0.0816, -0.0190,  0.0129,  0.0056,\n",
      "         0.0253, -0.0100,  0.0192,  0.0522,  0.0241, -0.0087,  0.0604, -0.0318,\n",
      "         0.0808, -0.0150,  0.0479, -0.0501, -0.0190, -0.0688,  0.0515, -0.0842,\n",
      "         0.0055, -0.0372, -0.0262, -0.0800, -0.0592, -0.0031, -0.0456, -0.0185,\n",
      "        -0.0351, -0.0334, -0.0539,  0.0793,  0.0251, -0.0092,  0.0541,  0.0023,\n",
      "        -0.0241,  0.0774, -0.0006,  0.0177,  0.0649, -0.0829, -0.0681,  0.0405,\n",
      "        -0.0495, -0.0743,  0.0323, -0.0497,  0.0462, -0.0613,  0.0367, -0.0900,\n",
      "         0.0668,  0.0141, -0.0083,  0.0503, -0.0749, -0.0724, -0.0819, -0.0555,\n",
      "        -0.0540,  0.0328,  0.0205, -0.0773, -0.0623, -0.0566, -0.0378,  0.0048,\n",
      "         0.0434, -0.0867, -0.0651,  0.0378, -0.0162, -0.0350, -0.0790, -0.0145,\n",
      "        -0.0888, -0.0684,  0.0178,  0.0060, -0.0712, -0.0619,  0.0288,  0.0520,\n",
      "        -0.0443, -0.0020, -0.0843,  0.0806, -0.0777, -0.0651,  0.0217,  0.0405,\n",
      "        -0.0213,  0.0507,  0.0156, -0.0678,  0.0627, -0.0415, -0.0752,  0.0355,\n",
      "        -0.0239,  0.0476, -0.0366,  0.0846, -0.0227, -0.0231,  0.0219,  0.0866,\n",
      "        -0.0758, -0.0180,  0.0283,  0.0797, -0.0741, -0.0573,  0.0471, -0.0450,\n",
      "         0.0553,  0.0043,  0.0008, -0.0451,  0.0124,  0.0191,  0.0549, -0.0594,\n",
      "         0.0425, -0.0102, -0.0159, -0.0191,  0.0221, -0.0595,  0.0102, -0.0385,\n",
      "        -0.0076, -0.0882,  0.0812,  0.0205, -0.0831,  0.0481, -0.0019, -0.0598,\n",
      "         0.0800,  0.0816, -0.0577, -0.0332, -0.0530, -0.0850, -0.0579,  0.0708,\n",
      "        -0.0553, -0.0576, -0.0660, -0.0782,  0.0381, -0.0019, -0.0633,  0.0166,\n",
      "        -0.0830,  0.0350, -0.0360,  0.0087, -0.0120,  0.0088, -0.0716, -0.0675,\n",
      "         0.0117, -0.0346,  0.0469, -0.0031, -0.0241,  0.0142, -0.0438,  0.0232,\n",
      "         0.0897,  0.0805,  0.0669, -0.0667, -0.0312, -0.0372, -0.0656, -0.0612,\n",
      "         0.0613, -0.0809,  0.0133,  0.0816, -0.0444, -0.0419, -0.0279,  0.0232,\n",
      "         0.0838, -0.0360, -0.0021, -0.0327, -0.0245, -0.0669,  0.0549,  0.0160,\n",
      "         0.0458, -0.0056,  0.0592,  0.0730,  0.0578,  0.0186, -0.0094,  0.0372,\n",
      "         0.0167,  0.0810, -0.0549, -0.0781, -0.0641,  0.0297,  0.0592, -0.0580,\n",
      "         0.0090, -0.0438, -0.0189, -0.0852,  0.0631,  0.0448, -0.0569, -0.0131,\n",
      "         0.0395, -0.0052, -0.0008, -0.0271,  0.0033, -0.0020,  0.0707,  0.0852,\n",
      "        -0.0509,  0.0007,  0.0828,  0.0612,  0.0553,  0.0759, -0.0042, -0.0435,\n",
      "        -0.0749, -0.0871, -0.0361,  0.0858, -0.0625,  0.0154, -0.0892, -0.0112,\n",
      "         0.0042,  0.0224,  0.0279,  0.0743, -0.0506,  0.0494, -0.0910, -0.0823,\n",
      "         0.0552,  0.0752,  0.0536, -0.0025, -0.0837,  0.0902, -0.0193, -0.0805,\n",
      "        -0.0900, -0.0382, -0.0475, -0.0470, -0.0066,  0.0408, -0.0651, -0.0433,\n",
      "         0.0851, -0.0578,  0.0728,  0.0172, -0.0479, -0.0307,  0.0295,  0.0737,\n",
      "        -0.0737, -0.0533,  0.0838,  0.0813,  0.0125,  0.0453, -0.0369,  0.0656,\n",
      "         0.0538,  0.0375, -0.0588,  0.0621,  0.0867, -0.0354,  0.0735,  0.0886,\n",
      "        -0.0604, -0.0870, -0.0047, -0.0681, -0.0481, -0.0681,  0.0476, -0.0105,\n",
      "        -0.0846, -0.0615,  0.0812,  0.0018,  0.0414,  0.0144,  0.0474, -0.0752,\n",
      "         0.0175, -0.0433, -0.0524, -0.0647, -0.0655,  0.0408, -0.0792, -0.0499,\n",
      "         0.0047,  0.0117, -0.0544,  0.0284, -0.0047,  0.0552,  0.0774, -0.0515,\n",
      "         0.0782, -0.0797,  0.0431, -0.0179, -0.0307, -0.0385, -0.0488, -0.0451,\n",
      "         0.0624,  0.0571, -0.0769,  0.0614,  0.0872, -0.0390, -0.0532, -0.0603,\n",
      "        -0.0436,  0.0447,  0.0126,  0.0641, -0.0160, -0.0193,  0.0088, -0.0729,\n",
      "        -0.0363, -0.0587, -0.0054,  0.0083,  0.0444,  0.0789,  0.0767,  0.0484,\n",
      "        -0.0846, -0.0151,  0.0111, -0.0490,  0.0803,  0.0141,  0.0689,  0.0907,\n",
      "         0.0081,  0.0543, -0.0265,  0.0883, -0.0229, -0.0008, -0.0636,  0.0410,\n",
      "        -0.0853,  0.0309,  0.0425,  0.0644, -0.0278, -0.0095,  0.0525,  0.0730,\n",
      "        -0.0151, -0.0116,  0.0704, -0.0367,  0.0434,  0.0592, -0.0230,  0.0714,\n",
      "        -0.0905, -0.0078, -0.0141, -0.0831, -0.0226,  0.0685, -0.0727,  0.0902,\n",
      "         0.0676,  0.0542, -0.0084, -0.0148,  0.0678, -0.0554, -0.0782, -0.0720,\n",
      "         0.0404, -0.0303, -0.0133,  0.0174, -0.0257, -0.0782, -0.0082, -0.0823,\n",
      "        -0.0140, -0.0784,  0.0603, -0.0719,  0.0165, -0.0379, -0.0293, -0.0470,\n",
      "        -0.0317,  0.0164,  0.0816,  0.0019,  0.0415,  0.0477, -0.0597, -0.0489,\n",
      "         0.0142, -0.0546,  0.0288, -0.0769, -0.0203,  0.0840, -0.0389,  0.0688,\n",
      "        -0.0388,  0.0557, -0.0255, -0.0274, -0.0045, -0.0190,  0.0777,  0.0196,\n",
      "        -0.0721, -0.0701, -0.0350, -0.0534,  0.0776, -0.0222,  0.0190,  0.0741],\n",
      "       requires_grad=True)), ('weight_ih_l1', Parameter containing:\n",
      "tensor([[-0.0889,  0.0162, -0.0801,  ...,  0.0326,  0.0542,  0.0819],\n",
      "        [-0.0847, -0.0493, -0.0870,  ..., -0.0802,  0.0646,  0.0356],\n",
      "        [-0.0146,  0.0001, -0.0290,  ...,  0.0003, -0.0330,  0.0537],\n",
      "        ...,\n",
      "        [-0.0550,  0.0015, -0.0648,  ..., -0.0724,  0.0685,  0.0791],\n",
      "        [-0.0453, -0.0522, -0.0898,  ..., -0.0206, -0.0091, -0.0023],\n",
      "        [ 0.0181, -0.0869, -0.0178,  ..., -0.0028,  0.0622, -0.0660]],\n",
      "       requires_grad=True)), ('weight_hh_l1', Parameter containing:\n",
      "tensor([[ 0.0762, -0.0338, -0.0126,  ...,  0.0751, -0.0034, -0.0384],\n",
      "        [-0.0843, -0.0535, -0.0239,  ...,  0.0590, -0.0035, -0.0230],\n",
      "        [-0.0264, -0.0501, -0.0384,  ..., -0.0509,  0.0621,  0.0818],\n",
      "        ...,\n",
      "        [ 0.0076,  0.0494, -0.0899,  ...,  0.0780,  0.0417,  0.0861],\n",
      "        [ 0.0203, -0.0689,  0.0372,  ...,  0.0344,  0.0280, -0.0699],\n",
      "        [ 0.0338, -0.0338, -0.0807,  ...,  0.0808,  0.0649, -0.0271]],\n",
      "       requires_grad=True)), ('bias_ih_l1', Parameter containing:\n",
      "tensor([ 0.0903,  0.0128,  0.0827, -0.0699,  0.0478, -0.0034, -0.0245, -0.0725,\n",
      "         0.0772, -0.0124, -0.0649,  0.0796, -0.0874, -0.0755, -0.0352, -0.0026,\n",
      "         0.0831,  0.0426, -0.0461, -0.0412, -0.0887,  0.0384,  0.0444,  0.0143,\n",
      "         0.0530, -0.0483, -0.0316,  0.0635, -0.0486,  0.0390, -0.0128, -0.0247,\n",
      "        -0.0275, -0.0752,  0.0775, -0.0213, -0.0296,  0.0154,  0.0499,  0.0556,\n",
      "         0.0194,  0.0131,  0.0606, -0.0476, -0.0782,  0.0396,  0.0080,  0.0603,\n",
      "         0.0079, -0.0632,  0.0362, -0.0840, -0.0659,  0.0661, -0.0005,  0.0092,\n",
      "         0.0028,  0.0274,  0.0126,  0.0493,  0.0729,  0.0340,  0.0329,  0.0659,\n",
      "        -0.0213,  0.0284, -0.0173, -0.0731,  0.0076,  0.0800, -0.0091, -0.0003,\n",
      "        -0.0710, -0.0600,  0.0146, -0.0638, -0.0443, -0.0256,  0.0493, -0.0508,\n",
      "         0.0860, -0.0400,  0.0255,  0.0583, -0.0829,  0.0379,  0.0054, -0.0498,\n",
      "        -0.0695, -0.0577,  0.0694, -0.0763, -0.0765,  0.0556, -0.0057,  0.0430,\n",
      "        -0.0162,  0.0699, -0.0212, -0.0844, -0.0711, -0.0451, -0.0212,  0.0535,\n",
      "         0.0444, -0.0797, -0.0688, -0.0091,  0.0820,  0.0107,  0.0267,  0.0273,\n",
      "         0.0219,  0.0242, -0.0555,  0.0356, -0.0778, -0.0287,  0.0193,  0.0317,\n",
      "         0.0518, -0.0310, -0.0005,  0.0636,  0.0859, -0.0473, -0.0142,  0.0752,\n",
      "         0.0885, -0.0432, -0.0287, -0.0869, -0.0740,  0.0831, -0.0269, -0.0890,\n",
      "         0.0351,  0.0045, -0.0174, -0.0849,  0.0495,  0.0776, -0.0310,  0.0448,\n",
      "         0.0813,  0.0351, -0.0115,  0.0362,  0.0016,  0.0784, -0.0016,  0.0831,\n",
      "        -0.0012, -0.0776,  0.0584, -0.0792,  0.0123, -0.0445,  0.0278,  0.0253,\n",
      "         0.0147, -0.0125,  0.0370,  0.0401,  0.0321,  0.0088, -0.0766, -0.0313,\n",
      "        -0.0518, -0.0209, -0.0907,  0.0815,  0.0100, -0.0164,  0.0900, -0.0749,\n",
      "         0.0337,  0.0504,  0.0878,  0.0207, -0.0100,  0.0155, -0.0674, -0.0111,\n",
      "         0.0441,  0.0156,  0.0502, -0.0150, -0.0719,  0.0519, -0.0406,  0.0041,\n",
      "        -0.0082,  0.0702,  0.0553, -0.0641, -0.0828, -0.0089,  0.0007, -0.0499,\n",
      "         0.0857, -0.0872,  0.0682, -0.0535,  0.0155, -0.0746, -0.0010, -0.0632,\n",
      "         0.0829,  0.0856,  0.0906,  0.0418,  0.0322,  0.0787,  0.0697,  0.0315,\n",
      "        -0.0572, -0.0840, -0.0813,  0.0821, -0.0827, -0.0240, -0.0614,  0.0169,\n",
      "        -0.0640,  0.0166,  0.0036,  0.0100, -0.0793, -0.0289, -0.0415, -0.0502,\n",
      "        -0.0006,  0.0763,  0.0788, -0.0777, -0.0699, -0.0477, -0.0616, -0.0362,\n",
      "        -0.0028,  0.0727,  0.0527,  0.0431,  0.0537,  0.0701, -0.0816,  0.0447,\n",
      "         0.0225,  0.0016, -0.0421,  0.0851, -0.0204, -0.0626, -0.0344, -0.0333,\n",
      "         0.0756, -0.0692,  0.0680, -0.0111,  0.0798,  0.0736,  0.0016,  0.0468,\n",
      "        -0.0328,  0.0437,  0.0031,  0.0656,  0.0423, -0.0275,  0.0845, -0.0566,\n",
      "         0.0110, -0.0354,  0.0639,  0.0533,  0.0528,  0.0503,  0.0382,  0.0081,\n",
      "        -0.0357,  0.0729, -0.0821, -0.0239,  0.0639, -0.0257, -0.0724,  0.0884,\n",
      "         0.0267, -0.0670, -0.0583,  0.0051,  0.0552, -0.0529,  0.0612,  0.0781,\n",
      "        -0.0260, -0.0848, -0.0620, -0.0596, -0.0843, -0.0711, -0.0255,  0.0035,\n",
      "        -0.0780, -0.0742,  0.0036, -0.0884, -0.0904,  0.0343, -0.0179, -0.0657,\n",
      "         0.0043, -0.0007,  0.0887,  0.0490,  0.0607, -0.0236,  0.0659, -0.0340,\n",
      "         0.0108, -0.0844,  0.0099,  0.0090,  0.0672, -0.0492, -0.0755, -0.0610,\n",
      "        -0.0353, -0.0082,  0.0668,  0.0512,  0.0194,  0.0027,  0.0573,  0.0785,\n",
      "         0.0261,  0.0380, -0.0023, -0.0566, -0.0860,  0.0066,  0.0857, -0.0351,\n",
      "         0.0361, -0.0281,  0.0357, -0.0026, -0.0508,  0.0098, -0.0245, -0.0064,\n",
      "        -0.0595,  0.0330, -0.0550, -0.0405, -0.0423, -0.0080,  0.0129,  0.0826,\n",
      "        -0.0486,  0.0534,  0.0542, -0.0320,  0.0569, -0.0358,  0.0798,  0.0033,\n",
      "        -0.0048,  0.0089, -0.0716,  0.0036,  0.0587, -0.0378,  0.0754, -0.0005,\n",
      "         0.0271,  0.0741,  0.0569, -0.0825, -0.0067,  0.0379, -0.0380, -0.0149,\n",
      "        -0.0508, -0.0699, -0.0009,  0.0418, -0.0860,  0.0558, -0.0399, -0.0097,\n",
      "        -0.0164,  0.0094, -0.0025,  0.0623,  0.0051,  0.0483,  0.0373, -0.0662,\n",
      "         0.0673, -0.0705,  0.0305, -0.0276,  0.0770,  0.0403, -0.0092, -0.0094,\n",
      "        -0.0833, -0.0112,  0.0214, -0.0503,  0.0831, -0.0769, -0.0493,  0.0227,\n",
      "        -0.0336, -0.0212,  0.0520,  0.0229, -0.0724,  0.0587, -0.0032, -0.0273,\n",
      "        -0.0476,  0.0343,  0.0661,  0.0363, -0.0452, -0.0210,  0.0819, -0.0890,\n",
      "         0.0456, -0.0325,  0.0059, -0.0682,  0.0509, -0.0898, -0.0409,  0.0312,\n",
      "        -0.0124, -0.0796,  0.0703,  0.0025, -0.0909, -0.0822,  0.0353, -0.0843,\n",
      "        -0.0428, -0.0700,  0.0211,  0.0876, -0.0807, -0.0591, -0.0578,  0.0738,\n",
      "        -0.0181,  0.0013,  0.0255, -0.0052, -0.0005,  0.0019, -0.0309, -0.0733,\n",
      "        -0.0572, -0.0612,  0.0417,  0.0450, -0.0871, -0.0425,  0.0165,  0.0523,\n",
      "         0.0458,  0.0206, -0.0161,  0.0044, -0.0664,  0.0305,  0.0681,  0.0331],\n",
      "       requires_grad=True)), ('bias_hh_l1', Parameter containing:\n",
      "tensor([ 2.8972e-02, -7.5059e-02,  3.2915e-02,  5.5273e-02, -6.6384e-02,\n",
      "        -2.6834e-02, -1.9005e-02,  8.2106e-02,  8.1549e-02,  7.2246e-02,\n",
      "         8.2610e-02,  3.1338e-02,  6.1669e-02,  3.8612e-02,  2.2845e-02,\n",
      "        -8.7599e-02,  2.9101e-02, -4.5875e-02, -2.2923e-02, -1.8446e-02,\n",
      "        -1.4059e-02,  4.6484e-02,  8.2008e-02,  4.4654e-02,  6.9660e-03,\n",
      "        -5.3693e-02, -1.9947e-02,  4.8476e-02,  6.7558e-02, -8.4655e-02,\n",
      "        -4.1041e-02, -6.3748e-02, -3.3734e-02,  2.4536e-02,  1.7865e-02,\n",
      "        -3.4729e-02,  5.6626e-02,  5.0401e-02,  5.1023e-03, -8.8716e-02,\n",
      "        -3.7024e-02, -6.2906e-02,  5.2382e-02, -8.6940e-03, -8.8052e-02,\n",
      "         4.2311e-02,  1.1244e-02, -7.9603e-02, -2.9338e-02,  4.3044e-02,\n",
      "        -5.3806e-03, -4.5104e-02, -8.6288e-02, -2.2496e-02,  6.6508e-02,\n",
      "         2.9592e-02,  1.0707e-02,  4.1307e-02, -4.8396e-02,  6.9715e-02,\n",
      "        -1.5729e-02,  3.2719e-02,  1.5566e-02, -2.6542e-02,  4.0787e-02,\n",
      "         9.0948e-02, -2.9176e-02,  8.4665e-02,  5.7868e-02,  6.3867e-02,\n",
      "        -6.6773e-03,  7.0459e-02,  3.9208e-02, -7.3758e-02,  8.3040e-03,\n",
      "         5.8020e-02,  4.6913e-02, -9.2185e-03,  8.8933e-02,  8.2812e-02,\n",
      "        -2.1310e-02,  4.8081e-02,  8.5332e-02,  2.5712e-02, -4.0294e-02,\n",
      "        -2.2554e-02,  7.2414e-02, -4.0130e-02, -7.1260e-02, -8.1627e-02,\n",
      "         6.1198e-03,  8.1302e-02, -8.4134e-02, -1.7101e-02, -6.3306e-02,\n",
      "        -6.3341e-02, -3.2739e-02,  1.7451e-02, -3.6684e-02, -7.1285e-02,\n",
      "         8.6440e-02, -2.5999e-02,  2.4396e-03, -3.3847e-02, -1.5017e-02,\n",
      "         6.6468e-02,  6.7485e-02, -1.7447e-02, -4.3116e-02,  8.2214e-02,\n",
      "        -5.0340e-02,  6.6853e-03, -3.3652e-02, -1.9608e-02,  1.5793e-02,\n",
      "         3.2837e-02,  1.5894e-02,  1.4176e-02,  7.9011e-02,  7.0162e-02,\n",
      "        -5.1396e-02, -8.8651e-02,  7.7310e-02, -7.9494e-02,  6.9446e-02,\n",
      "        -2.5144e-02,  4.2306e-02, -1.8871e-02, -1.1625e-02,  3.9205e-02,\n",
      "         3.5090e-02, -2.3925e-02, -5.0302e-02,  5.9756e-02, -2.6737e-02,\n",
      "         3.2412e-02, -3.8694e-02,  6.1963e-02, -8.2053e-02, -8.6074e-02,\n",
      "         4.5456e-02, -7.8847e-02, -5.3475e-02,  8.9540e-02,  5.6825e-02,\n",
      "        -3.1627e-02, -6.9585e-04,  6.4401e-02,  2.5750e-02, -4.2405e-02,\n",
      "        -2.9642e-02, -2.0936e-02, -8.6058e-02,  7.8527e-02, -6.9250e-02,\n",
      "         1.2775e-02, -8.0752e-03, -5.2798e-02,  5.7473e-02, -4.1073e-02,\n",
      "         4.4631e-02, -6.8571e-02,  8.6159e-02, -6.9449e-02,  7.5508e-02,\n",
      "         7.6651e-02,  2.3288e-03,  4.6848e-03,  2.9066e-02, -6.7371e-02,\n",
      "         3.8591e-02, -5.3842e-02, -3.9480e-02, -4.9400e-02, -5.4034e-03,\n",
      "         7.6937e-03,  4.9098e-02, -5.3078e-02,  4.0668e-02, -2.1634e-02,\n",
      "        -6.1849e-02,  7.4886e-02, -5.6683e-02, -9.8903e-03,  7.1134e-02,\n",
      "        -3.8306e-02, -4.8895e-02, -1.7476e-02, -1.5149e-02,  1.6319e-02,\n",
      "         5.5137e-03, -7.9341e-02, -4.4181e-02,  6.1167e-02, -3.2343e-02,\n",
      "         4.1053e-02, -4.6011e-02, -7.7770e-02, -1.9599e-02,  3.8851e-02,\n",
      "         5.9980e-02,  2.5685e-03,  6.0261e-02,  6.4558e-02,  2.5681e-02,\n",
      "         2.9342e-02,  1.3283e-02, -6.5496e-02,  7.1200e-02,  5.6005e-02,\n",
      "        -2.9484e-02, -6.6508e-02,  6.5577e-02,  7.5778e-02, -5.7944e-02,\n",
      "         1.8380e-02,  2.8164e-02, -7.0109e-02, -1.1632e-02, -5.8251e-02,\n",
      "        -7.3060e-02,  8.0840e-02,  4.7346e-02,  6.8781e-02,  1.8502e-02,\n",
      "         1.7171e-02, -2.9929e-02,  4.8116e-02,  1.2024e-02,  8.2641e-02,\n",
      "        -2.5112e-02,  2.7272e-03, -2.8400e-02, -6.9078e-02, -5.5414e-02,\n",
      "        -5.6988e-02,  6.6838e-02, -2.2912e-02,  2.3409e-02,  8.3946e-02,\n",
      "         3.8303e-02,  5.5025e-02,  2.8971e-02, -4.6636e-02, -5.8425e-02,\n",
      "        -7.4745e-02, -2.8680e-02, -4.4011e-02, -4.3977e-02, -1.8572e-02,\n",
      "        -7.4992e-03,  6.6207e-02,  8.5082e-02,  1.5459e-02, -4.4347e-02,\n",
      "        -9.1353e-03,  5.0042e-02, -1.6224e-02,  2.9858e-02, -6.7372e-02,\n",
      "        -1.2895e-02,  2.3684e-02,  1.5731e-02, -3.7811e-02,  6.4101e-02,\n",
      "        -8.5861e-02, -5.3501e-02, -6.1270e-02,  7.5868e-02, -8.5153e-02,\n",
      "        -8.7771e-03,  2.7667e-02,  7.6658e-02,  6.1545e-02, -5.2441e-02,\n",
      "        -5.6883e-02, -8.6228e-02,  6.9145e-02, -3.4355e-02,  2.3904e-02,\n",
      "        -8.5452e-02,  5.7158e-02, -1.9300e-02,  5.7066e-02,  7.3244e-02,\n",
      "        -1.1477e-02,  3.9541e-02,  2.7321e-02,  8.3679e-03,  5.6454e-02,\n",
      "         4.4908e-02, -2.6323e-02, -8.3615e-02,  4.9817e-02, -2.4638e-03,\n",
      "         6.8901e-02, -2.8677e-02,  2.7261e-03, -3.4609e-02,  4.4504e-02,\n",
      "        -1.4408e-02,  2.4559e-02, -7.7258e-02, -5.3619e-02,  3.0385e-02,\n",
      "         1.9882e-02,  4.4960e-03,  5.0332e-02,  3.4554e-02,  2.1591e-02,\n",
      "        -8.4679e-02,  1.5497e-02,  5.5164e-02,  5.8999e-03, -7.5973e-02,\n",
      "         8.9117e-02, -4.3579e-02,  4.9421e-02,  9.4727e-03, -6.2444e-02,\n",
      "         2.6421e-02,  8.2296e-02,  4.2471e-02,  8.5213e-02,  3.1660e-02,\n",
      "        -8.0027e-02, -4.0718e-02, -1.4915e-02, -1.1715e-02,  4.8265e-05,\n",
      "         1.0979e-02,  1.0884e-02, -4.5726e-02, -5.0802e-02, -7.7880e-02,\n",
      "        -4.5359e-02,  6.6603e-02,  8.0271e-02, -8.8263e-02, -3.3082e-02,\n",
      "         4.6203e-02,  7.6626e-02, -8.1790e-02,  5.2374e-02, -6.6845e-02,\n",
      "        -2.1438e-02, -3.5175e-02,  7.7253e-02, -2.0677e-02, -2.2827e-02,\n",
      "         8.4969e-02, -3.1297e-02,  8.2118e-02, -6.2940e-02, -7.6489e-02,\n",
      "        -5.5012e-02, -6.8554e-02, -9.0688e-02,  8.0245e-02,  7.7623e-02,\n",
      "        -5.9769e-03,  1.9612e-02, -6.1466e-02, -7.6262e-02,  3.1039e-03,\n",
      "         5.6001e-02, -1.8298e-02, -4.0191e-02,  9.3545e-03, -7.6067e-02,\n",
      "        -8.6706e-02, -5.7372e-03,  1.8934e-02, -2.9800e-02,  7.7738e-03,\n",
      "         5.8590e-02, -7.8362e-02, -5.6723e-02, -6.6749e-02, -8.0165e-02,\n",
      "        -1.6945e-03,  4.6976e-02, -5.8727e-02,  3.7423e-02,  3.4473e-02,\n",
      "         7.8435e-02, -2.3314e-02,  7.1177e-02,  1.9704e-02, -6.0860e-02,\n",
      "         1.0729e-02,  7.3016e-02, -4.3674e-02, -4.5458e-03,  7.1777e-02,\n",
      "        -6.7966e-02, -4.9269e-02, -6.3094e-02,  3.6826e-02,  5.8123e-02,\n",
      "         7.1424e-03, -7.1543e-02, -3.2221e-02, -9.1025e-02, -8.5365e-02,\n",
      "        -4.1001e-02,  7.8515e-02, -1.9401e-02, -7.8962e-02, -5.7349e-02,\n",
      "         5.9700e-02,  8.2975e-02, -3.0289e-02,  6.2804e-02,  8.7166e-02,\n",
      "         4.4902e-02, -5.7262e-02,  6.5014e-02, -4.1329e-02, -2.4090e-02,\n",
      "         6.3150e-02, -1.0183e-02,  2.8451e-03, -8.6603e-02, -7.5023e-02,\n",
      "        -5.7606e-02,  6.9097e-02, -1.9272e-02, -6.3263e-02, -8.7541e-02,\n",
      "         1.0776e-02,  4.1401e-02, -8.9634e-02, -6.9320e-02, -2.4163e-02,\n",
      "        -6.0263e-02,  4.3269e-02,  6.9744e-02,  7.8722e-02,  9.1259e-02,\n",
      "         5.4340e-02, -4.3658e-03,  5.0143e-02,  2.2485e-02, -5.0850e-02,\n",
      "        -8.3584e-02,  2.4456e-02,  5.9257e-02, -1.6344e-02, -8.7101e-02,\n",
      "         1.4017e-02, -4.5950e-02,  1.8461e-02,  2.1187e-02,  4.4055e-02,\n",
      "        -6.6686e-02,  7.2644e-02, -4.2049e-02,  4.9509e-02,  5.0433e-02,\n",
      "         7.2747e-02, -4.8424e-02, -1.8524e-02, -5.9943e-02, -6.6348e-02,\n",
      "         3.7628e-02, -8.5219e-02,  6.7578e-02, -5.9692e-02,  1.1833e-02,\n",
      "         8.2774e-02,  2.7347e-02, -6.4227e-02,  4.3999e-02, -3.5341e-02,\n",
      "        -4.7912e-02,  2.8160e-02, -4.7271e-03,  4.9825e-02,  6.5165e-02],\n",
      "       requires_grad=True))]\n",
      "[('weight', Parameter containing:\n",
      "tensor([[ 0.0830,  0.0757,  0.0397,  0.0384, -0.0270, -0.0850,  0.0391,  0.0733,\n",
      "          0.0162, -0.0488, -0.0665, -0.0681,  0.0388, -0.0323,  0.0069,  0.0904,\n",
      "          0.0351, -0.0796,  0.0169,  0.0016, -0.0479,  0.0703, -0.0395,  0.0703,\n",
      "         -0.0804,  0.0793,  0.0146,  0.0342,  0.0572,  0.0465, -0.0808,  0.0334,\n",
      "          0.0387,  0.0657,  0.0699, -0.0184, -0.0160, -0.0668,  0.0256, -0.0709,\n",
      "          0.0804,  0.0515, -0.0522,  0.0008, -0.0264, -0.0420,  0.0461, -0.0121,\n",
      "         -0.0609, -0.0360, -0.0838, -0.0435, -0.0402,  0.0877,  0.0572, -0.0655,\n",
      "         -0.0579,  0.0740, -0.0498,  0.0742,  0.0596, -0.0469, -0.0168,  0.0308,\n",
      "         -0.0428,  0.0567,  0.0289,  0.0147, -0.0831, -0.0720,  0.0091, -0.0582,\n",
      "          0.0626,  0.0775, -0.0703, -0.0712,  0.0152,  0.0091, -0.0350, -0.0047,\n",
      "          0.0526, -0.0694,  0.0199,  0.0176,  0.0076,  0.0069,  0.0189, -0.0660,\n",
      "          0.0514, -0.0830,  0.0229, -0.0102,  0.0557, -0.0458,  0.0808, -0.0686,\n",
      "         -0.0475, -0.0130, -0.0025, -0.0596, -0.0548,  0.0505,  0.0219, -0.0420,\n",
      "          0.0842, -0.0793, -0.0863, -0.0861, -0.0057, -0.0384,  0.0807,  0.0352,\n",
      "          0.0108,  0.0272, -0.0211, -0.0099, -0.0700,  0.0357, -0.0681, -0.0233]],\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([0.0207], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "for layer_name, layer in model.features.named_children():\n",
    "    print(list(layer.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[ 0.0830,  0.0757,  0.0397,  0.0384, -0.0270, -0.0850,  0.0391,  0.0733,\n",
      "          0.0162, -0.0488, -0.0665, -0.0681,  0.0388, -0.0323,  0.0069,  0.0904,\n",
      "          0.0351, -0.0796,  0.0169,  0.0016, -0.0479,  0.0703, -0.0395,  0.0703,\n",
      "         -0.0804,  0.0793,  0.0146,  0.0342,  0.0572,  0.0465, -0.0808,  0.0334,\n",
      "          0.0387,  0.0657,  0.0699, -0.0184, -0.0160, -0.0668,  0.0256, -0.0709,\n",
      "          0.0804,  0.0515, -0.0522,  0.0008, -0.0264, -0.0420,  0.0461, -0.0121,\n",
      "         -0.0609, -0.0360, -0.0838, -0.0435, -0.0402,  0.0877,  0.0572, -0.0655,\n",
      "         -0.0579,  0.0740, -0.0498,  0.0742,  0.0596, -0.0469, -0.0168,  0.0308,\n",
      "         -0.0428,  0.0567,  0.0289,  0.0147, -0.0831, -0.0720,  0.0091, -0.0582,\n",
      "          0.0626,  0.0775, -0.0703, -0.0712,  0.0152,  0.0091, -0.0350, -0.0047,\n",
      "          0.0526, -0.0694,  0.0199,  0.0176,  0.0076,  0.0069,  0.0189, -0.0660,\n",
      "          0.0514, -0.0830,  0.0229, -0.0102,  0.0557, -0.0458,  0.0808, -0.0686,\n",
      "         -0.0475, -0.0130, -0.0025, -0.0596, -0.0548,  0.0505,  0.0219, -0.0420,\n",
      "          0.0842, -0.0793, -0.0863, -0.0861, -0.0057, -0.0384,  0.0807,  0.0352,\n",
      "          0.0108,  0.0272, -0.0211, -0.0099, -0.0700,  0.0357, -0.0681, -0.0233]],\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([0.0207], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(layer.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
