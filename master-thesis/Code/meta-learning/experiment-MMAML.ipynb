{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the conditional layer without MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import learn2learn as l2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "from ts_dataset import TSDataset\n",
    "from base_models import LSTMModel, FCN\n",
    "from metrics import torch_mae as mae\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"HR\"\n",
    "dataset_name = \"POLLUTION\"\n",
    "window_size = 32\n",
    "window_size = 5\n",
    "task_size = 50\n",
    "batch_size = 64\n",
    "input_dim = 13\n",
    "input_dim = 14\n",
    "output_dim = 1\n",
    "hidden_dim = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(  open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "train_data_ML = pickle.load( open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "validation_data = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "validation_data_ML = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "test_data_ML = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_encoder_input(data_ML):\n",
    "    \n",
    "    task_encoder_input = np.concatenate((data_ML.x[:,:,0,:], data_ML.y), axis=2)\n",
    "    \n",
    "    return task_encoder_input\n",
    "\n",
    "\n",
    "   \n",
    "class LSTMDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, seq_len, output_dim, n_layers, hidden_dim, latent_dim, device):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(1, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.hidden_to_output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    \n",
    "        self.decoder_inputs = torch.zeros( self.batch_size, self.sequence_length, 1, requires_grad=True).to(device)\n",
    "        self.c_0 = torch.zeros(self.n_layers, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.latent_to_hidden.weight)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_output.weight)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent):\n",
    "        \n",
    "        h_state = self.latent_to_hidden(latent).unsqueeze(0)\n",
    "        h_0 = torch.cat([h_state for _ in range(self.n_layers)], axis=0)\n",
    "        decoder_output, _ = self.lstm(self.decoder_inputs, (h_0, self.c_0))\n",
    "        out = self.hidden_to_output(decoder_output)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "\n",
    "    \"\"\"Lambda module converts output of encoder to latent vector\n",
    "\n",
    "    :param hidden_size: hidden size of the encoder\n",
    "    :param latent_length: latent vector length\n",
    "    https://github.com/abhmalik/timeseries-clustering-vae/blob/master/vrae/vrae.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, latent_dim):\n",
    "\n",
    "        super(Lambda, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.hidden_to_mean = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden_to_mean.weight)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logvar.weight)\n",
    "\n",
    "\n",
    "    def forward(self, cell_output):\n",
    "\n",
    "        \"\"\"Given last hidden state of encoder, passes through a linear layer, and finds the mean and variance\n",
    "\n",
    "        :param cell_output: last hidden state of encoder\n",
    "        :return: latent vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.latent_mean = self.hidden_to_mean(cell_output)\n",
    "        self.latent_logvar = self.hidden_to_logvar(cell_output)\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * self.latent_logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(self.latent_mean)\n",
    "        else:\n",
    "            return self.latent_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_model = LSTMModel(batch_size=batch_size, seq_len = window_size, input_dim = input_dim, n_layers = 2, hidden_dim = hidden_dim, output_dim =1)\n",
    "task_encoder = LSTMModel(batch_size=batch_size, seq_len = task_size, input_dim = input_dim+1, n_layers = 1, hidden_dim = hidden_dim, output_dim =1)\n",
    "task_decoder = LSTMDecoder(batch_size = 1, n_layers =1 , seq_len = task_size, output_dim = input_dim +1,  hidden_dim = hidden_dim, latent_dim = hidden_dim, device = device)\n",
    "lmbd = Lambda(hidden_dim, hidden_dim)\n",
    "\n",
    "class MultimodalLearner(nn.Module):\n",
    "    \n",
    "    def __init__(self, task_model, task_encoder, task_decoder, lmbd):\n",
    "        \n",
    "        super(MultimodalLearner, self).__init__()\n",
    "        \n",
    "        self.task_model = task_model\n",
    "        self.task_encoder = task_encoder\n",
    "        self.task_decoder = task_decoder\n",
    "        self.lmbd = lmbd\n",
    "        self.modulation_layer = nn.Linear(task_encoder.hidden_dim, task_model.hidden_dim*2)\n",
    "        self.output_layer = nn.Linear(task_model.hidden_dim, 1)\n",
    "        self.task_decoder = task_decoder\n",
    "        self.rec_loss = nn.SmoothL1Loss(size_average=False)\n",
    "    \n",
    "    \n",
    "    def conditional_layer(self, x, embedding):\n",
    "        \n",
    "        ###apply by deffault the affine transformation\n",
    "        \n",
    "        gammas, betas = torch.split(embedding, x.size(1), dim=-1)\n",
    "        gammas = gammas + torch.ones_like(gammas)\n",
    "        x = x*gammas + betas\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, x_decoded, x):\n",
    "        \n",
    "        \n",
    "        latent_mean, latent_logvar = self.lmbd.latent_mean, self.lmbd.latent_logvar\n",
    "        kl_loss = -0.5 * torch.mean(1 + latent_logvar - latent_mean.pow(2) - latent_logvar.exp())\n",
    "        recon_loss = self.rec_loss(x_decoded, x)\n",
    "        \n",
    "        return recon_loss + kl_loss, kl_loss, recon_loss\n",
    "    \n",
    "    def forward (self, x, task, params=None, embeddings=None):\n",
    "        \n",
    "        if params is None:\n",
    "            params = OrderedDict(self.named_parameters())\n",
    "            \n",
    "        x = self.task_model.encoder(x)\n",
    "        encoding = self.task_encoder.encoder(task)\n",
    "        latent = self.lmbd(encoding)\n",
    "        task_rec = self.task_decoder(latent)\n",
    "        \n",
    "        modulation_embeddings = self.modulation_layer(encoding)\n",
    "        modulated_output = self.conditional_layer(x, modulation_embeddings)\n",
    "        output = self.output_layer(modulated_output)\n",
    "        \n",
    "        loss = self.compute_loss(task_rec, task)\n",
    "        \n",
    "        return output, loss\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: tensor(155.9806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0889653386193372\n",
      "Test loss: 0.1124294653332027\n",
      "Train loss: tensor(190.5320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.08385251000346172\n",
      "Test loss: 0.10719440502142256\n",
      "Train loss: tensor(162.5385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.07954846497270324\n",
      "Test loss: 0.10285253091576961\n",
      "Train loss: tensor(131.7786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.07582304910978391\n",
      "Test loss: 0.09916017518037616\n",
      "Train loss: tensor(159.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.07262590542169554\n",
      "Test loss: 0.09597204983381942\n",
      "Train loss: tensor(112.8615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06944818806257985\n",
      "Test loss: 0.09314426516948066\n",
      "Train loss: tensor(196.4892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06644067486659402\n",
      "Test loss: 0.08985814708515559\n",
      "Train loss: tensor(246.2649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06320205303352504\n",
      "Test loss: 0.08627747122825373\n",
      "Train loss: tensor(122.5197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0606011137632387\n",
      "Test loss: 0.08332443858801138\n",
      "Train loss: tensor(144.7740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.058457154421401876\n",
      "Test loss: 0.08079491719824843\n",
      "Train loss: tensor(157.6826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.056507116028418146\n",
      "Test loss: 0.0783492349143518\n",
      "Train loss: tensor(204.7607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.054797780833074024\n",
      "Test loss: 0.07619437369711624\n",
      "Train loss: tensor(274.3177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0527899742392557\n",
      "Test loss: 0.07337727087854158\n",
      "Train loss: tensor(227.9004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050677045638717354\n",
      "Test loss: 0.07038366387939395\n",
      "Train loss: tensor(289.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04818698429014711\n",
      "Test loss: 0.06686431346553387\n",
      "Train loss: tensor(232.6292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04595062742009759\n",
      "Test loss: 0.06367737786973467\n",
      "Train loss: tensor(295.5759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.043643346783660705\n",
      "Test loss: 0.0603135414790399\n",
      "Train loss: tensor(157.1987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041732839978344385\n",
      "Test loss: 0.05749550241386831\n",
      "Train loss: tensor(174.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04011649878784305\n",
      "Test loss: 0.05507475698348319\n",
      "Train loss: tensor(179.3588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.038833740899073224\n",
      "Test loss: 0.0530570144165713\n",
      "Train loss: tensor(140.4875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0377348714108978\n",
      "Test loss: 0.05135372173188642\n",
      "Train loss: tensor(326.9459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037033333026227494\n",
      "Test loss: 0.050144553295161466\n",
      "Train loss: tensor(96.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03657637458028538\n",
      "Test loss: 0.0491346923062707\n",
      "Train loss: tensor(518.5217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03626851302438549\n",
      "Test loss: 0.04807692211084437\n",
      "Train loss: tensor(185.4218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03597384701438603\n",
      "Test loss: 0.047146192419868296\n",
      "Train loss: tensor(161.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03588510466118654\n",
      "Test loss: 0.046436495498044066\n",
      "Train loss: tensor(114.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03600395501901706\n",
      "Test loss: 0.0459621781759923\n",
      "Train loss: tensor(208.5274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03633716551232196\n",
      "Test loss: 0.04558510340676449\n",
      "Train loss: tensor(309.1447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0368090003817564\n",
      "Test loss: 0.04535463274373572\n",
      "Train loss: tensor(335.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03735177595877931\n",
      "Test loss: 0.045361151374728965\n",
      "Train loss: tensor(283.7359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03781385945067519\n",
      "Test loss: 0.045478063119151216\n",
      "Train loss: tensor(199.5855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03823560955268996\n",
      "Test loss: 0.04563115406228174\n",
      "Train loss: tensor(183.4186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03858511566760994\n",
      "Test loss: 0.045792800964474085\n",
      "Train loss: tensor(131.9982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039136467297517116\n",
      "Test loss: 0.0461589028450227\n",
      "Train loss: tensor(251.3145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039451528730846584\n",
      "Test loss: 0.04637307730863\n",
      "Train loss: tensor(279.0465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03976489017229705\n",
      "Test loss: 0.04661277716628986\n",
      "Train loss: tensor(480.4579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04002507007015603\n",
      "Test loss: 0.046815506191832\n",
      "Train loss: tensor(150.7536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04018171887312617\n",
      "Test loss: 0.046957299703418616\n",
      "Train loss: tensor(149.8399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04034942001813934\n",
      "Test loss: 0.04711481065738319\n",
      "Train loss: tensor(169.9087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0403086277966698\n",
      "Test loss: 0.04709042300756025\n",
      "Train loss: tensor(114.3118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04044095715951352\n",
      "Test loss: 0.04722981641788294\n",
      "Train loss: tensor(113.7248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04045258357766129\n",
      "Test loss: 0.047273476875507\n",
      "Train loss: tensor(172.5930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04029957492436682\n",
      "Test loss: 0.04715479475141752\n",
      "Train loss: tensor(139.3304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0400132280996158\n",
      "Test loss: 0.0469463394014257\n",
      "Train loss: tensor(216.5327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03977452297473238\n",
      "Test loss: 0.04677115740383615\n",
      "Train loss: tensor(175.6269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039753435649687335\n",
      "Test loss: 0.0467764145076865\n",
      "Train loss: tensor(263.6197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03949616836117847\n",
      "Test loss: 0.04657042142704572\n",
      "Train loss: tensor(140.0932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039234853793113006\n",
      "Test loss: 0.04635795676103323\n",
      "Train loss: tensor(149.7983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03892570883922634\n",
      "Test loss: 0.04611226055070315\n",
      "Train loss: tensor(232.8120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0388296885948096\n",
      "Test loss: 0.046035589152338484\n",
      "Train loss: tensor(258.9326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03870518531295515\n",
      "Test loss: 0.04592713798479278\n",
      "Train loss: tensor(172.1939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.038567561230489183\n",
      "Test loss: 0.04580169211667363\n",
      "Train loss: tensor(133.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.038310277302350315\n",
      "Test loss: 0.045576717907396876\n",
      "Train loss: tensor(143.1812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03790592107744444\n",
      "Test loss: 0.045226289697065214\n",
      "Train loss: tensor(173.1894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03751383112477405\n",
      "Test loss: 0.04488375155937554\n",
      "Train loss: tensor(171.2088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037134402999210926\n",
      "Test loss: 0.044560605420334506\n",
      "Train loss: tensor(318.4088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.036618890631056965\n",
      "Test loss: 0.044128083467188446\n",
      "Train loss: tensor(172.7870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03621700359064908\n",
      "Test loss: 0.04379630018726434\n",
      "Train loss: tensor(182.2712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.035852956984724314\n",
      "Test loss: 0.043506291907022494\n",
      "Train loss: tensor(150.1871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0354374530592135\n",
      "Test loss: 0.04319641244883585\n",
      "Train loss: tensor(323.6124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03527945434408528\n",
      "Test loss: 0.04308503276050681\n",
      "Train loss: tensor(911.5406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03510890269563312\n",
      "Test loss: 0.04295802105328824\n",
      "Train loss: tensor(156.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03500959003078086\n",
      "Test loss: 0.04289742211562277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: tensor(123.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03498177371386971\n",
      "Test loss: 0.04287846912952638\n",
      "Train loss: tensor(185.8405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034964310102874326\n",
      "Test loss: 0.04285437624790881\n",
      "Train loss: tensor(473.6626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03473196817295892\n",
      "Test loss: 0.0426219000416522\n",
      "Train loss: tensor(181.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034538935949759826\n",
      "Test loss: 0.04243226031228752\n",
      "Train loss: tensor(103.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03438708729864586\n",
      "Test loss: 0.042288343385230784\n",
      "Train loss: tensor(202.6670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03425983985265096\n",
      "Test loss: 0.04215385702134359\n",
      "Train loss: tensor(143.3434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034238713562843345\n",
      "Test loss: 0.04209426499091753\n",
      "Train loss: tensor(344.3893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03431887068741378\n",
      "Test loss: 0.0420752043402431\n",
      "Train loss: tensor(402.3174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03436441279592968\n",
      "Test loss: 0.04205567830491184\n",
      "Train loss: tensor(345.2764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034382016637495585\n",
      "Test loss: 0.04204626680158152\n",
      "Train loss: tensor(162.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03448506724976358\n",
      "Test loss: 0.04207270932706571\n",
      "Train loss: tensor(162.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03464502093515226\n",
      "Test loss: 0.04213565846185873\n",
      "Train loss: tensor(306.6668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0347660947502369\n",
      "Test loss: 0.042269162686154396\n",
      "Train loss: tensor(335.5842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034928318716230845\n",
      "Test loss: 0.04242631633498586\n",
      "Train loss: tensor(264.7373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03510759449785664\n",
      "Test loss: 0.04258455088970685\n",
      "Train loss: tensor(181.6280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03525748766426529\n",
      "Test loss: 0.042726537738327346\n",
      "Train loss: tensor(143.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.035328958307703336\n",
      "Test loss: 0.04281515081293217\n",
      "Train loss: tensor(120.7916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03541520118181195\n",
      "Test loss: 0.04290753845494277\n",
      "Train loss: tensor(148.5864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.035541861859105885\n",
      "Test loss: 0.04299782918268206\n",
      "Train loss: tensor(271.4318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03575792985835246\n",
      "Test loss: 0.04302820657370704\n",
      "Train loss: tensor(200.3579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.035961920546279065\n",
      "Test loss: 0.04306967861692209\n",
      "Train loss: tensor(165.7433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03613472761852401\n",
      "Test loss: 0.043090401733718295\n",
      "Train loss: tensor(207.0999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03629437092514265\n",
      "Test loss: 0.043104980666522345\n",
      "Train loss: tensor(246.1446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03649959685724406\n",
      "Test loss: 0.0430798743307443\n",
      "Train loss: tensor(242.1423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03668791946201097\n",
      "Test loss: 0.04310115896128487\n",
      "Train loss: tensor(93.9353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03687810559003126\n",
      "Test loss: 0.043125968186719585\n",
      "Train loss: tensor(186.1047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03693461437665281\n",
      "Test loss: 0.043059797422720654\n",
      "Train loss: tensor(261.7835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03696670250168869\n",
      "Test loss: 0.04302254454739908\n",
      "Train loss: tensor(193.7288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03699601139163687\n",
      "Test loss: 0.04298863250135195\n",
      "Train loss: tensor(145.2926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0370520186743566\n",
      "Test loss: 0.04295387519246871\n",
      "Train loss: tensor(268.0263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03707049646902652\n",
      "Test loss: 0.04295338521536329\n",
      "Train loss: tensor(285.1306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0370826840666788\n",
      "Test loss: 0.0429698541981749\n",
      "Train loss: tensor(199.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03713265184667849\n",
      "Test loss: 0.04297157425876006\n",
      "Train loss: tensor(175.2442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037147196151670954\n",
      "Test loss: 0.0429093266311701\n",
      "Train loss: tensor(134.4256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03717338142118284\n",
      "Test loss: 0.042863664798217245\n",
      "Train loss: tensor(164.2008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03710654763770955\n",
      "Test loss: 0.04274868970681535\n",
      "Train loss: tensor(312.4329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03708956012768405\n",
      "Test loss: 0.0426887241146057\n",
      "Train loss: tensor(117.6633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03708629605493375\n",
      "Test loss: 0.042653625987765224\n",
      "Train loss: tensor(155.0679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037139217945791426\n",
      "Test loss: 0.04258468770191516\n",
      "Train loss: tensor(188.4844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037218555071879\n",
      "Test loss: 0.04256704191465189\n",
      "Train loss: tensor(162.9164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037213800705614544\n",
      "Test loss: 0.04258386306649092\n",
      "Train loss: tensor(263.0267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03720836548932961\n",
      "Test loss: 0.04264301538356755\n",
      "Train loss: tensor(210.6695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037206691601091905\n",
      "Test loss: 0.04270944168957153\n",
      "Train loss: tensor(284.1627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037260701037233786\n",
      "Test loss: 0.04262134633810803\n",
      "Train loss: tensor(277.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037254956515417215\n",
      "Test loss: 0.04262361141604067\n",
      "Train loss: tensor(220.6361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0372355608890454\n",
      "Test loss: 0.04263543877012954\n",
      "Train loss: tensor(152.8488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03722259360587313\n",
      "Test loss: 0.04265283687169304\n",
      "Train loss: tensor(260.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03723960828834346\n",
      "Test loss: 0.042657997532941326\n",
      "Train loss: tensor(113.7141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03726195096969605\n",
      "Test loss: 0.04266948723188131\n",
      "Train loss: tensor(124.7788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03744808345412214\n",
      "Test loss: 0.04284624938880748\n",
      "Train loss: tensor(379.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037639356688374564\n",
      "Test loss: 0.043032645081898364\n",
      "Train loss: tensor(124.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.038065124436148574\n",
      "Test loss: 0.04344740882515907\n",
      "Train loss: tensor(229.9576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.038509713335051425\n",
      "Test loss: 0.04391088141220631\n",
      "Train loss: tensor(154.3264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03928166391061885\n",
      "Test loss: 0.044461045285115144\n",
      "Train loss: tensor(147.4821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039860314600879236\n",
      "Test loss: 0.04484194731063182\n",
      "Train loss: tensor(360.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.040447452220888365\n",
      "Test loss: 0.04515431384270144\n",
      "Train loss: tensor(170.1280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.040782304693545615\n",
      "Test loss: 0.04528407501702261\n",
      "Train loss: tensor(249.9021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041186496739586194\n",
      "Test loss: 0.04547877498155478\n",
      "Train loss: tensor(199.9054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04146848447798263\n",
      "Test loss: 0.04560488219013308\n",
      "Train loss: tensor(136.5428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041491394507743064\n",
      "Test loss: 0.04552537034090498\n",
      "Train loss: tensor(147.6698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041386613808572295\n",
      "Test loss: 0.04535984861120434\n",
      "Train loss: tensor(240.8874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041226582122700556\n",
      "Test loss: 0.04519175577650566\n",
      "Train loss: tensor(245.2306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.040876788104928674\n",
      "Test loss: 0.04482312572400759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: tensor(269.6441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04099651000329426\n",
      "Test loss: 0.04483531204301237\n",
      "Train loss: tensor(199.3589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04113766596253429\n",
      "Test loss: 0.04489040986109193\n",
      "Train loss: tensor(153.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0411347322609453\n",
      "Test loss: 0.04482569773266516\n",
      "Train loss: tensor(162.0400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041096681320951095\n",
      "Test loss: 0.044768704304305636\n",
      "Train loss: tensor(145.5974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04104510504042819\n",
      "Test loss: 0.04471227821737233\n",
      "Train loss: tensor(165.7450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041097721359914255\n",
      "Test loss: 0.044790565469624975\n",
      "Train loss: tensor(110.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04114386902323791\n",
      "Test loss: 0.044869217588243505\n",
      "Train loss: tensor(155.8040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04103507387141387\n",
      "Test loss: 0.04480942471487687\n",
      "Train loss: tensor(140.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.040933915892882006\n",
      "Test loss: 0.04475637394381632\n",
      "Train loss: tensor(128.0254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04110462261097772\n",
      "Test loss: 0.04494697891065095\n",
      "Train loss: tensor(206.9440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04109348912856409\n",
      "Test loss: 0.04498236097100348\n",
      "Train loss: tensor(320.5529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041335059312127886\n",
      "Test loss: 0.04528274760572332\n",
      "Train loss: tensor(111.1375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04150857434918483\n",
      "Test loss: 0.04550407214625047\n",
      "Train loss: tensor(129.6126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041588423543033146\n",
      "Test loss: 0.04562493048534535\n",
      "Train loss: tensor(258.5813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04187556350869792\n",
      "Test loss: 0.046007188073921915\n",
      "Train loss: tensor(234.5867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041905213803762484\n",
      "Test loss: 0.04608418852422792\n",
      "Train loss: tensor(185.6662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0421112663955206\n",
      "Test loss: 0.046309072561193224\n",
      "Train loss: tensor(125.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04231046635125365\n",
      "Test loss: 0.046515812036940954\n",
      "Train loss: tensor(139.3644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04271908122159186\n",
      "Test loss: 0.046890903052200776\n",
      "Train loss: tensor(165.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04308680784666822\n",
      "Test loss: 0.04720614239167754\n",
      "Train loss: tensor(265.9812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04356777129606122\n",
      "Test loss: 0.04761980072881031\n",
      "Train loss: tensor(243.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04403049587493851\n",
      "Test loss: 0.048051857085216164\n",
      "Train loss: tensor(172.2914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04475920965806359\n",
      "Test loss: 0.04869688774394517\n",
      "Train loss: tensor(181.8589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04608474156508843\n",
      "Test loss: 0.049819744809871856\n",
      "Train loss: tensor(112.5339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.047857820624042126\n",
      "Test loss: 0.05125399865210056\n",
      "Train loss: tensor(240.0338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.049781407100991125\n",
      "Test loss: 0.05276625994408485\n",
      "Train loss: tensor(171.7723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05184483360499144\n",
      "Test loss: 0.05437874733147645\n",
      "Train loss: tensor(159.2379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05360242507110039\n",
      "Test loss: 0.055723856360015304\n",
      "Train loss: tensor(151.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05482536000864846\n",
      "Test loss: 0.05664320432092294\n",
      "Train loss: tensor(221.9397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05581375209703332\n",
      "Test loss: 0.057357814199853654\n",
      "Train loss: tensor(309.8374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05673227936384224\n",
      "Test loss: 0.05797737630287019\n",
      "Train loss: tensor(133.8446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0580590093330968\n",
      "Test loss: 0.05897330690064643\n",
      "Train loss: tensor(319.8609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05935414199318204\n",
      "Test loss: 0.06003456217891509\n",
      "Train loss: tensor(224.8774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0603419486521965\n",
      "Test loss: 0.06081532058075513\n",
      "Train loss: tensor(302.6982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.061325648480228014\n",
      "Test loss: 0.06169865326494864\n",
      "Train loss: tensor(333.3979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06225987385426249\n",
      "Test loss: 0.06257604377915954\n",
      "Train loss: tensor(254.7036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06311942425866922\n",
      "Test loss: 0.06343056202524959\n",
      "Train loss: tensor(151.5755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.062192342998016445\n",
      "Test loss: 0.06297986273939657\n",
      "Train loss: tensor(329.3348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06175310545201813\n",
      "Test loss: 0.0633705105300587\n",
      "Train loss: tensor(297.6720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.06125406396708318\n",
      "Test loss: 0.06352118985487683\n",
      "Train loss: tensor(140.6201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.060333733313849995\n",
      "Test loss: 0.06338277756061295\n",
      "Train loss: tensor(175.3884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05883251684939578\n",
      "Test loss: 0.06285048224548302\n",
      "Train loss: tensor(358.4563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05707031326102359\n",
      "Test loss: 0.06130966769144087\n",
      "Train loss: tensor(165.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05537655796146109\n",
      "Test loss: 0.059731383263917255\n",
      "Train loss: tensor(163.4262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.053667108954063486\n",
      "Test loss: 0.05812092306147708\n",
      "Train loss: tensor(141.7469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05178016559886081\n",
      "Test loss: 0.05642422998674435\n",
      "Train loss: tensor(204.8271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050224976915688745\n",
      "Test loss: 0.055011478888958985\n",
      "Train loss: tensor(376.5668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.048737597101855844\n",
      "Test loss: 0.053413723807523746\n",
      "Train loss: tensor(211.2162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.047576505229586646\n",
      "Test loss: 0.052200187292724555\n",
      "Train loss: tensor(124.2570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04651920298735301\n",
      "Test loss: 0.05112526652467723\n",
      "Train loss: tensor(274.2664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.045368315882626034\n",
      "Test loss: 0.04985056339882978\n",
      "Train loss: tensor(190.9500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.044346430418746814\n",
      "Test loss: 0.04876981279121177\n",
      "Train loss: tensor(146.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0435084654727862\n",
      "Test loss: 0.047882565195755206\n",
      "Train loss: tensor(331.8836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04269521896328245\n",
      "Test loss: 0.046956444168371136\n",
      "Train loss: tensor(452.9982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04241708103744757\n",
      "Test loss: 0.046779225110122474\n",
      "Train loss: tensor(139.8141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04217364773863838\n",
      "Test loss: 0.04664105173237253\n",
      "Train loss: tensor(136.9541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04202246734251579\n",
      "Test loss: 0.046497706499061375\n",
      "Train loss: tensor(156.7080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04141622641611667\n",
      "Test loss: 0.04575509282917079\n",
      "Train loss: tensor(136.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04091635498085192\n",
      "Test loss: 0.045148998443590535\n",
      "Train loss: tensor(326.3226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04033231653627895\n",
      "Test loss: 0.04442704783402162\n",
      "Train loss: tensor(207.1850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039836262539029124\n",
      "Test loss: 0.04381380329775338\n",
      "Train loss: tensor(145.8890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03939789570868015\n",
      "Test loss: 0.043352013481504255\n",
      "Train loss: tensor(136.8556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03901941065809556\n",
      "Test loss: 0.042968724347134626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: tensor(136.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03864172929454417\n",
      "Test loss: 0.04259799568369837\n",
      "Train loss: tensor(155.7238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03849276534858204\n",
      "Test loss: 0.04245240266185881\n",
      "Train loss: tensor(364.9574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03829288160694497\n",
      "Test loss: 0.0421467916210099\n",
      "Train loss: tensor(221.6950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0386044371340956\n",
      "Test loss: 0.04244163433218946\n",
      "Train loss: tensor(304.7272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03889625809554543\n",
      "Test loss: 0.0427226058050695\n",
      "Train loss: tensor(158.0247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.039182911006112896\n",
      "Test loss: 0.04301718533002209\n",
      "Train loss: tensor(228.8185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03946438408678486\n",
      "Test loss: 0.043259671566509963\n",
      "Train loss: tensor(117.0640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04000972892556872\n",
      "Test loss: 0.043699357455762304\n",
      "Train loss: tensor(95.4834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04052724143756287\n",
      "Test loss: 0.044128620019643613\n",
      "Train loss: tensor(178.6773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04102095484378792\n",
      "Test loss: 0.044597127640822735\n",
      "Train loss: tensor(154.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04134649656535614\n",
      "Test loss: 0.04483569837590255\n",
      "Train loss: tensor(163.7072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04164018752496867\n",
      "Test loss: 0.04505022861395437\n",
      "Train loss: tensor(223.8598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04195883612015418\n",
      "Test loss: 0.0452786120447782\n",
      "Train loss: tensor(197.7351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04225356758882602\n",
      "Test loss: 0.04549353077343785\n",
      "Train loss: tensor(115.3090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.042579628731168453\n",
      "Test loss: 0.04583149844098209\n",
      "Train loss: tensor(301.3809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.042245914015386785\n",
      "Test loss: 0.04516189431974498\n",
      "Train loss: tensor(124.4156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04193749833142474\n",
      "Test loss: 0.044550364486651846\n",
      "Train loss: tensor(223.3739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04212636065092825\n",
      "Test loss: 0.04465308563212062\n",
      "Train loss: tensor(167.8897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04221680975918259\n",
      "Test loss: 0.04461550733793785\n",
      "Train loss: tensor(290.1596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04182105549566802\n",
      "Test loss: 0.043840212194193706\n",
      "Train loss: tensor(105.6735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04155003595210257\n",
      "Test loss: 0.043245407789576765\n",
      "Train loss: tensor(124.6401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041363779003066675\n",
      "Test loss: 0.04281747238802733\n",
      "Train loss: tensor(629.3135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04112218929720776\n",
      "Test loss: 0.04227933126252772\n",
      "Train loss: tensor(142.4472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04110194287661995\n",
      "Test loss: 0.04212085788350294\n",
      "Train loss: tensor(246.0319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041210987516457125\n",
      "Test loss: 0.04221150698601314\n",
      "Train loss: tensor(134.5751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04130376617291144\n",
      "Test loss: 0.04239909748297812\n",
      "Train loss: tensor(192.6436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04138920270793495\n",
      "Test loss: 0.04270622849759489\n",
      "Train loss: tensor(201.2522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04136000381161769\n",
      "Test loss: 0.04293476728408938\n",
      "Train loss: tensor(165.5901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04124612957239151\n",
      "Test loss: 0.043097834259566696\n",
      "Train loss: tensor(277.2747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04111069643071719\n",
      "Test loss: 0.04310544182795404\n",
      "Train loss: tensor(335.4068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04084109868854284\n",
      "Test loss: 0.04262484966011921\n",
      "Train loss: tensor(130.7931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04057174891765628\n",
      "Test loss: 0.04215899986758976\n",
      "Train loss: tensor(313.9820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04051219244443235\n",
      "Test loss: 0.04156575744768771\n",
      "Train loss: tensor(179.0303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.041024335908393064\n",
      "Test loss: 0.041908550288270015\n",
      "Train loss: tensor(202.3235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04206040372539844\n",
      "Test loss: 0.04292419382214251\n",
      "Train loss: tensor(275.4584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.043226707731151864\n",
      "Test loss: 0.04408851832077645\n",
      "Train loss: tensor(204.5212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04437054281256029\n",
      "Test loss: 0.04523796816863636\n",
      "Train loss: tensor(149.8600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.045384059048124724\n",
      "Test loss: 0.04627260641370079\n",
      "Train loss: tensor(146.9750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04643557138208832\n",
      "Test loss: 0.04734407009390911\n",
      "Train loss: tensor(304.8910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.047814780761975616\n",
      "Test loss: 0.04879622002786929\n",
      "Train loss: tensor(143.9032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.049244781796421326\n",
      "Test loss: 0.050306404701037574\n",
      "Train loss: tensor(109.5586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05082182883329335\n",
      "Test loss: 0.05194881310372954\n",
      "Train loss: tensor(207.7786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050224890639739374\n",
      "Test loss: 0.05152429276063006\n",
      "Train loss: tensor(165.5533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.048472953259590126\n",
      "Test loss: 0.049961655972396386\n",
      "Train loss: tensor(106.9450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.0465211935075266\n",
      "Test loss: 0.0482158327364538\n",
      "Train loss: tensor(180.2486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04439218843444472\n",
      "Test loss: 0.046169594944556164\n",
      "Train loss: tensor(168.5556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04299175670991341\n",
      "Test loss: 0.04468549563005419\n",
      "Train loss: tensor(105.6184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04225290744964565\n",
      "Test loss: 0.043754096207494785\n",
      "Train loss: tensor(305.8756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04167871625118312\n",
      "Test loss: 0.04276934807364008\n",
      "Train loss: tensor(303.7738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.042167858894736995\n",
      "Test loss: 0.0433738948777318\n",
      "Train loss: tensor(120.4727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04282351153947058\n",
      "Test loss: 0.04413069085688284\n",
      "Train loss: tensor(117.2068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04387293291233835\n",
      "Test loss: 0.04513437339648752\n",
      "Train loss: tensor(202.2323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04549824033996889\n",
      "Test loss: 0.04650146207117503\n",
      "Train loss: tensor(215.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04733543790699471\n",
      "Test loss: 0.048092057587265376\n",
      "Train loss: tensor(172.7775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.049352690595246496\n",
      "Test loss: 0.04981390616423128\n",
      "Train loss: tensor(177.6942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050608156031618515\n",
      "Test loss: 0.05104122751641391\n",
      "Train loss: tensor(209.9366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05129313957212227\n",
      "Test loss: 0.05189950879441925\n",
      "Train loss: tensor(106.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.051083003636449575\n",
      "Test loss: 0.05219474530751162\n",
      "Train loss: tensor(301.4862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05101232000937064\n",
      "Test loss: 0.052416336556693706\n",
      "Train loss: tensor(114.1290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05107565638830974\n",
      "Test loss: 0.052701753416642697\n",
      "Train loss: tensor(146.4213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050987914559387025\n",
      "Test loss: 0.05288499140060774\n",
      "Train loss: tensor(318.7324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.051039469862977666\n",
      "Test loss: 0.05272182386995542\n",
      "Train loss: tensor(162.5588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.05109402621608405\n",
      "Test loss: 0.05262024558637992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: tensor(333.6547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.050884396325619446\n",
      "Test loss: 0.05318004896145056\n",
      "Train loss: tensor(151.9789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04997580422177201\n",
      "Test loss: 0.053259972169405166\n",
      "Train loss: tensor(180.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04921749125101737\n",
      "Test loss: 0.05350364035567139\n",
      "Train loss: tensor(165.3530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04874281576346783\n",
      "Test loss: 0.053991477844296115\n",
      "Train loss: tensor(201.6448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04854925520540703\n",
      "Test loss: 0.054670488922902855\n",
      "Train loss: tensor(121.2903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04849442458223729\n",
      "Test loss: 0.05559587460195664\n",
      "Train loss: tensor(243.1237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.04881928723660253\n",
      "Test loss: 0.056654431792621564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-456ff2037789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mmultimodal_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data_ML\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultimodal_learner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_ML\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultimodal_learner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-456ff2037789>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(data_ML, multimodal_learner, loss_fn)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtask_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test(data_ML, multimodal_learner, loss_fn):\n",
    "    \n",
    "    total_tasks, task_size, window_size, input_dim = data_ML.x.shape\n",
    "    \n",
    "    task_data = torch.FloatTensor(get_task_encoder_input(data_ML))\n",
    "    x_tensor = torch.FloatTensor(data_ML.x)\n",
    "    y_tensor = torch.FloatTensor(data_ML.y) \n",
    "\n",
    "    count = 0.0\n",
    "    accum_loss = 0.0\n",
    "    \n",
    "    for task_id in range(0, total_tasks, total_tasks//100):\n",
    "        \n",
    "        task =task_data[task_id:task_id+1].cuda()\n",
    "        x = x_tensor[task_id+1].cuda()\n",
    "        y = y_tensor[task_id+1].cuda()\n",
    "\n",
    "        y_pred, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x, task)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        accum_loss += loss.cpu().detach().numpy()\n",
    "        count +=1\n",
    "        \n",
    "    return accum_loss/count\n",
    "        \n",
    "\n",
    "        \n",
    "epochs = 500\n",
    "total_tasks = len(train_data_ML)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "multimodal_learner = MultimodalLearner(task_model, task_encoder, task_decoder, lmbd)\n",
    "multimodal_learner.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(multimodal_learner.parameters(), lr = 0.0001)\n",
    "\n",
    "task_data = torch.FloatTensor(get_task_encoder_input(train_data_ML))\n",
    "x_tensor = torch.FloatTensor(train_data_ML.x)\n",
    "y_tensor = torch.FloatTensor(train_data_ML.y)\n",
    "\n",
    "loss_fn = nn.SmoothL1Loss(size_average=False)\n",
    "#loss_fn = mae\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    multimodal_learner.train()\n",
    "    multimodal_learner.zero_grad()\n",
    "    \n",
    "    task_id = np.random.randint(0, total_tasks-1)\n",
    "    task =task_data[task_id:task_id+1].cuda()\n",
    "    x = x_tensor[task_id+1].cuda()\n",
    "    y = y_tensor[task_id+1].cuda()\n",
    "    \n",
    "    y_pred, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x, task)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y) + vrae_loss\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    \n",
    "    multimodal_learner.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = test(validation_data_ML, multimodal_learner, mae)\n",
    "        test_loss = test(test_data_ML, multimodal_learner, mae)\n",
    "    \n",
    "    print(\"Train loss:\",loss)\n",
    "    print(\"Val loss:\", val_loss)\n",
    "    print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(223.9370, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
