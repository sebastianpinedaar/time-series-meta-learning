{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import higher\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1,\"..\")\n",
    "from ts_dataset import TSDataset\n",
    "from base_models import LSTMModel, FCN\n",
    "from metrics import torch_mae as mae\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"HR\"\n",
    "dataset_name = \"POLLUTION\"\n",
    "model_name = \"LSTM\"\n",
    "\n",
    "task_size = 50\n",
    "batch_size = 64\n",
    "output_dim = 1\n",
    "\n",
    "batch_size = 20\n",
    "horizon = 10\n",
    "meta_learning_rate = 10e-6\n",
    "learning_rate = 10e-5\n",
    "n_inner_iter = 1\n",
    "##test\n",
    "\n",
    "if dataset_name == \"HR\":\n",
    "    window_size = 32\n",
    "    input_dim = 13\n",
    "elif dataset_name == \"POLLUTION\":\n",
    "    window_size = 5\n",
    "    input_dim = 14\n",
    "\n",
    "def to_torch(numpy_tensor):\n",
    "    \n",
    "    return torch.tensor(numpy_tensor).float().cuda()\n",
    "\n",
    "\n",
    "train_data = pickle.load(  open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "train_data_ML = pickle.load( open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "validation_data = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "validation_data_ML = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "test_data_ML = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"LSTM\":\n",
    "    model = LSTMModel( batch_size=batch_size, seq_len = window_size, input_dim = input_dim, n_layers = 2, hidden_dim = 120, output_dim =1)\n",
    "\n",
    "elif model_name == \"FCN\":\n",
    "    kernels = [8,5,3] if window_size != 5 else [4,2,1]\n",
    "    model = FCN(time_steps = window_size,  channels=[input_dim, 128, 128, 128] , kernels=kernels)\n",
    "    \n",
    "model.cuda()\n",
    "meta_opt = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_data_ML, horizon, n_inner_iter):\n",
    "\n",
    "    model.train()\n",
    "    total_tasks_test, task_size, window_size, input_dim = test_data_ML.x.shape\n",
    "    \n",
    "    qry_losses = []\n",
    "\n",
    "\n",
    "    for task in range(0, (total_tasks_test-horizon-1), total_tasks_test//100):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Sample a batch of support and query images and labels.\n",
    "\n",
    "        x_spt, y_spt = test_data_ML[task]\n",
    "        x_qry = test_data_ML.x[(task+1):(task+1+horizon)].reshape(-1, window_size, input_dim)\n",
    "        y_qry = test_data_ML.y[(task+1):(task+1+horizon)].reshape(-1, output_dim)\n",
    "\n",
    "        x_spt, y_spt = to_torch(x_spt), to_torch(y_spt)\n",
    "        x_qry = to_torch(x_qry)\n",
    "        y_qry = to_torch(y_qry)\n",
    "        \n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        # TODO: Maybe pull this out into a separate module so it\n",
    "        # doesn't have to be duplicated between `train` and `test`?\n",
    "\n",
    "        # Initialize the inner optimizer to adapt the parameters to\n",
    "        # the support set.\n",
    "        \n",
    "        inner_opt = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "        with higher.innerloop_ctx(model, inner_opt, track_higher_grads=False) as (fnet, diffopt):\n",
    "            # Optimize the likelihood of the support set by taking\n",
    "            # gradient steps w.r.t. the model's parameters.\n",
    "            # This adapts the model's meta-parameters to the task.\n",
    "            for _ in range(n_inner_iter):\n",
    "                spt_logits = fnet(x_spt)\n",
    "                spt_loss = mae(spt_logits, y_spt)\n",
    "                diffopt.step(spt_loss)\n",
    "\n",
    "            # The query loss and acc induced by these parameters.\n",
    "            qry_logits = fnet(x_qry).detach()\n",
    "            qry_loss = mae(qry_logits, y_qry)\n",
    "            \n",
    "            qry_losses.append(qry_loss.detach())\n",
    "\n",
    "\n",
    "    qry_losses = torch.stack(qry_losses).mean().item()\n",
    "\n",
    "    \n",
    "    print(qry_losses)\n",
    "    return qry_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-44b36652f247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mask_qry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx_spt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_spt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_ML\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#x_qry, y_qry = train_data_ML[(task+1):(task+1+horizon)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_qry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_qry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_ML\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch_size = 20\n",
    "n_iterations = 10\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "\n",
    "    #n_train_iter = train_data_ML.x.shape[0] // batch_size\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        start_time = time.time()\n",
    "        # Sample a batch of support and query images and labels.\n",
    "        \n",
    "        task = np.random.randint(1,horizon+1)\n",
    "        x_spt, y_spt = train_data_ML[task]\n",
    "        #x_qry, y_qry = train_data_ML[(task+1):(task+1+horizon)]\n",
    "        x_qry, y_qry = train_data_ML[task+1]\n",
    "\n",
    "        #x_spt, y_spt = train_data_ML[batch_idx:batch_size+batch_idx]\n",
    "        #x_qry, y_qry = train_data_ML[batch_idx+1 : batch_idx+batch_size+1]\n",
    "        \n",
    "        x_spt, y_spt = to_torch(x_spt), to_torch(y_spt)\n",
    "        x_qry = to_torch(x_qry)\n",
    "        y_qry = to_torch(y_qry)\n",
    "        \n",
    "        task_num, setsz, c_, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        # TODO: Maybe pull this out into a separate module so it\n",
    "        # doesn't have to be duplicated between `train` and `test`?\n",
    "\n",
    "        # Initialize the inner optimizer to adapt the parameters to\n",
    "        # the support set.\n",
    "        n_inner_iter = 5\n",
    "        inner_opt = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "        qry_losses = []\n",
    "        meta_opt.zero_grad()\n",
    "        for i in range(task_num):\n",
    "            with higher.innerloop_ctx(\n",
    "                model, inner_opt, copy_initial_weights=False\n",
    "            ) as (fnet, diffopt):\n",
    "                # Optimize the likelihood of the support set by taking\n",
    "                # gradient steps w.r.t. the model's parameters.\n",
    "                # This adapts the model's meta-parameters to the task.\n",
    "                # higher is able to automatically keep copies of\n",
    "                # your network's parameters as they are being updated.\n",
    "                for _ in range(n_inner_iter):\n",
    "                    spt_logits = fnet(x_spt[i])\n",
    "                    spt_loss = mae(spt_logits, y_spt[i])\n",
    "                    diffopt.step(spt_loss)\n",
    "\n",
    "                # The final set of adapted parameters will induce some\n",
    "                # final loss and accuracy on the query dataset.\n",
    "                # These will be used to update the model's meta-parameters.\n",
    "                qry_logits = fnet(x_qry[i])\n",
    "                qry_loss = mae(qry_logits, y_qry[i])\n",
    "                qry_losses.append(qry_loss.detach())\n",
    "\n",
    "                # Update the model's meta-parameters to optimize the query\n",
    "                # losses across all of the tasks sampled in this batch.\n",
    "                # This unrolls through the gradient steps.\n",
    "                qry_loss.backward()\n",
    "\n",
    "        meta_opt.step()\n",
    "        qry_losses = sum(qry_losses) / task_num\n",
    "        i = epoch + float(batch_idx) / n_train_iter\n",
    "        iter_time = time.time() - start_time\n",
    "        if batch_idx % 1 == 0:\n",
    "            print(\n",
    "                f'[Epoch {i:.2f}] Train Loss: {qry_losses:.4f}  | Time: {iter_time:.2f}'\n",
    "            )\n",
    "            test(model, validation_data_ML, horizon, n_inner_iter)\n",
    "            test(model, test_data_ML, horizon, n_inner_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
