{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the conditional layer without MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import learn2learn as l2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from meta_base_models import LinearModel, Task\n",
    "from multimodallearner import MultimodalLearner\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "from ts_dataset import TSDataset\n",
    "from base_models import LSTMModel, FCN\n",
    "from metrics import torch_mae as mae\n",
    "from pytorchtools import EarlyStopping, to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"HR\"\n",
    "dataset_name = \"POLLUTION\"\n",
    "window_size = 32\n",
    "window_size = 5\n",
    "task_size = 50\n",
    "batch_size = 64\n",
    "input_dim = 13\n",
    "input_dim = 14\n",
    "output_dim = 1\n",
    "hidden_dim = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(  open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "train_data_ML = pickle.load( open( \"../../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "validation_data = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "validation_data_ML = pickle.load( open( \"../../Data/VAL-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n",
    "test_data_ML = pickle.load( open( \"../../Data/TEST-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-ML.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetaLearner(object):\n",
    "    def __init__(self, model, optimizer, fast_lr, loss_func,\n",
    "                 first_order, num_updates, inner_loop_grad_clip,\n",
    "                 device):\n",
    "\n",
    "        self._model = model\n",
    "        self._fast_lr = fast_lr\n",
    "        self._optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._first_order = first_order\n",
    "        self._num_updates = num_updates\n",
    "        self._inner_loop_grad_clip = inner_loop_grad_clip\n",
    "        self._device = device\n",
    "        self._grads_mean = []\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def update_params(self, loss, params):\n",
    "        \"\"\"Apply one step of gradient descent on the loss function `loss`,\n",
    "        with step-size `self._fast_lr`, and returns the updated parameters.\n",
    "        \"\"\"\n",
    "        create_graph = not self._first_order\n",
    "        grads = torch.autograd.grad(loss, params.values(),\n",
    "                                    create_graph=create_graph, allow_unused=True)\n",
    "        for (name, param), grad in zip(params.items(), grads):\n",
    "            if self._inner_loop_grad_clip > 0 and grad is not None:\n",
    "                grad = grad.clamp(min=-self._inner_loop_grad_clip,\n",
    "                                  max=self._inner_loop_grad_clip)\n",
    "            if grad is not None:\n",
    "              params[name] = param - self._fast_lr * grad\n",
    "\n",
    "        return params\n",
    "\n",
    "    def adapt(self, train_tasks):\n",
    "        adapted_params = []\n",
    "\n",
    "        for task in train_tasks:\n",
    "            params = self._model.param_dict\n",
    "\n",
    "            for i in range(self._num_updates):\n",
    "                preds = self._model(task.x, params=params)\n",
    "                loss = self._loss_func(preds, task.y)\n",
    "                params = self.update_params(loss, params=params)\n",
    "\n",
    "            adapted_params.append(params)\n",
    "\n",
    "        return adapted_params\n",
    "\n",
    "    def step(self, adapted_params_list, val_tasks,\n",
    "             is_training, additional_loss_term = 0.0):\n",
    "        \n",
    "        self._optimizer.zero_grad()\n",
    "        post_update_losses = []\n",
    "\n",
    "        for adapted_params, task in zip(\n",
    "                adapted_params_list, val_tasks):\n",
    "            preds = self._model(task.x, params=adapted_params)\n",
    "\n",
    "            if ~is_training:\n",
    "                preds = torch.clamp(preds, 0, 1)\n",
    "                loss = mae(preds, task.y)\n",
    "            else:\n",
    "\n",
    "                loss = self._loss_func(preds, task.y)\n",
    "            post_update_losses.append(loss)\n",
    "\n",
    "        mean_loss = torch.mean(torch.stack(post_update_losses)) + additional_loss_term\n",
    "        if is_training:\n",
    "            mean_loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "\n",
    "        return mean_loss\n",
    "\n",
    "    def to(self, device, **kwargs):\n",
    "        self._device = device\n",
    "        self._model.to(device, **kwargs)\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {\n",
    "            'model_state_dict': self._model.state_dict(),\n",
    "            'optimizer': self._optimizer.state_dict() \n",
    "        }\n",
    "\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "\n",
    "        self._model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "        self._optimizer.load_state_dict(state_dict[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_encoder_input(data_ML):\n",
    "    \n",
    "    task_encoder_input = np.concatenate((data_ML.x[:,:,0,:], data_ML.y), axis=2)\n",
    "    \n",
    "    return task_encoder_input\n",
    "\n",
    "\n",
    "   \n",
    "class LSTMDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, seq_len, output_dim, n_layers, hidden_dim, latent_dim, device):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(1, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.hidden_to_output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    \n",
    "        self.decoder_inputs = torch.zeros( self.batch_size, self.sequence_length, 1, requires_grad=True).to(device)\n",
    "        self.c_0 = torch.zeros(self.n_layers, self.batch_size, self.hidden_dim, requires_grad=True).to(device)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.latent_to_hidden.weight)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_output.weight)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, latent):\n",
    "        \n",
    "        h_state = self.latent_to_hidden(latent).unsqueeze(0)\n",
    "        h_0 = torch.cat([h_state for _ in range(self.n_layers)], axis=0)\n",
    "        decoder_output, _ = self.lstm(self.decoder_inputs, (h_0, self.c_0))\n",
    "        out = self.hidden_to_output(decoder_output)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "\n",
    "    \"\"\"Lambda module converts output of encoder to latent vector\n",
    "\n",
    "    :param hidden_size: hidden size of the encoder\n",
    "    :param latent_length: latent vector length\n",
    "    https://github.com/abhmalik/timeseries-clustering-vae/blob/master/vrae/vrae.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, latent_dim):\n",
    "\n",
    "        super(Lambda, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.hidden_to_mean = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden_to_mean.weight)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logvar.weight)\n",
    "\n",
    "\n",
    "    def forward(self, cell_output):\n",
    "\n",
    "        \"\"\"Given last hidden state of encoder, passes through a linear layer, and finds the mean and variance\n",
    "\n",
    "        :param cell_output: last hidden state of encoder\n",
    "        :return: latent vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.latent_mean = self.hidden_to_mean(cell_output)\n",
    "        self.latent_logvar = self.hidden_to_logvar(cell_output)\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * self.latent_logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(self.latent_mean)\n",
    "        else:\n",
    "            return self.latent_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramters wto increase capactiy of the model\n",
    "n_layers_task_net = 2\n",
    "n_layers_task_encoder = 1\n",
    "n_layers_task_decoder = 1\n",
    "\n",
    "hidden_dim_task_net = 120\n",
    "hidden_dim_encoder = 120\n",
    "hidden_dim_decoder = 120\n",
    "\n",
    "#fixed values\n",
    "input_dim_task_net = input_dim\n",
    "input_dim_task_encoder = input_dim +1 \n",
    "output_dim_task_net = 1\n",
    "output_dim_task_decoder = input_dim+1\n",
    "\n",
    "        \n",
    "epochs = 1000\n",
    "total_tasks = len(train_data_ML)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "task_net = LSTMModel(batch_size=batch_size, \n",
    "                       seq_len = window_size, \n",
    "                       input_dim = input_dim_task_net, \n",
    "                       n_layers = n_layers_task_net, \n",
    "                       hidden_dim = hidden_dim_task_net, \n",
    "                       output_dim = output_dim_task_net)\n",
    "\n",
    "task_encoder = LSTMModel(batch_size=batch_size, \n",
    "                         seq_len = task_size, \n",
    "                         input_dim = input_dim_task_encoder, \n",
    "                         n_layers = n_layers_task_encoder, \n",
    "                         hidden_dim = hidden_dim_encoder, \n",
    "                         output_dim =1)\n",
    "\n",
    "task_decoder = LSTMDecoder(batch_size = 1, \n",
    "                           n_layers = n_layers_task_decoder , \n",
    "                           seq_len = task_size, \n",
    "                           output_dim = output_dim_task_decoder, \n",
    "                           hidden_dim = hidden_dim_encoder, \n",
    "                           latent_dim = hidden_dim_decoder, \n",
    "                           device = device)\n",
    "\n",
    "lmbd = Lambda(hidden_dim_encoder, hidden_dim_task_net)\n",
    "\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: tensor(0.3467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.034710622528224315\n",
      "Test loss: 0.0508692994409678\n",
      "Epoch: 1\n",
      "Train loss: tensor(0.3169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03455266906219779\n",
      "Test loss: 0.04878122952519631\n",
      "Epoch: 2\n",
      "Train loss: tensor(0.3293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03462524089998412\n",
      "Test loss: 0.047014401883495094\n",
      "Epoch: 3\n",
      "Train loss: tensor(0.3438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.035164096980418974\n",
      "Test loss: 0.04535632717366121\n",
      "Epoch: 4\n",
      "Train loss: tensor(0.3162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03603381323582918\n",
      "Test loss: 0.04422942959532446\n",
      "Epoch: 5\n",
      "Train loss: tensor(0.2961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.036945963368832484\n",
      "Test loss: 0.043506233059630105\n",
      "Epoch: 6\n",
      "Train loss: tensor(0.3069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03737715841497032\n",
      "Test loss: 0.04304733081739776\n",
      "Epoch: 7\n",
      "Train loss: tensor(0.3362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03808678006662906\n",
      "Test loss: 0.04283062779173559\n",
      "Epoch: 8\n",
      "Train loss: tensor(0.3072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.03803686493808783\n",
      "Test loss: 0.04258784955861617\n",
      "Epoch: 9\n",
      "Train loss: tensor(0.2755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Val loss: 0.037523202525759206\n",
      "Test loss: 0.042322990845660775\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8ee34ea31b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mval_loss_hist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m writer.add_hparams({\"lr\": learning_rate,\n\u001b[0m\u001b[0;32m    135\u001b[0m                \u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                \"Loss\": test_loss})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "def test(data_ML, multimodal_learner, meta_learner, loss_fn, task_data, horizon = 10):\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_tasks = len(data_ML)\n",
    "    task_size = data_ML.x.shape[-3]\n",
    "    input_dim = data_ML.x.shape[-1]\n",
    "    window_size = data_ML.x.shape[-2]\n",
    "    output_dim = data_ML.y.shape[-1]\n",
    "\n",
    "    accum_error = 0.0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for task_id in range(0, (total_tasks-horizon-1), total_tasks//100):\n",
    "\n",
    "        x_spt, y_spt = data_ML[task_id]\n",
    "        x_qry = data_ML.x[(task_id+1):(task_id+1+horizon)].reshape(-1, window_size, input_dim)\n",
    "        y_qry = data_ML.y[(task_id+1):(task_id+1+horizon)].reshape(-1, output_dim)\n",
    "        task = task_data[task_id:task_id+1].cuda()\n",
    "        \n",
    "        x_spt, y_spt = to_torch(x_spt), to_torch(y_spt)\n",
    "        x_qry = to_torch(x_qry)\n",
    "        y_qry = to_torch(y_qry)\n",
    "        x_spt_encod, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x_spt, task, output_encoding = True)\n",
    "        x_qry_encod, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x_qry, task, output_encoding = True)\n",
    "\n",
    "        train_task = [Task(x_spt_encod, y_spt)]\n",
    "        val_task = [Task(x_qry_encod, y_qry)]\n",
    "\n",
    "        adapted_params = meta_learner.adapt(train_task)\n",
    "        mean_loss = meta_learner.step(adapted_params, val_task, is_training = 0)\n",
    "\n",
    "        count += 1\n",
    "        accum_error += mean_loss.data\n",
    "\n",
    "    return accum_error.cpu().detach().numpy()/count\n",
    "\n",
    "#hyperparaemters: lr, weight_vrae, number of layers     \n",
    "fast_lr = 0.01\n",
    "first_order = False\n",
    "n_inner_iter = 1\n",
    "inner_loop_grad_clip = 20\n",
    "noise_level = 0.0\n",
    "noise_type = \"additive\"\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "grid = [0., noise_level]\n",
    "\n",
    "multimodal_learner = MultimodalLearner(task_net, task_encoder, task_decoder, lmbd)\n",
    "multimodal_learner.to(device)\n",
    "\n",
    "output_layer = LinearModel(120,1)\n",
    "opt = torch.optim.Adam(list(multimodal_learner.parameters())+list(output_layer.parameters()), lr = 0.0001)\n",
    "\n",
    "meta_learner = MetaLearner(output_layer, opt, fast_lr, loss_func, first_order, n_inner_iter, inner_loop_grad_clip, device)\n",
    "\n",
    "task_data_train = torch.FloatTensor(get_task_encoder_input(train_data_ML))\n",
    "task_data_validation = torch.FloatTensor(get_task_encoder_input(validation_data_ML))\n",
    "task_data_test = torch.FloatTensor(get_task_encoder_input(test_data_ML))\n",
    "\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "#loss_fn = mae\n",
    "\n",
    "val_loss_hist = []\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    multimodal_learner.train()\n",
    "    \n",
    "    batch_idx = np.random.randint(0, total_tasks-1, batch_size)\n",
    "        \n",
    "    task =task_data_train[batch_idx].cuda()\n",
    "    \n",
    "    x_spt, y_spt = train_data_ML[batch_idx]\n",
    "    x_qry, y_qry = train_data_ML[batch_idx+1]\n",
    "\n",
    "    x_spt, y_spt = to_torch(x_spt), to_torch(y_spt)\n",
    "    x_qry = to_torch(x_qry)\n",
    "    y_qry = to_torch(y_qry)\n",
    "\n",
    "    # data augmentation\n",
    "    epsilon = grid[np.random.randint(0, len(grid))]\n",
    "\n",
    "    if noise_type == \"additive\":\n",
    "        y_spt = y_spt + epsilon\n",
    "        y_qry = y_qry + epsilon\n",
    "    else:\n",
    "        y_spt = y_spt * (1 + epsilon)\n",
    "        y_qry = y_qry * (1 + epsilon)\n",
    "\n",
    "    x_spt_encodings = []\n",
    "    x_qry_encodings = []\n",
    "    vrae_loss_accum = 0.0\n",
    "    for i in range(batch_size):\n",
    "        x_spt_encoding, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x_spt[i],task[i:i+1], output_encoding = True)\n",
    "        x_spt_encodings.append(x_spt_encoding)\n",
    "        vrae_loss_accum += vrae_loss\n",
    "        #print(\"accum_vrae:\", vrae_loss_accum)\n",
    "        #print(\"vrae:\", vrae_loss)\n",
    "        \n",
    "        x_qry_encoding, (vrae_loss, kl_loss, rec_loss) = multimodal_learner(x_qry[i],task[i:i+1], output_encoding = True)\n",
    "        x_qry_encodings.append(x_qry_encoding)\n",
    "    \n",
    "    train_tasks = [Task(x_spt_encodings[i], y_spt[i]) for i in range(x_spt.shape[0])]\n",
    "    val_tasks = [Task(x_qry_encodings[i], y_qry[i]) for i in range(x_qry.shape[0])]\n",
    "    \n",
    "    #print(vrae_loss)\n",
    "\n",
    "    adapted_params = meta_learner.adapt(train_tasks)\n",
    "    mean_loss = meta_learner.step(adapted_params, val_tasks, is_training = True, additional_loss_term = vrae_loss_accum/batch_size) \n",
    "  \n",
    "    for tag, parm in output_layer.linear.named_parameters():\n",
    "        writer.add_histogram(\"Grads\"+tag, parm.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "    multimodal_learner.eval()\n",
    "    #with torch.no_grad():\n",
    "    val_loss = test(validation_data_ML, multimodal_learner, meta_learner, mae, task_data_validation)\n",
    "    test_loss = test(test_data_ML, multimodal_learner, meta_learner, mae, task_data_test)\n",
    "    \n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Train loss:\",mean_loss)\n",
    "    print(\"Val loss:\", val_loss)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", mean_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "    \n",
    "\n",
    "        \n",
    "    val_loss_hist.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_hparams({\"lr\": fast_lr,\n",
    "               \"batch_size\": batch_size},\n",
    "               {\"Loss\": test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of Linear(in_features=120, out_features=1, bias=True)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.linear.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.8550, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrae_loss_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
