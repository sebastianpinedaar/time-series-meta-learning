{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on https://dzlab.github.io/timeseries/2018/11/25/LSTM-FCN-pytorch-part-1/\n",
    "\n",
    "class BlockFCNConv(nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=128, kernel_size=8, momentum=0.99, epsilon=0.001, squeeze=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=kernel_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=out_channel, eps=epsilon, momentum=momentum)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        # input (batch_size, num_variables, time_steps), e.g. (128, 1, 512)\n",
    "        x = self.conv(x)\n",
    "        # input (batch_size, out_channel, L_out)\n",
    "        x = self.batch_norm(x)\n",
    "        # same shape as input\n",
    "        y = self.relu(x)\n",
    "        return y\n",
    "    \n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, time_steps, channels=[1, 128, 256, 128], kernels=[8, 5, 3], output_dim =1, mom=0.99, eps=0.001):\n",
    "        super().__init__()\n",
    "        self.conv1 = BlockFCNConv(channels[0], channels[1], kernels[0], momentum=mom, epsilon=eps, squeeze=True)\n",
    "        self.conv2 = BlockFCNConv(channels[1], channels[2], kernels[1], momentum=mom, epsilon=eps, squeeze=True)\n",
    "        self.conv3 = BlockFCNConv(channels[2], channels[3], kernels[2], momentum=mom, epsilon=eps)\n",
    "        output_size = time_steps - sum(kernels) + len(kernels)\n",
    "        self.global_pooling = nn.AvgPool1d(kernel_size=output_size)\n",
    "        self.linear = nn.Linear(channels[2], output_dim )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # apply Global Average Pooling 1D\n",
    "        x = self.global_pooling(x)\n",
    "        x = self.linear(x.squeeze())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = FCN(13, channels=[13, 128, 128, 128])\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.conv.weight\n",
      "conv1.conv.bias\n",
      "conv1.batch_norm.weight\n",
      "conv1.batch_norm.bias\n",
      "conv2.conv.weight\n",
      "conv2.conv.bias\n",
      "conv2.batch_norm.weight\n",
      "conv2.batch_norm.bias\n",
      "conv3.conv.weight\n",
      "conv3.conv.bias\n",
      "conv3.batch_norm.weight\n",
      "conv3.batch_norm.bias\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, seq_len, input_dim, n_layers, hidden_dim, output_dim, lin_hidden_dim = 50):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)#\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        out1 = self.linear(hn[-1].view(len(x),-1))\n",
    "\n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(64, 15, 13, 2, 120, 1)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ts_dataset import TSDataset\n",
    "\n",
    "\n",
    "dataset_name = \"POLLUTION\"\n",
    "if dataset_name == \"POLLUTION\":\n",
    "    task_size = 50\n",
    "    window_size = 5\n",
    "    input_dim = 14\n",
    "\n",
    "elif dataset_name == \"HR\":\n",
    "    task_size = 50\n",
    "    window_size = 32\n",
    "    input_dim = 13 \n",
    "\n",
    "elif dataset_name == \"BATTERY\":\n",
    "    task_size = 50\n",
    "    window_size = 20\n",
    "    input_dim = 3\n",
    "\n",
    "kernels = [8,5,3] if dataset_name!= \"POLLUTION\" else [4,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(  open( \"../Data/TRAIN-\"+dataset_name+\"-W\"+str(window_size)+\"-T\"+str(task_size)+\"-NOML.pickle\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = np.transpose(train_data.x[:64], [0,2,1])\n",
    "y = train_data.y[:64]\n",
    "        \n",
    "x = torch.tensor(x).float().to(device)\n",
    "y = torch.tensor(y).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FCN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8a73149a6a4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FCN' is not defined"
     ]
    }
   ],
   "source": [
    "model = FCN(time_steps = window_size,  channels=[input_dim, 128, 128, 128] , kernels=kernels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
