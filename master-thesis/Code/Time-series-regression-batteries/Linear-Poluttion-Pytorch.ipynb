{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import time\n",
    "import winsound\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '2016JD024877.pdf', 'BeijingPM20100101_20151231.csv', 'ChengduPM20100101_20151231.csv', 'Data-exploration-five-cities.ipynb', 'GuangzhouPM20100101_20151231.csv', 'ShanghaiPM20100101_20151231.csv', 'ShenyangPM20100101_20151231.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"C:/Users/Sebastian/Documents/Data Analytics Master/Semester4-Thesis/Datasets/FiveCitiePMData/\"\n",
    "files = os.listdir(path)\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BeijingPM20100101_20151231.csv', 'ChengduPM20100101_20151231.csv', 'GuangzhouPM20100101_20151231.csv', 'ShanghaiPM20100101_20151231.csv', 'ShenyangPM20100101_20151231.csv']\n"
     ]
    }
   ],
   "source": [
    "files = [file for file in filter(lambda x: x[-4:] ==\".csv\", files)]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def windows (data, window_size, stride):\n",
    "    \n",
    "    x = np.array(data)\n",
    "    length = data.shape[0]\n",
    "    windows_list = []\n",
    "    \n",
    "    for i in range(0, length-window_size, stride):\n",
    "        windows_list.append(x[i:i+window_size][np.newaxis,:])\n",
    "        \n",
    "    return tuple(windows_list)\n",
    "\n",
    "\n",
    "class TSDataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, window_size, stride, scale = 0, max_list=[]):\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for i in range(0, len(data)-window_size, stride):             \n",
    "            x_list.append(data[i:i+window_size, :-1][np.newaxis,:])\n",
    "            y_list.append(data[i+window_size,-1])\n",
    "        \n",
    "        self.x = np.vstack(x_list)\n",
    "        self.y = np.vstack(y_list)\n",
    "        \n",
    "        if scale:         \n",
    "            self.max_list = max_list\n",
    "            \n",
    "        else:\n",
    "            self.max_y = np.max(self.y)\n",
    "            self.max_list = [np.max(np.max(self.x, axis=1), axis=0), self.max_y]\n",
    "            \n",
    "            self.max_list[0] = [i if i!=0 else 0.00001 for i in self.max_list[0]  ]\n",
    "        \n",
    "        #self.x = np.divide(self.x, self.max_list[0])\n",
    "        #self.y = self.y/self.max_list[-1]\n",
    "        #self.y = (self.y-np.mean(self.y))/np.std(self.y)\n",
    "        \n",
    "        print(\"x shape:\", self.x.shape)\n",
    "        print(\"y shape:\", self.y.shape)\n",
    " \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __add__(self, other):\n",
    "        object_copy = copy.deepcopy(self)\n",
    "        object_copy.x = np.concatenate([object_copy.x, other.x], axis=0)\n",
    "        object_copy.y = np.concatenate([object_copy.y, other.y], axis=0)\n",
    "        object_copy.max_list[0] = np.max((object_copy.max_list[0], other.max_list[0]), axis=0)\n",
    "        object_copy.max_list[1] = np.max((object_copy.max_list[1], other.max_list[1]), axis=0)\n",
    "        \n",
    "        return object_copy\n",
    "    \n",
    "    def normalize(self):\n",
    "        \n",
    "        self.x = np.divide(self.x, self.max_list[0])\n",
    "        self.y = self.y/self.max_list[-1]        \n",
    "   \n",
    " \n",
    "def mae(y_pred, y_true):\n",
    "    loss = (torch.abs(y_pred - y_true)).mean()\n",
    "    return loss    \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, batch_size, seq_len, input_dim, n_layers, hidden_dim, output_dim, lin_hidden_dim = 100):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)#\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.n_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #self.batch_size = len(x)\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        #out1 = self.linear(output\n",
    "        out1 = self.linear(hn[-1].view(len(x),-1))\n",
    "        #out2 = self.linear2(F.relu(out1))\n",
    "        \n",
    "        #print(output.shape)\n",
    "        #out = self.linear(output.view(len(x),-1))\n",
    "        return out1\n",
    "    \n",
    "class LinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, seq_len):\n",
    "        \n",
    "        super(LinearModel, self).__init__()\n",
    "        self.n_features = input_dim*seq_len\n",
    "        self.linear = nn.Linear(self.n_features, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.linear(x.view(-1,self.n_features))\n",
    "        return out\n",
    "       \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_na(x):\n",
    "    \n",
    "    return np.sum(np.isnan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 2\n",
    "window_size = 24\n",
    "backcast = window_size\n",
    "forecast = 1\n",
    "\n",
    "batch_size = 1024\n",
    "seq_len = backcast\n",
    "output_dim = forecast\n",
    "input_dim = 16\n",
    "n_layers = 2\n",
    "hidden_dim = 30\n",
    "\n",
    "var = ['month', 'day', 'hour', 'season', 'DEWP', 'HUMI',\n",
    "       'PRES', 'TEMP', 'cbwd', 'Iws', 'precipitation', 'Iprec', 'PM_US Post'] #PM_US Post is the targe\n",
    "\n",
    "cbwd_dict = {\"NE\":1, \"SE\":2, \"SW\":3, \"NW\":4, \"cv\":0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path+files[0])\n",
    "\n",
    "\n",
    "def pre_process(data, var):\n",
    "\n",
    "    #imputing target\n",
    "    data[(\"PM_US Post\")].loc[np.isnan(data[\"PM_US Post\"])] = 0\n",
    "\n",
    "    #imputing numerical vars\n",
    "    for i, v in enumerate(var):    \n",
    "        try:\n",
    "            data[v] = data[v].interpolate()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #new varibale for calm and variable wind\n",
    "    data[\"cv_wd\"] = (data[\"cbwd\"]==\"cv\").astype(int)\n",
    "\n",
    "    #inputting wind directioin\n",
    "    data[\"cbwd\"] = data[\"cbwd\"].replace(cbwd_dict)\n",
    "    \n",
    "    data[\"na_cbwd\"] = np.isnan(data[\"cbwd\"]).astype(int)\n",
    "    data[(\"cbwd\")].loc[np.isnan(data[\"cbwd\"])] = -1\n",
    "    \n",
    "    data[\"na_precipitation\"] = np.isnan(data[\"precipitation\"]).astype(int)\n",
    "    data[(\"precipitation\")].loc[np.isnan(data[\"precipitation\"])] = -1\n",
    "    \n",
    "    data[\"na_Iprec\"] = np.isnan(data[\"Iprec\"]).astype(int)\n",
    "    data[(\"Iprec\")].loc[np.isnan(data[\"Iprec\"])] = -1\n",
    "   \n",
    "    var = var[:-1] + [\"na_cbwd\", \"na_precipitation\", \"na_Iprec\", \"cv_wd\"] +[var[-1]]\n",
    "    #padding wind direction column\n",
    "    #print(\"NAs before padding:\", data.cbwd.isna().sum())\n",
    "    #data[\"cbwd\"] = data[\"cbwd\"].fillna(method=\"pad\", limit=100)\n",
    "    #print(\"NAs after padding:\", data.cbwd.isna().sum())\n",
    "\n",
    "    return data[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebastian\\Anaconda3\\envs\\pytorch12\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (15763, 24, 16)\n",
      "y shape: (15763, 1)\n",
      "x shape: (5247, 24, 16)\n",
      "y shape: (5247, 1)\n",
      "x shape: (5247, 24, 16)\n",
      "y shape: (5247, 1)\n",
      "Training size: (15763, 24, 16)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(path+files[0])\n",
    "data = np.array(pre_process(data, var))\n",
    "idx = np.arange(len(data))\n",
    "train_split = int(len(data)*0.6)\n",
    "val_split = int(len(data)*0.8)\n",
    "\n",
    "train_idx = idx[:train_split]\n",
    "val_idx = idx[train_split:val_split]\n",
    "test_idx = idx[val_split:]\n",
    "\n",
    "train_dataset = TSDataset(data[train_idx], window_size, stride)\n",
    "test_dataset = TSDataset(data[test_idx], window_size, stride, scale = 1, max_list = train_dataset.max_list)\n",
    "val_dataset = TSDataset(data[val_idx], window_size, stride, scale = 1, max_list = train_dataset.max_list)\n",
    "\n",
    "\n",
    "for file in files[1:1]:\n",
    "    data = pd.read_csv(path+file)\n",
    "    data = np.array(pre_process(data, var))\n",
    "    idx = np.arange(len(data))\n",
    "    train_split = int(len(data)*0.6)\n",
    "    val_split = int(len(data)*0.8)\n",
    "    #dont apply it for time series\n",
    "    #np.random.shuffle(idx)\n",
    "\n",
    "    train_idx = idx[:train_split]\n",
    "    val_idx = idx[train_split:val_split]\n",
    "    test_idx = idx[val_split:]\n",
    "\n",
    "    train_dataset = TSDataset(data[train_idx], window_size, stride) + train_dataset\n",
    "    test_dataset = TSDataset(data[test_idx], window_size, stride, scale = 1, max_list = train_dataset.max_list) + test_dataset\n",
    "    val_dataset = TSDataset(data[val_idx], window_size, stride, scale = 1, max_list = train_dataset.max_list) + test_dataset\n",
    "\n",
    "    \n",
    "train_dataset.normalize()\n",
    "test_dataset.normalize()\n",
    "val_dataset.normalize()\n",
    "\n",
    "print(\"Training size:\", train_dataset.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **params)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, **params)\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, **params)\n",
    "val_iter = iter(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 385\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel (input_dim,  output_dim, seq_len)\n",
    "\n",
    "print(\"Model size:\", count_parameters(model))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "max_epochs = 100\n",
    "val_loss_list = []\n",
    "prev_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebastian\\Anaconda3\\envs\\pytorch12\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11784772803953877\n",
      "0.11198620120468471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebastian\\Anaconda3\\envs\\pytorch12\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10757092868363669\n",
      "0.10311518723831328\n",
      "Train loss: 0.09892785047810597\n",
      "0.09544810173919094\n",
      "Train loss: 0.09212190990135889\n",
      "0.08909087790428671\n",
      "Train loss: 0.08704822063011196\n",
      "0.08475172169377333\n",
      "Train loss: 0.0832016381697365\n",
      "0.08113443809297849\n",
      "Train loss: 0.08015951434884029\n",
      "0.07814631274433984\n",
      "Train loss: 0.07761507526806737\n",
      "0.07553040916968101\n",
      "Train loss: 0.0753384784860394\n",
      "0.07315513280907063\n",
      "Train loss: 0.07326599574403654\n",
      "0.07110958741503896\n",
      "Train loss: 0.07137655817510619\n",
      "0.06943147763828561\n",
      "Train loss: 0.06981340479165976\n",
      "0.06758028698354897\n",
      "Train loss: 0.06817234947229107\n",
      "0.06578227260924076\n",
      "Train loss: 0.06674580409019677\n",
      "0.0647331369266536\n",
      "Train loss: 0.06544401269298077\n",
      "0.06317889672933544\n",
      "Train loss: 0.06421007766061451\n",
      "0.06213292319129507\n",
      "Train loss: 0.06320278812772935\n",
      "0.061314616121574655\n",
      "Train loss: 0.062252219418262394\n",
      "0.06019161928573608\n",
      "Train loss: 0.06138615470235665\n",
      "0.05960472528624812\n",
      "Train loss: 0.06060278578134068\n",
      "0.058675535567060795\n",
      "Train loss: 0.05995922491203965\n",
      "0.05814841415641511\n",
      "Train loss: 0.05929535622827551\n",
      "0.057591563748482955\n",
      "Train loss: 0.05877709792694003\n",
      "0.05668094891530118\n",
      "Train loss: 0.05829978594984801\n",
      "0.05614012692138561\n",
      "Train loss: 0.05776998472981917\n",
      "0.05613633255950492\n",
      "Train loss: 0.057314978068415484\n",
      "0.055325940325188824\n",
      "Train loss: 0.05696303683259777\n",
      "0.05543407094431464\n",
      "Train loss: 0.05664077503019985\n",
      "0.05478135282246526\n",
      "Train loss: 0.056282657646869315\n",
      "0.05419645772131143\n",
      "Train loss: 0.05601952373858583\n",
      "0.05432487490255196\n",
      "Train loss: 0.05573908987851575\n",
      "0.053825024677266936\n",
      "Train loss: 0.05559291114695883\n",
      "0.054349649977276204\n",
      "Train loss: 0.055351325905907994\n",
      "0.05362104729374568\n",
      "Train loss: 0.05512298846550341\n",
      "0.053673128993432365\n",
      "Train loss: 0.054901789608438376\n",
      "0.05298751206901361\n",
      "Train loss: 0.054975401688598026\n",
      "0.053659200979308305\n",
      "Train loss: 0.054725785436565894\n",
      "0.05310205429187497\n",
      "Train loss: 0.05443139983234631\n",
      "0.05240587121518608\n",
      "Train loss: 0.0543220921474567\n",
      "0.052847445287323006\n",
      "Train loss: 0.05425679299103377\n",
      "0.0525309295768576\n",
      "Train loss: 0.05408413288863731\n",
      "0.052126114922113276\n",
      "Train loss: 0.05397010672335764\n",
      "0.05175739554296068\n",
      "Train loss: 0.053888553089049766\n",
      "0.052103432073226444\n",
      "Train loss: 0.05376646274912632\n",
      "0.052140918893486474\n",
      "Train loss: 0.053674888814888286\n",
      "0.05200772196173736\n",
      "Train loss: 0.05368890502194512\n",
      "0.05180205734617396\n",
      "Train loss: 0.053516502978204206\n",
      "0.05200955745209756\n",
      "Train loss: 0.05350203377627248\n",
      "0.052001086823544276\n",
      "Train loss: 0.05339397145477163\n",
      "0.0519351303301949\n",
      "Train loss: 0.05333586268196446\n",
      "0.05152481335565116\n",
      "Train loss: 0.053257237993639704\n",
      "0.0514736517508393\n",
      "Train loss: 0.053174788093433414\n",
      "0.05159351209220101\n",
      "Train loss: 0.05310409000184885\n",
      "0.05131386320847022\n",
      "Train loss: 0.0530927422523975\n",
      "0.05117152245122068\n",
      "Train loss: 0.053062645796326476\n",
      "0.051500288818236734\n",
      "Train loss: 0.05304911096658848\n",
      "0.05111628160099314\n",
      "Train loss: 0.052930971591377184\n",
      "0.05138697189994033\n",
      "Train loss: 0.052969505969882355\n",
      "0.05090583706026489\n",
      "Train loss: 0.05291275682570267\n",
      "0.05119565840045884\n",
      "Train loss: 0.052899310005504845\n",
      "0.051088912516537\n",
      "Train loss: 0.05274729144265603\n",
      "0.05094119566484749\n",
      "Train loss: 0.0527287243549869\n",
      "0.0507496975962948\n",
      "Train loss: 0.05259767489774021\n",
      "0.05098394897351248\n",
      "Train loss: 0.052558509098718155\n",
      "0.05065443066868392\n",
      "Train loss: 0.052571853766129086\n",
      "0.05061763727354826\n",
      "Train loss: 0.05275341457042311\n",
      "0.05093598385450271\n",
      "Train loss: 0.05261069193363288\n",
      "0.05056156260367005\n",
      "Train loss: 0.05268143027149269\n",
      "0.05051171332413137\n",
      "Train loss: 0.05248377632134904\n",
      "0.050580633861355855\n",
      "Train loss: 0.052434371449329796\n",
      "0.05042701430884297\n",
      "Train loss: 0.05243756474085374\n",
      "0.050634558473583946\n",
      "Train loss: 0.052325617968194765\n",
      "0.0504716570163048\n",
      "Train loss: 0.05225934023853487\n",
      "0.05050407905697436\n",
      "Train loss: 0.05224798634187004\n",
      "0.05071353921086329\n",
      "Train loss: 0.05224527121875852\n",
      "0.05019946913645793\n",
      "Train loss: 0.05227564752394004\n",
      "0.0504176087481523\n",
      "Train loss: 0.05215732348488573\n",
      "0.050386128524342835\n",
      "Train loss: 0.052183223834748965\n",
      "0.050503860702565996\n",
      "Train loss: 0.05212848104003843\n",
      "0.050243744759763426\n",
      "Train loss: 0.05210024285816922\n",
      "0.05057115367984168\n",
      "Train loss: 0.05205459038813896\n",
      "0.049981478909827515\n",
      "Train loss: 0.052027991584275936\n",
      "0.050847052955584275\n",
      "Train loss: 0.0520994786206428\n",
      "0.05006054993516517\n",
      "Train loss: 0.05206244895076953\n",
      "0.050455624120864094\n",
      "Train loss: 0.05197272520229454\n",
      "0.04986883403901284\n",
      "Train loss: 0.05194377473371817\n",
      "0.0502076293014335\n",
      "Train loss: 0.05191302451815035\n",
      "0.05024382465719404\n",
      "Train loss: 0.05191880199768243\n",
      "0.05022704939950596\n",
      "Train loss: 0.051883903014011766\n",
      "0.04981193821490299\n",
      "Train loss: 0.051947386482660673\n",
      "0.05033372571840612\n",
      "Train loss: 0.05182768661115955\n",
      "0.05017099445158581\n",
      "Train loss: 0.05180535932682246\n",
      "0.05006287994485527\n",
      "Train loss: 0.05179396956782803\n",
      "0.05021569014166409\n",
      "Train loss: 0.051840189698789806\n",
      "0.05026909943555727\n",
      "Train loss: 0.0518000984495533\n",
      "0.04979306402983087\n",
      "Train loss: 0.051726798053748385\n",
      "0.04980427474678074\n",
      "Train loss: 0.051709542060056436\n",
      "0.0502273246293775\n",
      "Train loss: 0.05172271461848838\n",
      "0.0498351293816745\n",
      "Train loss: 0.051746855916192235\n",
      "0.049906019607748056\n",
      "Train loss: 0.051778333916364945\n",
      "0.05020337418071061\n",
      "74.36718344688416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    \n",
    "    train_iter = iter(train_loader)\n",
    "    train_loss = 0.0\n",
    "    for x_train, y_train in train_iter:\n",
    "    \n",
    "        model.zero_grad()\n",
    "        x_train, y_train = torch.tensor(x_train).float(), torch.tensor(y_train).float()\n",
    "\n",
    "        #model.batch_size = x_train.shape[0]\n",
    "        #hidden_init = model.init_hidden()\n",
    "        y_pred = model(x_train)\n",
    "        loss = mae(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data.numpy()*x_train.shape[0]\n",
    "    print(\"Train loss:\", train_loss/len(train_dataset))\n",
    "        \n",
    "        \n",
    "    val_loss = 0.0\n",
    "    for x_val, y_val in val_loader:\n",
    "        x_val, y_val = torch.tensor(x_val).float(), torch.tensor(y_val).float()\n",
    "        #if(x_val.shape[0]==batch_size):\n",
    "        model.batch_size = x_val.shape[0]\n",
    "        y_pred = model(x_val)\n",
    "        val_loss += mae(y_pred, y_val).data.numpy()*x_val.shape[0]\n",
    "    val_loss /= (len(val_dataset)*forecast)\n",
    "    \n",
    "    #if(val_loss>prev_loss):\n",
    "    #    break\n",
    "        \n",
    "    prev_loss = val_loss\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(val_loss)\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 100  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
